<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang=""><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>SAP HANA System Replication Scale-Up - Performance Optimized Scenario | SUSE Linux Enterprise Server for SAP Applications 15 SP1</title><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" /><link rel="stylesheet" type="text/css" href="static/css/style.css" /><link rel="stylesheet" type="text/css" href="static/css/highlight.css" /><meta name="generator" content="DAPS 3.0.0 (https://opensuse.github.io/daps) using SUSE XSL Stylesheets for SUSE Best Practices 2.0.17 (based on DocBook XSL Stylesheets 1.79.2)" /><meta name="product-name" content="SUSE Linux Enterprise Server for SAP Applications" /><meta name="product-number" content="15 SP1" /><meta name="book-title" content="SAP HANA System Replication Scale-Up - Performance Optimized Scenario" /><meta name="description" content="SUSE® Linux Enterprise Server for SAP Applications is optimized in various ways for SAP* applications. This guide provides detailed information about installing and customizing SUSE Linux Enterprise Server for SAP Applications for SAP HANA system replication in the perfomance optimized scenario. The document focuses on the steps to integrate an already installed and working SAP HANA with system replication." /><meta name="tracker-url" content="https://github.com/SUSE/suse-best-practices/issues/new" /><meta name="tracker-type" content="gh" /><link rel="canonical" href="https://documentation.suse.com/sbp/all/single-html/SLES4SAP-hana-sr-guide-PerfOpt-15/index.html" />
<meta property="og:title" content="SUSE Linux Enterprise Server for SAP Applications 15 SP1: SAP HANA System Replication Scale-Up - Performance Optimized Scenario" />
<meta property="og:image" content="https://documentation.suse.com/sbp/all/single-html/SLES4SAP-hana-sr-guide-PerfOpt-15/images/hana_sr_in_cluster.svg" />
<meta property="og:description" content="SUSE® Linux Enterprise Server for SAP Applications is optimized in various ways for SAP* applications. This guide provides detailed information about installing and customizing SUSE Linux Enterprise Server for SAP Applications for SAP HANA system replication in the perfomance optimized scenario. The document focuses on the steps to integrate an already installed and working SAP HANA with system replication." />
<meta property="og:url" content="https://documentation.suse.com/sbp/all/single-html/SLES4SAP-hana-sr-guide-PerfOpt-15/index.html" />
<script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://static.opensuse.org/fonts/fonts.css"></link>');
}
else {
  document.write('<link rel="stylesheet" type="text/css" href="static/css/fonts-onlylocal.css"></link>');
}

</script><noscript><link rel="stylesheet" type="text/css" href="https://static.opensuse.org/fonts/fonts.css" /></noscript><script src="static/js/jquery-1.10.2.min.js" type="text/javascript"></script><script src="static/js/script.js" type="text/javascript"></script><script src="static/js/highlight.min.js" type="text/javascript"></script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script><script type="text/javascript">
if (window.location.protocol.toLowerCase() != 'file:') {
  var externalScript1 = document.createElement("script");
  externalScript1.src = "/docserv/res/extra.js";
  document.head.appendChild(externalScript1);
}
      </script></head><body class="single offline js-off"><div class="bypass-block"><a href="#_content">Jump to content</a></div><div id="_outer-wrap"><div id="_white-bg"><div id="_header"><div id="_logo"><a href="https://www.suse.com/"><img src="static/images/logo.png" alt="Logo" /></a></div><div class="crumbs inactive"><a class="overview-link" href="https://documentation.suse.com/" title="documentation.suse.com"><span class="overview-icon">documentation.suse.com</span></a><span> › </span><a class="single-crumb" href="#id-1" accesskey="c"><span class="single-contents-icon"></span>SAP HANA System Replication Scale-Up - Performance Optimized Scenario</a><div class="bubble-corner active-contents"></div></div><div class="clearme"></div></div></div><div id="_fixed-header-wrap" class="inactive"><div id="_fixed-header"><div class="crumbs inactive"><a class="single-crumb" href="#id-1" accesskey="c"><span class="single-contents-icon"></span>Show Contents: SAP HANA System Replication Scale-Up - Performance Optimized Scenario</a></div><div class="buttons"><a class="top-button button" href="#">Top</a><div class="clearme"></div></div><div class="clearme"></div></div><div class="active-contents bubble"><div class="bubble-container"><div id="_bubble-toc"><ol><li class="inactive"><a href="#pre.hana-sr"><span class="number">1 </span><span class="name">About This Guide</span></a></li><li class="inactive"><a href="#cha.hana-sr.scenario"><span class="number">2 </span><span class="name">Supported Scenarios and Prerequisites</span></a></li><li class="inactive"><a href="#cha.hana-sr.scope"><span class="number">3 </span><span class="name">Scope of This Documentation</span></a></li><li class="inactive"><a href="#_planning_the_installation"><span class="number">4 </span><span class="name">Planning the Installation</span></a></li><li class="inactive"><a href="#_operating_system_setup"><span class="number">5 </span><span class="name">Operating System Setup</span></a></li><li class="inactive"><a href="#cha.s4s.hana-install"><span class="number">6 </span><span class="name">Installing the SAP HANA Databases on both cluster nodes</span></a></li><li class="inactive"><a href="#_set_up_sap_hana_system_replication"><span class="number">7 </span><span class="name">Set Up SAP HANA System Replication</span></a></li><li class="inactive"><a href="#_set_up_sap_hana_hadr_providers"><span class="number">8 </span><span class="name">Set Up SAP HANA HA/DR providers</span></a></li><li class="inactive"><a href="#_configuration_of_the_cluster"><span class="number">9 </span><span class="name">Configuration of the Cluster</span></a></li><li class="inactive"><a href="#cha.s4s.test-cluster"><span class="number">10 </span><span class="name">Testing the Cluster</span></a></li><li class="inactive"><a href="#cha.hana-sr.administrate"><span class="number">11 </span><span class="name">Administration</span></a></li><li class="inactive"><a href="#app.hana-sr.information"><span class="number">12 </span><span class="name">Useful Links, Manuals, and SAP Notes</span></a></li><li class="inactive"><a href="#app.hana-sr.example"><span class="number">13 </span><span class="name">Examples</span></a></li><li class="inactive"><a href="#_reference"><span class="number">14 </span><span class="name">Reference</span></a></li><li class="inactive"><a href="#_legal_notice"><span class="number">15 </span><span class="name">Legal Notice</span></a></li><li class="inactive"><a href="#_gnu_free_documentation_license"><span class="number">16 </span><span class="name">GNU Free Documentation License</span></a></li></ol></div><div class="clearme"></div></div></div></div><div id="_toc-bubble-wrap"></div><div id="_content" class=""><div class="documentation"><div xml:lang="en" class="article " id="id-1" lang="en"><div class="titlepage"><div><h6 class="version-info"><span class="productname ">SUSE Linux Enterprise Server for SAP Applications</span> <span class="productnumber ">15 SP1</span></h6><div><h1 class="title">SAP HANA System Replication Scale-Up - Performance Optimized Scenario</h1></div><div class="abstract "><p>SUSE® Linux Enterprise Server for SAP Applications is optimized in various ways for SAP*
        applications. This guide provides detailed information about installing and customizing SUSE
        Linux Enterprise Server for SAP Applications for SAP HANA system replication in the perfomance
        optimized scenario. The document focuses on the steps to integrate an already installed and working SAP
        HANA with system replication.</p><p><span class="strong"><strong>Disclaimer</strong></span>: This document is part of the SUSE Best Practices series. All documents
        published in this series were contributed voluntarily by SUSE employees and by third parties.
        If not stated otherwise inside the document, the articles are intended only to be one example of how a particular action could be taken.
        Also, SUSE cannot verify either that the actions described in the articles do what they
        claim to do or that they do not have unintended consequences. All information found in this
        document has been compiled with utmost attention to detail. However, this does not guarantee
        complete accuracy. Therefore, we need to specifically state that neither SUSE LLC, its affiliates,
        the authors, nor the translators may be held liable for possible errors or the consequences thereof.
    </p></div><div class="author"><span class="imprint-label">Author: </span><span class="firstname ">Fabian</span> <span class="surname ">Herschel, Distinguished Architect SAP, SUSE</span></div><div class="author"><span class="imprint-label">Author: </span><span class="firstname ">Bernd</span> <span class="surname ">Schubert, SAP Solution Architect, SUSE</span></div><div class="author"><span class="imprint-label">Author: </span><span class="firstname ">Lars</span> <span class="surname ">Pinne, System Engineer, SUSE</span></div><div class="date"><span class="imprint-label">Publication Date: </span>2020-03-19</div></div></div><div class="line"><div class="toc"><dl><dt><span class="section"><a href="#pre.hana-sr"><span class="number">1 </span><span class="name">About This Guide</span></a></span></dt><dt><span class="section"><a href="#cha.hana-sr.scenario"><span class="number">2 </span><span class="name">Supported Scenarios and Prerequisites</span></a></span></dt><dt><span class="section"><a href="#cha.hana-sr.scope"><span class="number">3 </span><span class="name">Scope of This Documentation</span></a></span></dt><dt><span class="section"><a href="#_planning_the_installation"><span class="number">4 </span><span class="name">Planning the Installation</span></a></span></dt><dt><span class="section"><a href="#_operating_system_setup"><span class="number">5 </span><span class="name">Operating System Setup</span></a></span></dt><dt><span class="section"><a href="#cha.s4s.hana-install"><span class="number">6 </span><span class="name">Installing the SAP HANA Databases on both cluster nodes</span></a></span></dt><dt><span class="section"><a href="#_set_up_sap_hana_system_replication"><span class="number">7 </span><span class="name">Set Up SAP HANA System Replication</span></a></span></dt><dt><span class="section"><a href="#_set_up_sap_hana_hadr_providers"><span class="number">8 </span><span class="name">Set Up SAP HANA HA/DR providers</span></a></span></dt><dt><span class="section"><a href="#_configuration_of_the_cluster"><span class="number">9 </span><span class="name">Configuration of the Cluster</span></a></span></dt><dt><span class="section"><a href="#cha.s4s.test-cluster"><span class="number">10 </span><span class="name">Testing the Cluster</span></a></span></dt><dt><span class="section"><a href="#cha.hana-sr.administrate"><span class="number">11 </span><span class="name">Administration</span></a></span></dt><dt><span class="section"><a href="#app.hana-sr.information"><span class="number">12 </span><span class="name">Useful Links, Manuals, and SAP Notes</span></a></span></dt><dt><span class="section"><a href="#app.hana-sr.example"><span class="number">13 </span><span class="name">Examples</span></a></span></dt><dt><span class="section"><a href="#_reference"><span class="number">14 </span><span class="name">Reference</span></a></span></dt><dt><span class="section"><a href="#_legal_notice"><span class="number">15 </span><span class="name">Legal Notice</span></a></span></dt><dt><span class="section"><a href="#_gnu_free_documentation_license"><span class="number">16 </span><span class="name">GNU Free Documentation License</span></a></span></dt></dl></div></div><div class="sect1" id="pre.hana-sr"><div class="titlepage"><div><div><h2 class="title"><span class="number">1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">About This Guide</span> <a title="Permalink" class="permalink" href="#pre.hana-sr">#</a></h2></div></div></div><div class="sect2" id="_introduction"><div class="titlepage"><div><div><h3 class="title"><span class="number">1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Introduction</span> <a title="Permalink" class="permalink" href="#_introduction">#</a></h3></div></div></div><p>SUSE® Linux Enterprise Server for SAP Applications is optimized in various ways for SAP* applications. This
guide provides detailed information about installing and customizing
SUSE Linux Enterprise Server for SAP Applications for SAP HANA system replication in the perfomance optimized scenario.</p><p>“SAP customers invest in SAP HANA” is the conclusion reached by a recent
market study carried out by Pierre Audoin Consultants (PAC). In Germany,
half of the companies expect SAP HANA to become the dominant
database platform in the SAP environment. Often the “SAP
Business Suite* powered by SAP HANA*” scenario is already being discussed
in concrete terms.</p><p>SUSE is also accommodating this development by providing SUSE Linux Enterprise Server for SAP Applications
– the recommended and supported operating system for SAP HANA. In close
collaboration with SAP and hardware partners, SUSE provides two
resource agents for customers to ensure the high availability of SAP HANA system
replications.</p><div class="sect3" id="_abstract"><div class="titlepage"><div><div><h4 class="title"><span class="number">1.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Abstract</span> <a title="Permalink" class="permalink" href="#_abstract">#</a></h4></div></div></div><p>This guide describes planning, setup, and basic testing of
SUSE Linux Enterprise Server for SAP Applications based on
the high availability solution scenario
"SAP HANA Scale-Up System Replication Performance Optimized".</p><p>From the application perspective the following variants are covered:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>plain system replication</p></li><li class="listitem "><p>system replication with secondary site read-enabled</p></li><li class="listitem "><p>multi-tier (chained) system replication</p></li><li class="listitem "><p>multi-target system replication</p></li><li class="listitem "><p>multi-tenant database containers for all above</p></li></ul></div><p>From the infrastructure perspective the following variants are covered:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>2-node cluster with  disk-based SBD</p></li><li class="listitem "><p>3-node cluster with disk-less SBD</p></li></ul></div></div><div class="sect3" id="_scale_up_versus_scale_out"><div class="titlepage"><div><div><h4 class="title"><span class="number">1.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Scale-Up Versus Scale-Out</span> <a title="Permalink" class="permalink" href="#_scale_up_versus_scale_out">#</a></h4></div></div></div><p>The first set of scenarios includes the architecture and development of
<span class="emphasis"><em>scale-up</em></span> solutions.</p><div class="figure" id="id-1.2.2.6.3"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/hana_sr_in_cluster.svg" target="_blank"><img src="images/hana_sr_in_cluster.svg" width="" alt="hana sr in cluster" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 1: </span><span class="name">SAP HANA System Replication Scale-Up in the Cluster </span><a title="Permalink" class="permalink" href="#id-1.2.2.6.3">#</a></h6></div></div><p>For these scenarios SUSE developed the scale-up
resource agent package <code class="literal">SAPHanaSR</code>. System replication will help to
replicate the database data from one computer to another computer to compensate for database failures (single-box replication).</p><p>The second set of scenarios includes the architecture and development of
<span class="emphasis"><em>scale-out</em></span> solutions (multi-box replication). For these scenarios SUSE
developed the scale-out resource agent package <code class="literal">SAPHanaSR-ScaleOut</code>.</p><div class="figure" id="id-1.2.2.6.6"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/SAPHanaSR-ScaleOut-Cluster.svg" target="_blank"><img src="images/SAPHanaSR-ScaleOut-Cluster.svg" width="" alt="SAPHanaSR ScaleOut Cluster" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 2: </span><span class="name">SAP HANA System Replication Scale-Out in the Cluster </span><a title="Permalink" class="permalink" href="#id-1.2.2.6.6">#</a></h6></div></div><p>With this mode of operation, internal SAP HANA high availability (HA)
mechanisms and the resource agent must work together or be coordinated
with each other. SAP HANA system replication automation for scale-out
is described in an own document available on our documentation Web page
at <a class="link" href="https://documentation.suse.com/sbp/all/" target="_blank">https://documentation.suse.com/sbp/all/</a>. The document for scale-out is named <span class="emphasis"><em>"SAP HANA System Replication Scale-Out - Performance Optimized Scenario"</em></span>.</p></div><div class="sect3" id="_scale_up_scenarios_and_resource_agents"><div class="titlepage"><div><div><h4 class="title"><span class="number">1.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Scale-Up Scenarios and Resource Agents</span> <a title="Permalink" class="permalink" href="#_scale_up_scenarios_and_resource_agents">#</a></h4></div></div></div><p>SUSE has implemented the scale-up scenario with the <code class="literal">SAPHana</code> resource
agent (RA), which performs the actual check of the SAP HANA database
instances. This RA is configured as a master/slave resource. In the
scale-up scenario, the master assumes responsibility for the SAP HANA
databases running in primary mode. The slave is responsible for
instances that are operated in synchronous (secondary) status.</p><p>To make configuring the cluster as simple as possible, SUSE also
developed the <code class="literal">SAPHanaTopology</code> resource agent. This RA runs on all nodes
of a SUSE Linux Enterprise Server for SAP Applications cluster and gathers information about the
statuses and configurations of SAP HANA system replications. It is
designed as a normal (stateless) clone.</p><p>SAP HANA System replication for Scale-Up is supported in the following
scenarios or use cases:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p><span class="strong"><strong>Performance optimized</strong></span> (<span class="emphasis"><em>A ⇒ B</em></span>). This scenario and setup <span class="strong"><strong>is
described in this document.</strong></span></p><div class="figure" id="id-1.2.2.7.5.1.2"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/SAPHanaSR-ScaleUP-perfOpt.svg" target="_blank"><img src="images/SAPHanaSR-ScaleUP-perfOpt.svg" width="" alt="SAPHanaSR ScaleUP perfOpt" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 3: </span><span class="name">SAP HANA System Replication Scale-Up in the Cluster - perfomance optimized </span><a title="Permalink" class="permalink" href="#id-1.2.2.7.5.1.2">#</a></h6></div></div><p>In the performance optimized scenario an SAP HANA RDBMS site A is synchronizing
with an SAP HANA RDBMS site B on a second node. As the HANA RDBMS on the second node
is configured to pre-load the tables, the takeover time is typically very
short.</p><p>One big advance of the performance optimized scenario of SAP HANA is the
possibility to allow read access on the secondary database site. To support
this <span class="strong"><strong>read enabled</strong></span> scenario, a second virtual IP address is added to the cluster
and bound to the secondary role of the system replication.</p></li><li class="listitem "><p><span class="strong"><strong>Cost optimized</strong></span> (<span class="emphasis"><em>A ⇒ B, Q</em></span>). This scenario and setup is described
in another document available from the documentation Web page (<a class="link" href="https://documentation.suse.com/sbp/all/" target="_blank">https://documentation.suse.com/sbp/all/</a>). The document for <span class="emphasis"><em>cost optimized</em></span> is named
<span class="emphasis"><em>"Setting up a SAP HANA SR Cost Optimized Infrastructure"</em></span>.</p><div class="figure" id="id-1.2.2.7.5.2.2"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/SAPHanaSR-ScaleUP-costOpt2.svg" target="_blank"><img src="images/SAPHanaSR-ScaleUP-costOpt2.svg" width="" alt="SAPHanaSR ScaleUP costOpt2" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 4: </span><span class="name">SAP HANA System Replication Scale-Up in the Cluster - cost optimized </span><a title="Permalink" class="permalink" href="#id-1.2.2.7.5.2.2">#</a></h6></div></div><p>In the cost optimized scenario the second node is also used for a
non-productive SAP HANA RDBMS system (like QAS or TST). Whenever a takeover
is needed the non-productive system must be stopped first. As the
productive secondary system on this node must be limited in using system
resources, the table preload must be switched off. A possible
takeover needs longer than in the performance optimized use case.</p><p>In the cost optimized scenario the secondary needs to be running in a reduced
memory consumption configuration. This why <span class="emphasis"><em>read enabled</em></span> must not be used in this
scenario.</p></li><li class="listitem "><p><span class="strong"><strong>Multi Tier</strong></span> (<span class="emphasis"><em>A ⇒ B → C</em></span>) and <span class="strong"><strong>Multi Target</strong></span> (<span class="emphasis"><em>B ⇐ A ⇒ C</em></span>).</p><div class="figure" id="id-1.2.2.7.5.3.2"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/SAPHanaSR-ScaleUP-Chain.svg" target="_blank"><img src="images/SAPHanaSR-ScaleUP-Chain.svg" width="" alt="SAPHanaSR ScaleUP Chain" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 5: </span><span class="name">SAP HANA System Replication Scale-Up in the Cluster - perfomance optimized chain </span><a title="Permalink" class="permalink" href="#id-1.2.2.7.5.3.2">#</a></h6></div></div><p>A <span class="emphasis"><em>Multi Tier</em></span> system replication has an additional target. In the past this third
side must have been connected to the secondary (chain topology). With current SAP HANA
versions also <span class="emphasis"><em>multiple target topology</em></span> is allowed by SAP.</p><div class="figure" id="id-1.2.2.7.5.3.4"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/SAPHanaSR-ScaleUP-MultiTarget.svg" target="_blank"><img src="images/SAPHanaSR-ScaleUP-MultiTarget.svg" width="" alt="SAPHanaSR ScaleUP MultiTarget" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 6: </span><span class="name">SAP HANA System Replication Scale-Up in the Cluster - perfomance optimized multi target </span><a title="Permalink" class="permalink" href="#id-1.2.2.7.5.3.4">#</a></h6></div></div><p>Multi tier and multi target systems are implemented as described in this document.
Only the first replication pair (A and B) is handled by the cluster itself.
The main difference to the plain performance optimized scenario is that the auto
registration must be switched off.</p></li><li class="listitem "><p><span class="strong"><strong>Multi-tenancy</strong></span> or MDC.</p><p>Multi-tenancy is supported for all above scenarios and use cases. This
scenario is supported since SAP HANA SPS09. The setup and configuration
from a cluster point of view is the same for multi-tenancy and single
container. Thus you can use the above documents for both kinds of
scenarios.</p></li></ul></div></div><div class="sect3" id="_the_concept_of_the_performance_optimized_scenario"><div class="titlepage"><div><div><h4 class="title"><span class="number">1.1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">The Concept of the Performance Optimized Scenario</span> <a title="Permalink" class="permalink" href="#_the_concept_of_the_performance_optimized_scenario">#</a></h4></div></div></div><p>In case of failure of the primary SAP HANA on node 1 (node or database
instance) the cluster first tries to start the takeover process. This
allows to use the already loaded data at the secondary site. Typically
the takeover is much faster than the local restart.</p><p>To achieve an automation of this resource handling process, you must
use the SAP HANA resource agents included in SAPHanaSR. System
replication of the productive database is automated with SAPHana and
SAPHanaTopology.</p><p>The cluster only allows a takeover to the secondary site if the SAP HANA
system replication was in sync until the point when the service of the primary
got lost. This ensures that the last commits processed on the primary site are
already available at the secondary site.</p><p>SAP did improve the interfaces between SAP HANA and external software such as
cluster frameworks. These improvements also include the implementation of SAP HANA
call outs in case of special events such as status changes for services or system replication
channels. These calls outs are also called HA/DR providers. This interface can be used by
implementing SAP HANA hooks written in python. SUSE improved the SAPHanaSR package
to include such SAP HANA hooks to optimize the cluster interface. Using the
SAP HANA hook described in this document allows to inform the cluster immediately
if the SAP HANA system replication brakes. In addition to the SAP HANA hook status
the cluster continues to poll the system replication status on regular base.</p><p>You can set up the level of automation by setting the parameter
<code class="literal">AUTOMATED_REGISTER</code>. If automated registration is activated, the cluster
will also automatically register a former failed primary to get the new
secondary.</p><div id="id-1.2.2.8.7" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>The solution is not designed to manually 'migrate' the primary or
secondary instance using HAWK or any other cluster client commands. In the
<span class="emphasis"><em>Administration</em></span> section of this document we describe how to 'migrate' the primary to the secondary site
using SAP and cluster commands.</p></div></div><div class="sect3" id="_customers_receive_complete_package"><div class="titlepage"><div><div><h4 class="title"><span class="number">1.1.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Customers Receive Complete Package</span> <a title="Permalink" class="permalink" href="#_customers_receive_complete_package">#</a></h4></div></div></div><p>Using the SAPHana and SAPHanaTopology resource agents, customers
are be able to integrate SAP HANA system replications in their
cluster. This has the advantage of enabling companies to use not only
their business-critical SAP systems but also their SAP HANA databases
without interruption while noticeably reducing needed budgets. SUSE
provides the extended solution together with best practices
documentation.</p><p>SAP and hardware partners who do not have their own SAP HANA
high availability solution will also benefit from this development from SUSE.</p></div></div><div class="sect2" id="_additional_documentation_and_resources"><div class="titlepage"><div><div><h3 class="title"><span class="number">1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Additional Documentation and Resources</span> <a title="Permalink" class="permalink" href="#_additional_documentation_and_resources">#</a></h3></div></div></div><p>Chapters in this manual contain links to additional documentation
resources that are either available on the system or on the Internet.</p><p>For the latest documentation updates, see
<a class="link" href="http://documentation.suse.com/" target="_blank">http://documentation.suse.com/</a>.</p><p>You can also find numerous white-papers, best-practices, setup guides, and
other resources at the SUSE Linux Enterprise Server for SAP Applications best practices Web page:
<a class="link" href="https://documentation.suse.com/sbp/all/" target="_blank">https://documentation.suse.com/sbp/all/</a>.</p><p>SUSE also publishes blog articles about SAP and high availability.
Join us by using the hashtag #TowardsZeroDowntime. Use the following link:
<a class="link" href="https://www.suse.com/c/tag/TowardsZeroDowntime/" target="_blank">https://www.suse.com/c/tag/TowardsZeroDowntime/</a>.</p></div><div class="sect2" id="_errata"><div class="titlepage"><div><div><h3 class="title"><span class="number">1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Errata</span> <a title="Permalink" class="permalink" href="#_errata">#</a></h3></div></div></div><p>To deliver urgent smaller fixes and important information in a timely manner,
the Technical Information Document (TID) for this setup guide
will be updated, maintained and published at a higher frequency:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>SAP HANA SR Performance Optimized Scenario - Setup Guide - Errata
(<a class="link" href="https://www.suse.com/support/kb/doc/?id=7023882" target="_blank">https://www.suse.com/support/kb/doc/?id=7023882</a>)</p></li><li class="listitem "><p>Showing SOK Status in Cluster Monitoring Tools Workaround
(<a class="link" href="https://www.suse.com/support/kb/doc/?id=7023526" target="_blank">https://www.suse.com/support/kb/doc/?id=7023526</a> -
see also the blog article <a class="link" href="https://www.suse.com/c/lets-flip-the-flags-is-my-sap-hana-database-in-sync-or-not/" target="_blank">https://www.suse.com/c/lets-flip-the-flags-is-my-sap-hana-database-in-sync-or-not/</a>)</p></li></ul></div><p>In addition to this guide, check the SUSE SAP Best Practice Guide Errata for other solutions
(<a class="link" href="https://www.suse.com/support/kb/doc/?id=7023713" target="_blank">https://www.suse.com/support/kb/doc/?id=7023713</a>).</p></div><div class="sect2" id="_feedback"><div class="titlepage"><div><div><h3 class="title"><span class="number">1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Feedback</span> <a title="Permalink" class="permalink" href="#_feedback">#</a></h3></div></div></div><p>Several feedback channels are available:</p><div class="variablelist "><dl class="variablelist"><dt id="id-1.2.5.3.1"><span class="term ">Bugs and Enhancement Requests</span></dt><dd><p>For services and support options available for your product, refer to <a class="link" href="http://www.suse.com/support/" target="_blank">http://www.suse.com/support/</a>.</p></dd></dl></div><p>To report bugs for a product component, go to <a class="link" href="https://scc.suse.com/support/" target="_blank">https://scc.suse.com/support/</a> requests, log in, and select <span class="emphasis"><em>Submit New SR</em></span> (Service Request).</p><div class="variablelist "><dl class="variablelist"><dt id="id-1.2.5.5.1"><span class="term ">Mail</span></dt><dd><p>For feedback on the documentation of this product, you can send a mail to <a class="link" href="mailto:doc-team@suse.com" target="_blank">doc-team@suse.com</a>. Make sure to include the document title, the product version and the publication date of the documentation. To report errors or suggest enhancements, provide a concise description of the problem and refer to the respective section number and page (or URL).</p></dd></dl></div></div></div><div class="sect1" id="cha.hana-sr.scenario"><div class="titlepage"><div><div><h2 class="title"><span class="number">2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Supported Scenarios and Prerequisites</span> <a title="Permalink" class="permalink" href="#cha.hana-sr.scenario">#</a></h2></div></div></div><p>With the <code class="literal">SAPHanaSR</code> resource agent software package, we limit the
support to Scale-Up (single-box to single-box) system replication with
the following configurations and parameters:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Two-node clusters are standard. Three node clusters are fine if you install
the resource agents also on that third node. But define in the cluster that SAP HANA
resources must never run on that third node. In this case the third node is an
additional decision maker in case of cluster separation.</p></li><li class="listitem "><p>The cluster must include a valid STONITH method.</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Any STONITH mechanism supported by SUSE Linux Enterprise 15 High Availability Extension (like SDB, IPMI)
is supported with SAPHanaSR.</p></li><li class="listitem "><p>This guide is focusing on the SBD fencing method as this is hardware
independent.</p></li><li class="listitem "><p>If you use disk-based SBD as the fencing mechanism, you need one or more shared
drives. For productive environments, we recommend more than one SBD
device. For details on disk-based SBD, read the product documentation for
SUSE Linux Enterprise High Availability Extension and the manual pages
sbd.8 and stonith_sbd.8.</p></li><li class="listitem "><p>For disk-less SBD you need at least three cluster nodes. The disk-less SBD mechanism
has the benefit that you do not need a shared drive for fencing.</p></li></ul></div></li><li class="listitem "><p>Both nodes are in the same network segment (layer 2). Similar methods provided
by cloud environments such as overlay IP addresses and load balancer functionality
are also fine. Follow the cloud specific guides to set up your SUSE Linux Enterprise Server for SAP Applications cluster.</p></li><li class="listitem "><p>Technical users and groups, such as <span class="emphasis"><em>&lt;sid&gt;adm</em></span> are defined
locally in the Linux system.</p></li><li class="listitem "><p>Name resolution of the cluster nodes and the virtual IP address must
be done locally on all cluster nodes.</p></li><li class="listitem "><p>Time synchronization between the cluster nodes like NTP.</p></li><li class="listitem "><p>Both SAP HANA instances (primary and secondary) have the same SAP Identifier (SID) and instance
number.</p></li><li class="listitem "><p>If the cluster nodes are installed in different data centers or data
center areas, the environment must match the requirements of the SUSE Linux Enterprise High Availability Extension
cluster product. Of particular concern are the network latency and
recommended maximum distance between the nodes. Review our
product documentation for SUSE Linux Enterprise High Availability Extension about those recommendations.</p></li><li class="listitem "><p>Automated registration of a failed primary after takeover.</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>As a good starting configuration for projects, we recommend to switch
off the automated registration of a failed primary. The setup
<code class="literal">AUTOMATED_REGISTER="false"</code> is the default. In this case, you need to
register a failed primary after a takeover manually. Use SAP tools like
SAP HANA cockpit or <span class="emphasis"><em>hdbnsutil</em></span>.</p></li><li class="listitem "><p>For optimal automation, we recommend <code class="literal">AUTOMATED_REGISTER="true"</code>.</p></li></ul></div></li><li class="listitem "><p>Automated start of SAP HANA instances during system boot must be
switched off.</p></li><li class="listitem "><p>Multi-tenancy (MDC) databases are supported.</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Multi-tenancy databases could be used in combination with any other
setup (performance based, cost optimized and multi-tier).</p></li><li class="listitem "><p>In MDC configurations the SAP HANA RDBMS is treated as a single
system including all database containers. Therefore cluster takeover
decisions are based on the complete RDBMS status independent of the
status of individual database containers.</p></li><li class="listitem "><p>For SAP HANA 1.0 you need version SPS10 rev3, SPS11 or newer if you want to stop
tenants during production and you want the cluster to be able to take over.
Older SAP HANA versions are marking the system replication as failed if
you stop a tenant.</p></li><li class="listitem "><p>Tests on Multi-tenancy databases could force a different test procedure
if you are using strong separation of the tenants. As an example, killing the
complete SAP HANA instance using <span class="emphasis"><em>HDB kill</em></span> does not work, because the tenants are
running with different Linux user UIDs. &lt;sidadm&gt; is not allowed to terminate the
processes of the other tenant users.</p></li></ul></div></li></ul></div><p>You need at least SAPHanaSR version 0.153 and in best SUSE Linux Enterprise Server for SAP Applications 15
SP1 or newer. SAP HANA 1.0 is supported since SPS09 (095) for all mentioned setups.
SAP HANA 2.0 is supported with all known SPS versions.</p><div id="id-1.3.5" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>Without a valid STONITH method, the complete cluster is unsupported
and will not work properly.</p></div><p>If you need to implement a different scenario, we strongly recommend to
define a PoC with SUSE. This PoC will focus on testing the existing
solution in your scenario. Most of the above mentioned limitations are because
careful testing is needed.</p><p>Besides SAP HANA, you need SAP Host Agent to be installed on your system.</p></div><div class="sect1" id="cha.hana-sr.scope"><div class="titlepage"><div><div><h2 class="title"><span class="number">3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Scope of This Documentation</span> <a title="Permalink" class="permalink" href="#cha.hana-sr.scope">#</a></h2></div></div></div><p>This document describes how to set up the cluster to control SAP HANA in
System Replication Scenarios. The document focuses on the steps to
integrate an already installed and working SAP HANA with System
Replication.</p><p>The described example setup builds an SAP HANA HA cluster in two data centers in Walldorf
(WDF) and in Rot (ROT), installed on two SLES for SAP
15 SP1 systems.</p><div class="figure" id="id-1.4.4"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/hana_sr_scaleup_perfopt.svg" target="_blank"><img src="images/hana_sr_scaleup_perfopt.svg" width="" alt="hana sr scaleup perfopt" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 7: </span><span class="name">Cluster with SAP HANA SR - performance optimized </span><a title="Permalink" class="permalink" href="#id-1.4.4">#</a></h6></div></div><p>You can either set up the cluster using the YaST wizard, doing it manually or
using your own automation.</p><p>If you like to use the YaST wizard, you can use the shortcut <span class="emphasis"><em>yast sap_ha</em></span> to
start the module. The procedure to set up SAPHanaSR using YaST is described in
the product documentation of SUSE Linux Enterprise Server for SAP Applications in section <span class="emphasis"><em>Setting Up an SAP HANA Cluster</em></span>
available at:
<a class="link" href="https://documentation.suse.com/sles-sap/15-SP1/single-html/SLES4SAP-guide/#cha-s4s-cluster" target="_blank">https://documentation.suse.com/sles-sap/15-SP1/single-html/SLES4SAP-guide/#cha-s4s-cluster</a></p><div class="figure" id="id-1.4.7"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/Yast_SAP_HA.png" target="_blank"><img src="images/Yast_SAP_HA.png" width="" alt="Yast SAP HA" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 8: </span><span class="name">Scenario Selection for SAP HANA in the YaST Module sap_ha </span><a title="Permalink" class="permalink" href="#id-1.4.7">#</a></h6></div></div><p>This guide focuses on the manual setup of the cluster to explain the details and
to give you the possibility to create your own automation.</p><p>The seven main setup steps are:</p><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/SAPHanaSR-ScaleOut-Plan-Phase0.svg" target="_blank"><img src="images/SAPHanaSR-ScaleOut-Plan-Phase0.svg" width="" alt="SAPHanaSR ScaleOut Plan Phase0" /></a></div></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Planning (see <a class="xref" href="#_planning_the_installation" title="4. Planning the Installation">Section 4, “Planning the Installation”</a>)</p></li><li class="listitem "><p>OS installation (see <a class="xref" href="#_operating_system_setup" title="5. Operating System Setup">Section 5, “Operating System Setup”</a>)</p></li><li class="listitem "><p>Database installation (see <a class="xref" href="#cha.s4s.hana-install" title="6. Installing the SAP HANA Databases on both cluster nodes">Section 6, “Installing the SAP HANA Databases on both cluster nodes”</a>)</p></li><li class="listitem "><p>SAP HANA system replication setup (see <a class="xref" href="#_set_up_sap_hana_system_replication" title="7. Set Up SAP HANA System Replication">Section 7, “Set Up SAP HANA System Replication”</a></p></li><li class="listitem "><p>SAP HANA HA/DR provider hooks (see <a class="xref" href="#_set_up_sap_hana_hadr_providers" title="8. Set Up SAP HANA HA/DR providers">Section 8, “Set Up SAP HANA HA/DR providers”</a>)</p></li><li class="listitem "><p>Cluster configuration (see <a class="xref" href="#_configuration_of_the_cluster" title="9. Configuration of the Cluster">Section 9, “Configuration of the Cluster”</a>)</p></li><li class="listitem "><p>Testing (see <a class="xref" href="#cha.s4s.test-cluster" title="10. Testing the Cluster">Section 10, “Testing the Cluster”</a>)</p></li></ul></div></div><div class="sect1" id="_planning_the_installation"><div class="titlepage"><div><div><h2 class="title"><span class="number">4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Planning the Installation</span> <a title="Permalink" class="permalink" href="#_planning_the_installation">#</a></h2></div></div></div><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/SAPHanaSR-ScaleOut-Plan-Phase1.svg" target="_blank"><img src="images/SAPHanaSR-ScaleOut-Plan-Phase1.svg" width="" alt="SAPHanaSR ScaleOut Plan Phase1" /></a></div></div><p>Planning the installation is essential for a successful SAP HANA cluster setup.</p><p>What you need before you start:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Software from SUSE: SUSE Linux Enterprise Server for SAP Applications installation media, a valid subscription,
and access to update channels</p></li><li class="listitem "><p>Software from SAP: SAP HANA installation media</p></li><li class="listitem "><p>Physical or virtual systems including disks</p></li><li class="listitem "><p>Filled parameter sheet (see below <a class="xref" href="#_parameter_sheet" title="4.2. Parameter Sheet">Section 4.2, “Parameter Sheet”</a>)</p></li></ul></div><div class="sect2" id="_minimum_lab_requirements_and_prerequisites"><div class="titlepage"><div><div><h3 class="title"><span class="number">4.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Minimum Lab Requirements and Prerequisites</span> <a title="Permalink" class="permalink" href="#_minimum_lab_requirements_and_prerequisites">#</a></h3></div></div></div><div id="id-1.5.6.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>The minimum lab requirements mentioned here are no SAP sizing information.
These data are provided only to rebuild the described cluster in a lab for test purposes.
Even for such tests the requirements can increase depending on your test scenario.
For productive systems ask your hardware vendor or use the official SAP sizing
tools and services.</p></div><div id="id-1.5.6.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>Refer to SAP HANA TDI documentation for allowed storage
configuration and file systems.</p></div><p>Requirements with 1 SAP instance per site (1 : 1) - without a
majority maker (2 node cluster):</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>2 VMs with each 32GB RAM, 50GB disk space for the system</p></li><li class="listitem "><p>1 shared disk for SBD with 10 MB disk space</p></li><li class="listitem "><p>2 data disks (one per site) with a capacity of each 96GB for SAP HANA</p></li><li class="listitem "><p>1 additional IP address for takeover</p></li><li class="listitem "><p>1 optional IP address for the read-enabled setup</p></li><li class="listitem "><p>1 optional IP address for HAWK Administration GUI</p></li></ul></div><p>Requirements with 1 SAP instance per site (1 : 1) - with a
majority maker (3 node cluster):</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>2 VMs with each 32GB RAM, 50GB disk space for the system</p></li><li class="listitem "><p>1 VM with 2GB RAM, 50GB disk space for the system</p></li><li class="listitem "><p>2 data disks (one per site) with a capacity of each 96GB for SAP HANA</p></li><li class="listitem "><p>1 additional IP address for takeover</p></li><li class="listitem "><p>1 optional IP address for the read-enabled setup</p></li><li class="listitem "><p>1 optional IP address for HAWK Administration GUI</p></li></ul></div></div><div class="sect2" id="_parameter_sheet"><div class="titlepage"><div><div><h3 class="title"><span class="number">4.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Parameter Sheet</span> <a title="Permalink" class="permalink" href="#_parameter_sheet">#</a></h3></div></div></div><p>Even if the setup of the cluster organizing two SAP HANA sites is quite simple, the installation
should be planned properly. You should have all needed parameters like SID, IP
addresses and much more in place. It is good practice to first fill out
the parameter sheet and then begin with the installation.</p><div class="table" id="id-1.5.7.3"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 1: </span><span class="name">Parameter Sheet for Planning </span><a title="Permalink" class="permalink" href="#id-1.5.7.3">#</a></h6></div><div class="table-contents"><table class="table" summary="Parameter Sheet for Planning" border="1" width="100%"><colgroup><col class="col_1" /><col class="col_2" /><col class="col_3" /></colgroup><thead><tr><th align="left" valign="top">Parameter</th><th align="left" valign="top">Value</th><th align="left" valign="top">Role</th></tr></thead><tbody><tr><td align="left" valign="top"><p>node 1</p></td><td align="left" valign="top"> </td><td align="left" valign="top"><p>Cluster node name and IP address.</p></td></tr><tr><td align="left" valign="top"><p>node 2</p></td><td align="left" valign="top"> </td><td align="left" valign="top"><p>Cluster node name and IP address.</p></td></tr><tr><td align="left" valign="top"><p>SID</p></td><td align="left" valign="top"> </td><td align="left" valign="top"><p>SAP System Identifier</p></td></tr><tr><td align="left" valign="top"><p>Instance Number</p></td><td align="left" valign="top"> </td><td align="left" valign="top"><p>Number of the SAP HANA database. For
system replication also Instance Number+1 is blocked.</p></td></tr><tr><td align="left" valign="top"><p>Network mask</p></td><td align="left" valign="top"> </td><td align="left" valign="top"> </td></tr><tr><td align="left" valign="top"><p>vIP primary</p></td><td align="left" valign="top"> </td><td align="left" valign="top"><p>Virtual IP address to be assigned to
the primary SAP HANA site</p></td></tr><tr><td align="left" valign="top"><p>vIP secondary</p></td><td align="left" valign="top"> </td><td align="left" valign="top"><p>Virtual IP address to be assigned to
the read-enabled secondary SAP HANA site (optional)</p></td></tr><tr><td align="left" valign="top"><p>Storage</p></td><td align="left" valign="top"> </td><td align="left" valign="top"><p>Storage for HDB data and log files is connected “locally”
(per node; not shared)</p></td></tr><tr><td align="left" valign="top"><p>SBD</p></td><td align="left" valign="top"> </td><td align="left" valign="top"><p>STONITH device (two for production)</p></td></tr><tr><td align="left" valign="top"><p>HAWK Port</p></td><td align="left" valign="top"><p><code class="literal">7630</code></p></td><td align="left" valign="top"> </td></tr><tr><td align="left" valign="top"><p>NTP Server</p></td><td align="left" valign="top"> </td><td align="left" valign="top"><p>Address or name of your time server</p></td></tr></tbody></table></div></div><div class="table" id="id-1.5.7.4"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 2: </span><span class="name">Parameter Sheet with Values used in this Document </span><a title="Permalink" class="permalink" href="#id-1.5.7.4">#</a></h6></div><div class="table-contents"><table class="table" summary="Parameter Sheet with Values used in this Document" border="1" width="100%"><colgroup><col class="col_1" /><col class="col_2" /><col class="col_3" /></colgroup><thead><tr><th align="left" valign="top">Parameter</th><th align="left" valign="top">Value</th><th align="left" valign="top">Role</th></tr></thead><tbody><tr><td align="left" valign="top"><p>node 1</p></td><td align="left" valign="top"><p><code class="literal">suse01</code>, <code class="literal">192.168.1.11</code></p></td><td align="left" valign="top"><p>Cluster node name and
IP address.</p></td></tr><tr><td align="left" valign="top"><p>node 2</p></td><td align="left" valign="top"><p><code class="literal">suse02</code>, <code class="literal">192.168.1.12</code></p></td><td align="left" valign="top"><p>Cluster node name and
IP address.</p></td></tr><tr><td align="left" valign="top"><p>SID</p></td><td align="left" valign="top"><p><code class="literal">HA1</code></p></td><td align="left" valign="top"><p>SAP System Identifier</p></td></tr><tr><td align="left" valign="top"><p>Instance Number</p></td><td align="left" valign="top"><p><code class="literal">10</code></p></td><td align="left" valign="top"><p>Number of the SAP HANA database. For
system replication also Instance Number+1 is blocked.</p></td></tr><tr><td align="left" valign="top"><p>Network mask</p></td><td align="left" valign="top"><p><code class="literal">255.255.255.0</code></p></td><td align="left" valign="top"> </td></tr><tr><td align="left" valign="top"><p>vIP primary</p></td><td align="left" valign="top"><p><code class="literal">192.168.1.20</code></p></td><td align="left" valign="top"> </td></tr><tr><td align="left" valign="top"><p>vIP secondary</p></td><td align="left" valign="top"><p><code class="literal">192.168.1.21</code></p></td><td align="left" valign="top"><p>(optional)</p></td></tr><tr><td align="left" valign="top"><p>Storage</p></td><td align="left" valign="top"> </td><td align="left" valign="top"><p>Storage for HDB data and log files is connected “locally”
(per node; not shared)</p></td></tr><tr><td align="left" valign="top"><p>SBD</p></td><td align="left" valign="top"><p><code class="literal">/dev/disk/by-id/SBDA</code></p></td><td align="left" valign="top"><p>STONITH device (two for production)</p></td></tr><tr><td align="left" valign="top"><p>HAWK Port</p></td><td align="left" valign="top"><p><code class="literal">7630</code></p></td><td align="left" valign="top"> </td></tr><tr><td align="left" valign="top"><p>NTP Server</p></td><td align="left" valign="top"><p>pool pool.ntp.org</p></td><td align="left" valign="top"><p>Address or name of your time server</p></td></tr></tbody></table></div></div></div></div><div class="sect1" id="_operating_system_setup"><div class="titlepage"><div><div><h2 class="title"><span class="number">5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Operating System Setup</span> <a title="Permalink" class="permalink" href="#_operating_system_setup">#</a></h2></div></div></div><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/SAPHanaSR-ScaleOut-Plan-Phase2.svg" target="_blank"><img src="images/SAPHanaSR-ScaleOut-Plan-Phase2.svg" width="" alt="SAPHanaSR ScaleOut Plan Phase2" /></a></div></div><p>This section contains information you should consider during the
installation of the operating system.</p><p>For the scope of this document, first SUSE Linux Enterprise Server for SAP Applications is installed and configured. Then the SAP HANA
database including the system replication is set up. Finally the
automation with the cluster is set up and configured.</p><div class="sect2" id="_installing_suse_linux_enterprise_server_for_sap_applications"><div class="titlepage"><div><div><h3 class="title"><span class="number">5.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installing SUSE Linux Enterprise Server for SAP Applications</span> <a title="Permalink" class="permalink" href="#_installing_suse_linux_enterprise_server_for_sap_applications">#</a></h3></div></div></div><p>Multiple installation guides are already existing, with different reasons to set
up the server in a certain way. Below it is outlined where this information can
be found. In addition, you will find important details you should consider to get
a well-working system.</p><div class="sect3" id="_installing_base_operating_system"><div class="titlepage"><div><div><h4 class="title"><span class="number">5.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installing Base Operating System</span> <a title="Permalink" class="permalink" href="#_installing_base_operating_system">#</a></h4></div></div></div><p>Depending on your infrastructure and the hardware used, you need to adapt the installation.
All supported installation methods and minimum requirement are described in the <span class="emphasis"><em>Deployment Guide</em></span>
(<a class="link" href="https://documentation.suse.com/sles/15-SP1/html/SLES-all/book-sle-deployment.html" target="_blank">https://documentation.suse.com/sles/15-SP1/html/SLES-all/book-sle-deployment.html</a>).
In case of automated installations you can find further information in the <span class="emphasis"><em>AutoYaST Guide</em></span>
(<a class="link" href="https://documentation.suse.com/sles/15-SP1/html/SLES-all/book-autoyast.html" target="_blank">https://documentation.suse.com/sles/15-SP1/html/SLES-all/book-autoyast.html</a>).
The main installation guides for SUSE Linux Enterprise Server for SAP Applications that fit all requirements for SAP HANA are available from the
SAP notes:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>2578899 SUSE LINUX Enterprise Server 15: Installation Note and</p></li><li class="listitem "><p>2684254 SAP HANA DB: Recommended OS settings for SLES 15 / SLES for SAP Applications 15.</p></li></ul></div></div><div class="sect3" id="_installing_additional_software"><div class="titlepage"><div><div><h4 class="title"><span class="number">5.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installing Additional Software</span> <a title="Permalink" class="permalink" href="#_installing_additional_software">#</a></h4></div></div></div><p>SUSE delivers with SUSE Linux Enterprise Server for SAP Applications special resource agents for SAP HANA. With the pattern <span class="emphasis"><em>sap-hana</em></span> the resource
agent for SAP HANA <span class="strong"><strong>scale-up</strong></span> is installed. For the <span class="strong"><strong>scale-out</strong></span> scenario you need a special resource agent.
Follow the instructions below on each node if you have installed the systems based on SAP note 1984787.
The pattern <span class="emphasis"><em>High Availability</em></span> summarizes all tools recommended to be installed on <span class="strong"><strong>all</strong></span> nodes, including the
<span class="emphasis"><em>majority maker</em></span>.</p><div class="complex-example"><div class="example" id="id-1.6.5.4.3"><div class="example-title-wrap"><h6 class="example-title"><span class="number">Example 1: </span><span class="name">Installing additional software for the HA cluster </span><a title="Permalink" class="permalink" href="#id-1.6.5.4.3">#</a></h6></div><div class="example-contents"><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Install the <code class="literal">High Availability</code> pattern on all nodes</p><div class="verbatim-wrap"><pre class="screen">suse01:~&gt; zypper in --type pattern ha_sles</pre></div></li><li class="listitem "><p>Install the <code class="literal">SAPHanaSR</code> resource agents on all nodes</p><div class="verbatim-wrap"><pre class="screen">suse01:~&gt; zypper in SAPHanaSR SAPHanaSR-doc</pre></div></li></ol></div></div></div></div><p>For more information, see <span class="emphasis"><em>Installation and Basic Setup</em></span>, SUSE
Linux Enterprise High Availability Extension.</p></div></div></div><div class="sect1" id="cha.s4s.hana-install"><div class="titlepage"><div><div><h2 class="title"><span class="number">6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installing the SAP HANA Databases on both cluster nodes</span> <a title="Permalink" class="permalink" href="#cha.s4s.hana-install">#</a></h2></div></div></div><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/SAPHanaSR-ScaleOut-Plan-Phase3.svg" target="_blank"><img src="images/SAPHanaSR-ScaleOut-Plan-Phase3.svg" width="" alt="SAPHanaSR ScaleOut Plan Phase3" /></a></div></div><p>Even though this document focuses on the integration of an
installed SAP HANA with system replication already set up into the
pacemaker cluster, this chapter summarizes the test environment.
Always use the official documentation from SAP to install SAP HANA and to
set up the system replication.</p><div class="sect2" id="_installing_the_sap_hana_databases_on_both_cluster_nodes"><div class="titlepage"><div><div><h3 class="title"><span class="number">6.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Installing the SAP HANA Databases on both cluster nodes</span> <a title="Permalink" class="permalink" href="#_installing_the_sap_hana_databases_on_both_cluster_nodes">#</a></h3></div></div></div><div class="itemizedlist "><div class="itemizedlist-title-wrap"><h6 class="itemizedlist-title"><span class="name">Preparation </span><a title="Permalink" class="permalink" href="#id-1.7.4.2">#</a></h6></div><ul class="itemizedlist"><li class="listitem "><p>Read the SAP Installation and Setup Manuals available at the SAP Marketplace.</p></li><li class="listitem "><p>Download the SAP HANA Software from SAP Marketplace.</p></li></ul></div><div class="orderedlist "><div class="orderedlist-title-wrap"><h6 class="orderedlist-title"><span class="name">Actions </span><a title="Permalink" class="permalink" href="#id-1.7.4.3">#</a></h6></div><ol class="orderedlist" type="1"><li class="listitem "><p>Install the SAP HANA Database as described in the SAP HANA Server
Installation Guide.</p></li><li class="listitem "><p>Check if the SAP Host Agent is installed on all cluster nodes. If this
SAP service is not installed, install it now.</p></li><li class="listitem "><p>Verify that both databases are up and all processes of these databases
are running correctly.</p></li></ol></div><p>As Linux user <span class="emphasis"><em>&lt;sid&gt;adm</em></span> use the command line tool <span class="emphasis"><em>HDB</em></span> to get an
overview of running HANA processes. The output of <code class="literal">HDB</code> <code class="literal">info</code> should
be similar to the output shown below:</p><div class="verbatim-wrap"><pre class="screen">suse02:ha1adm&gt; HDB info
USER          PID     PPID  ... COMMAND
ha1adm      13017    ... -sh
ha1adm      13072    ...  \_ /bin/sh /usr/sap/HA1/HDB10/HDB info
ha1adm      13103    ...      \_ ps fx -U ha1adm -o user:8,pid:8,ppid:8,pcpu:5,vsz:10,rss:10,args
ha1adm       9268    ... hdbrsutil  --start --port 31003 --volume 2 --volumesuffix mnt00001/hdb00002.00003 --identifier 1580897137
ha1adm       8911    ... hdbrsutil  --start --port 31001 --volume 1 --volumesuffix mnt00001/hdb00001 --identifier 1580897100
ha1adm       8729    ... sapstart pf=/hana/shared/HA1/profile/HA1_HDB10_suse02
ha1adm       8738    ...  \_ /usr/sap/HA1/HDB10/suse02/trace/hdb.sapHA1_HDB10 -d -nw -f /usr/sap/HA1/HDB10/suse02/daemon.ini pf=/usr/sap/HA1/SYS/profile/HA1_HDB10_suse02
ha1adm       8756    ...      \_ hdbnameserver
ha1adm       9031    ...      \_ hdbcompileserver
ha1adm       9034    ...      \_ hdbpreprocessor
ha1adm       9081    ...      \_ hdbindexserver -port 31003
ha1adm       9084    ...      \_ hdbxsengine -port 31007
ha1adm       9531    ...      \_ hdbwebdispatcher
ha1adm       8574    ... /usr/sap/HA1/HDB10/exe/sapstartsrv pf=/hana/shared/HA1/profile/HA1_HDB10_suse02 -D -u ha1adm</pre></div></div></div><div class="sect1" id="_set_up_sap_hana_system_replication"><div class="titlepage"><div><div><h2 class="title"><span class="number">7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Set Up SAP HANA System Replication</span> <a title="Permalink" class="permalink" href="#_set_up_sap_hana_system_replication">#</a></h2></div></div></div><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/SAPHanaSR-ScaleOut-Plan-Phase4.svg" target="_blank"><img src="images/SAPHanaSR-ScaleOut-Plan-Phase4.svg" width="" alt="SAPHanaSR ScaleOut Plan Phase4" /></a></div></div><p>For more information read the section <span class="emphasis"><em>Setting Up System Replication</em></span> of
the SAP HANA Administration Guide.</p><p><span class="strong"><strong>Procedure</strong></span></p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Back up the primary database</p></li><li class="listitem "><p>Enable primary database</p></li><li class="listitem "><p>Register the secondary database</p></li><li class="listitem "><p>Verify the system replication</p></li></ol></div><div class="sect2" id="_back_up_the_primary_database"><div class="titlepage"><div><div><h3 class="title"><span class="number">7.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Back Up the Primary Database</span> <a title="Permalink" class="permalink" href="#_back_up_the_primary_database">#</a></h3></div></div></div><p>Back up the primary database as described in the SAP HANA Administration
Guide, section <span class="emphasis"><em>SAP HANA Database Backup and Recovery</em></span>. We provide an
example with SQL commands. You need to adapt these backup commands to match your
backup infrastructure.</p><div class="complex-example"><div class="example" id="id-1.8.6.3"><div class="example-title-wrap"><h6 class="example-title"><span class="number">Example 2: </span><span class="name">Simple backup for the system database and all tenants with one singe backup call </span><a title="Permalink" class="permalink" href="#id-1.8.6.3">#</a></h6></div><div class="example-contents"><p>As user &lt;sidadm&gt; enter the following command:</p><div class="verbatim-wrap"><pre class="screen">hdbsql -u SYSTEM -d SYSTEMDB \
   "BACKUP DATA FOR FULL SYSTEM USING FILE ('backup')"</pre></div><p>You get the following command output (or similar):</p><div class="verbatim-wrap"><pre class="screen">0 rows affected (overall time 15.352069 sec; server time 15.347745 sec)</pre></div></div></div></div><div class="complex-example"><div class="example" id="id-1.8.6.4"><div class="example-title-wrap"><h6 class="example-title"><span class="number">Example 3: </span><span class="name">Simple backup for a single container (non MDC) database </span><a title="Permalink" class="permalink" href="#id-1.8.6.4">#</a></h6></div><div class="example-contents"><p>Enter the following command as user &lt;sidadm&gt;:</p><div class="verbatim-wrap"><pre class="screen">hdbsql -i &lt;instanceNumber&gt; -u &lt;dbuser&gt; \
   "BACKUP DATA USING FILE ('backup')"</pre></div></div></div></div><div id="id-1.8.6.5" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>Without a valid backup, you cannot bring SAP HANA into a system
replication configuration.</p></div></div><div class="sect2" id="_enable_primary_node"><div class="titlepage"><div><div><h3 class="title"><span class="number">7.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Enable Primary Node</span> <a title="Permalink" class="permalink" href="#_enable_primary_node">#</a></h3></div></div></div><p>As Linux user <span class="emphasis"><em>&lt;sid&gt;adm</em></span> enable the system replication at the
primary node. You need to define a site name (like WDF). This
site name must be unique for all SAP HANA databases which are connected
via system replication. This means the secondary must have a different
site name.</p><div id="id-1.8.7.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>Do not use strings like "primary" and "secondary" as site names.</p></div><div class="complex-example"><div class="example" id="id-1.8.7.4"><div class="example-title-wrap"><h6 class="example-title"><span class="number">Example 4: </span><span class="name">Enable the Primary </span><a title="Permalink" class="permalink" href="#id-1.8.7.4">#</a></h6></div><div class="example-contents"><p>Enable the primary using the -sr_enable option.</p><div class="verbatim-wrap"><pre class="screen">suse01:~&gt; hdbnsutil -sr_enable --name=WDF
checking local nameserver:
checking for active nameserver ...
nameserver is running, proceeding ...
configuring ini files ...
successfully enabled system as primary site ...
done.</pre></div></div></div></div><div class="complex-example"><div class="example" id="id-1.8.7.5"><div class="example-title-wrap"><h6 class="example-title"><span class="number">Example 5: </span><span class="name">Check SR Configuration on the Primary </span><a title="Permalink" class="permalink" href="#id-1.8.7.5">#</a></h6></div><div class="example-contents"><p>Check the primary using the command <code class="literal">hdbnsutil -sr_stateConfiguration</code>.</p><div class="verbatim-wrap"><pre class="screen">suse01:~&gt; hdbnsutil -sr_stateConfiguration --sapcontrol=1
SAPCONTROL-OK: &lt;begin&gt;
mode=primary
site id=1
site name=WDF
SAPCONTROL-OK: &lt;end&gt;
done.</pre></div></div></div></div><p>The mode has changed from “none” to “primary” and the site now has a
site name and a site ID.</p></div><div class="sect2" id="_register_the_secondary_node"><div class="titlepage"><div><div><h3 class="title"><span class="number">7.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Register the Secondary Node</span> <a title="Permalink" class="permalink" href="#_register_the_secondary_node">#</a></h3></div></div></div><p>The SAP HANA database instance on the secondary side must be stopped
before the instance can be registered for the system replication. You
can use your preferred method to stop the instance (like <code class="literal">HDB</code> or
<code class="literal">sapcontrol</code>). After the database instance has been stopped
successfully, you can register the instance using <code class="literal">hdbnsutil</code>. Again,
use Linux user <span class="emphasis"><em>&lt;sid&gt;adm</em></span>:</p><div class="complex-example"><div class="example" id="id-1.8.8.3"><div class="example-title-wrap"><h6 class="example-title"><span class="number">Example 6: </span><span class="name">Stop the Secondary </span><a title="Permalink" class="permalink" href="#id-1.8.8.3">#</a></h6></div><div class="example-contents"><p>To stop the secondary you can use the command line tool <span class="emphasis"><em>HDB</em></span>.</p><div class="verbatim-wrap"><pre class="screen">suse02:~&gt; HDB stop</pre></div></div></div></div><div class="complex-example"><div class="example" id="id-1.8.8.4"><div class="example-title-wrap"><h6 class="example-title"><span class="number">Example 7: </span><span class="name">Copy the KEY and KEY-DATA file from the primary to the secondary site </span><a title="Permalink" class="permalink" href="#id-1.8.8.4">#</a></h6></div><div class="example-contents"><p>Beginning with SAP HANA 2.0 the system replication is running encrypted. This
is why the key files needs to copied-over from the primary to the secondary
site.</p><div class="verbatim-wrap"><pre class="screen">cd /usr/sap/&lt;SID&gt;/SYS/global/security/rsecssfs
rsync -va {,&lt;node1-siteB&gt;:}$PWD/data/SSFS_&lt;SID&gt;.DAT
rsync -va {,&lt;node1-siteB&gt;:}$PWD/key/SSFS_&lt;SID&gt;.KEY</pre></div></div></div></div><div class="complex-example"><div class="example" id="id-1.8.8.5"><div class="example-title-wrap"><h6 class="example-title"><span class="number">Example 8: </span><span class="name">Register the Secondary </span><a title="Permalink" class="permalink" href="#id-1.8.8.5">#</a></h6></div><div class="example-contents"><p>The registration of the secondary is triggered by calling <span class="emphasis"><em>hdbnsutil -sr_register …​</em></span>.</p><div class="verbatim-wrap"><pre class="screen">...
suse02:~&gt; hdbnsutil -sr_register --name=ROT \
     --remoteHost=suse01 --remoteInstance=10 \
     --replicationMode=sync --operationMode=logreplay
adding site ...
checking for inactive nameserver ...
nameserver suse02:30001 not responding.
collecting information ...
updating local ini files ...
done.</pre></div></div></div></div><p>The <span class="emphasis"><em>remoteHost</em></span> is the primary node in our case, the <span class="emphasis"><em>remoteInstance</em></span> is
the database instance number (here 10).</p><p>Now start the database instance again and verify the system replication
status. On the secondary node, the mode should be one of "SYNC" or
"SYNCMEM". "ASYNC" is also a possible replication mode <span class="strong"><strong>but not supported with
automated cluster takeover</strong></span>. The mode depends on the <span class="strong"><strong>sync</strong></span> option defined during
the registration of the secondary.</p><div class="complex-example"><div class="example" id="id-1.8.8.8"><div class="example-title-wrap"><h6 class="example-title"><span class="number">Example 9: </span><span class="name">Start Secondary and Check SR Configuration </span><a title="Permalink" class="permalink" href="#id-1.8.8.8">#</a></h6></div><div class="example-contents"><p>To start the new secondary use the command line tool <span class="emphasis"><em>HDB</em></span>. Then check the
SR configuration using <code class="literal">hdbnsutil -sr_stateConfiguration</code>.</p><div class="verbatim-wrap"><pre class="screen">suse02:~&gt; HDB start
...
suse02:~&gt; hdbnsutil -sr_stateConfiguration --sapcontrol=1
SAPCONTROL-OK: &lt;begin&gt;
mode=sync
site id=2
site name=ROT
active primary site=1
primary masters=suse01
SAPCONTROL-OK: &lt;end&gt;
done.</pre></div></div></div></div><p>To view the replication state of the whole SAP HANA cluster use the
following command as <span class="emphasis"><em>&lt;sid&gt;adm</em></span> user on the primary node.</p><div class="complex-example"><div class="example" id="id-1.8.8.10"><div class="example-title-wrap"><h6 class="example-title"><span class="number">Example 10: </span><span class="name">Checking System Replication Status Details </span><a title="Permalink" class="permalink" href="#id-1.8.8.10">#</a></h6></div><div class="example-contents"><p>The python script <span class="emphasis"><em>systemReplicationStatus.py</em></span> provides details about the current
system replication.</p><div class="verbatim-wrap"><pre class="screen">suse01:~&gt; HDBSettings.sh systemReplicationStatus.py --sapcontrol=1
...
site/2/SITE_NAME=ROT1
site/2/SOURCE_SITE_ID=1
site/2/REPLICATION_MODE=SYNC
site/2/REPLICATION_STATUS=ACTIVE
site/1/REPLICATION_MODE=PRIMARY
site/1/SITE_NAME=WDF1
local_site_id=1
...</pre></div></div></div></div></div><div class="sect2" id="_manual_test_of_sap_hana_sr_takeover"><div class="titlepage"><div><div><h3 class="title"><span class="number">7.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Manual Test of SAP HANA SR Takeover</span> <a title="Permalink" class="permalink" href="#_manual_test_of_sap_hana_sr_takeover">#</a></h3></div></div></div><p>Before you integrate your SAP HANA system replication into the cluster
it is mandatory to do a manual takeover. Testing without the cluster helps to make sure that
basic operation (takeover and registration) is working as expected.</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Stop SAP HANA on node 1</p></li><li class="listitem "><p>Takeover SAP HANA to node 2</p></li><li class="listitem "><p>Register node 1 as secondary</p></li><li class="listitem "><p>Start SAP HANA on node 1</p></li><li class="listitem "><p>Wait till sync state is active</p></li></ul></div></div><div class="sect2" id="_optional_manually_re_establishing_of_sap_hana_sr_to_original_state"><div class="titlepage"><div><div><h3 class="title"><span class="number">7.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Optional: Manually Re-Establishing of SAP HANA SR to Original State</span> <a title="Permalink" class="permalink" href="#_optional_manually_re_establishing_of_sap_hana_sr_to_original_state">#</a></h3></div></div></div><p>Bring the systems back to the original state:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Stop SAP HANA on node 2</p></li><li class="listitem "><p>Takeover SAP HANA to node 1</p></li><li class="listitem "><p>Register node 2 as secondary</p></li><li class="listitem "><p>Start SAP HANA on node2</p></li><li class="listitem "><p>Wait until sync state is active</p></li></ul></div></div></div><div class="sect1" id="_set_up_sap_hana_hadr_providers"><div class="titlepage"><div><div><h2 class="title"><span class="number">8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Set Up SAP HANA HA/DR providers</span> <a title="Permalink" class="permalink" href="#_set_up_sap_hana_hadr_providers">#</a></h2></div></div></div><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/SAPHanaSR-ScaleOut-Plan-Phase5.svg" target="_blank"><img src="images/SAPHanaSR-ScaleOut-Plan-Phase5.svg" width="" alt="SAPHanaSR ScaleOut Plan Phase5" /></a></div></div><p>This step is mandatory to inform the cluster immediately if the secondary gets out of sync.
The hook is called by SAP HANA using the HA/DR provider interface in point-of-time when the secondary gets out of
sync. This is typically the case when the first commit pending is released. The hook is
called by SAP HANA again when the system replication is back.</p><p><span class="strong"><strong>Procedure</strong></span></p><div class="orderedlist "><ol class="orderedlist" type="1"><li class="listitem "><p>Implement the python hook SAPHanaSR</p></li><li class="listitem "><p>Configure system replication operation mode</p></li><li class="listitem "><p>Allow &lt;sidadm&gt; to access the cluster</p></li><li class="listitem "><p>Start SAP HANA</p></li><li class="listitem "><p>Test the hook integration</p></li></ol></div><div class="sect2" id="_implementing_the_python_hook_saphanasr"><div class="titlepage"><div><div><h3 class="title"><span class="number">8.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Implementing the Python Hook SAPHanaSR</span> <a title="Permalink" class="permalink" href="#_implementing_the_python_hook_saphanasr">#</a></h3></div></div></div><p>This step must be done on both sites. SAP HANA must be stopped to change the
global.ini and allow SAP HANA to integrate the HA/DR hook script during start.</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Install the HA/DR hook script into a read/writable directory</p></li><li class="listitem "><p>Integrate the hook into global.ini (SAP HANA needs to be stopped for doing that offline)</p></li><li class="listitem "><p>Check integration of the hook during start-up</p></li></ul></div><p>Use the hook from the SAPHanaSR package (available since version 0.153). Optionally copy it to your preferred
directory like /hana/share/myHooks. The hook must be available on all SAP HANA cluster nodes.</p><div class="complex-example"><div class="example" id="id-1.9.6.5"><div class="example-title-wrap"><h6 class="example-title"><span class="number">Example 11: </span><span class="name">Stop SAP HANA </span><a title="Permalink" class="permalink" href="#id-1.9.6.5">#</a></h6></div><div class="example-contents"><p>Stop SAP HANA either with <span class="emphasis"><em>HDB</em></span> or using <span class="emphasis"><em>sapcontrol</em></span>.</p><div class="verbatim-wrap"><pre class="screen">sapcontrol -nr &lt;instanceNumber&gt; -function StopSystem</pre></div></div></div></div><div class="example" id="id-1.9.6.6"><div class="example-title-wrap"><h6 class="example-title"><span class="number">Example 12: </span><span class="name">Adding SAPHanaSR via global.ini </span><a title="Permalink" class="permalink" href="#id-1.9.6.6">#</a></h6></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen">[ha_dr_provider_SAPHanaSR]
provider = SAPHanaSR
path = /usr/share/SAPHanaSR
execution_order = 1

[trace]
ha_dr_saphanasr = info</pre></div></div></div></div><div class="sect2" id="_configuring_system_replication_operation_mode"><div class="titlepage"><div><div><h3 class="title"><span class="number">8.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring System Replication Operation Mode</span> <a title="Permalink" class="permalink" href="#_configuring_system_replication_operation_mode">#</a></h3></div></div></div><p>When your system is connected as an SAPHanaSR target you can find an entry in the <span class="emphasis"><em>global.ini</em></span>
which defines the operation mode. Up to now there are the following modes available.</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p><span class="emphasis"><em>delta_datashipping</em></span></p></li><li class="listitem "><p><span class="emphasis"><em>logreplay</em></span></p></li><li class="listitem "><p><span class="emphasis"><em>logreplay_readaccess</em></span></p></li></ul></div><p>Until a takeover and re-registration in the opposite direction, the entry for the operation mode is missing on
your primary site. The first operation mode which was available was <span class="emphasis"><em>delta_datashipping</em></span>. Today the preferred modes for HA are
<span class="emphasis"><em>logreplay</em></span> or <span class="emphasis"><em>logreplay_readaccess</em></span>. Using the operation mode <span class="emphasis"><em>logreplay</em></span> makes your secondary site in the SAP HANA
system replication a hot standby system.
For more details regarding all operation modes check the available SAP documentation such as
"How To Perform System Replication for SAP HANA ".</p><div class="complex-example"><div class="example" id="id-1.9.7.5"><div class="example-title-wrap"><h6 class="example-title"><span class="number">Example 13: </span><span class="name">Checking the Operation Mode </span><a title="Permalink" class="permalink" href="#id-1.9.7.5">#</a></h6></div><div class="example-contents"><p>Check both <span class="emphasis"><em>global.ini</em></span> files and add the operation mode if needed.</p><div class="variablelist "><dl class="variablelist"><dt id="id-1.9.7.5.3.1"><span class="term ">section</span></dt><dd><p>[ system_replication ]</p></dd><dt id="id-1.9.7.5.3.2"><span class="term ">entry</span></dt><dd><p>operation_mode = logreplay</p></dd></dl></div><p>Path for the <span class="emphasis"><em>global.ini</em></span>: /hana/shared/&lt;SID&gt;/global/hdb/custom/config/</p><div class="verbatim-wrap"><pre class="screen">[system_replication]
operation_mode = logreplay</pre></div></div></div></div></div><div class="sect2" id="_allowing_sidadm_to_access_the_cluster"><div class="titlepage"><div><div><h3 class="title"><span class="number">8.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Allowing &lt;sidadm&gt; to access the Cluster</span> <a title="Permalink" class="permalink" href="#_allowing_sidadm_to_access_the_cluster">#</a></h3></div></div></div><p>The current version of the SAPHanaSR python hook uses the command <code class="literal">sudo</code> to allow
the &lt;sidadm&gt; user to access the cluster attributes. In Linux you can use <code class="literal">visudo</code>
to start the vi editor for the <span class="emphasis"><em>/etc/sudoers</em></span> configuration file.</p><p>The user &lt;sidadm&gt; must be able to set the cluster attributes hana_&lt;sid&gt;_site_srHook_*.
The SAP HANA system replication hook needs password free access. The following
example limits the sudo access to exactly setting the needed attribute.</p><p>Replace the &lt;sid&gt; by the <span class="strong"><strong>lowercase</strong></span> SAP system ID (like <code class="literal">ha1</code>).</p><div class="complex-example"><div class="example" id="id-1.9.8.5"><div class="example-title-wrap"><h6 class="example-title"><span class="number">Example 14: </span><span class="name">Entry in sudo permissions /etc/sudoers file </span><a title="Permalink" class="permalink" href="#id-1.9.8.5">#</a></h6></div><div class="example-contents"><p>Basic sudoers entry to allow &lt;sidadm&gt; to use the srHook.</p><div class="verbatim-wrap"><pre class="screen"># SAPHanaSR-ScaleUp entries for writing srHook cluster attribute
&lt;sidadm&gt; ALL=(ALL) NOPASSWD: /usr/sbin/crm_attribute -n hana_&lt;sid&gt;_site_srHook_*</pre></div><p>More specific sudoers entries to meet a high security level. All Cmnd_Alias entries must be each defined as a single line entry.
In the following example the lines might include a line-break forced by document formatting. In our example we would have
four separate lines with Cmnd_Alias entries, one line for the &lt;sidadm&gt; user and one or more lines for comments.</p><div class="verbatim-wrap"><pre class="screen"># SAPHanaSR-ScaleUp entries for writing srHook cluster attribute
Cmnd_Alias SOK_SITEA   = /usr/sbin/crm_attribute -n hana_&lt;sid&gt;_site_srHook_&lt;siteA&gt; -v SOK   -t crm_config -s SAPHanaSR
Cmnd_Alias SFAIL_SITEA = /usr/sbin/crm_attribute -n hana_&lt;sid&gt;_site_srHook_&lt;siteA&gt; -v SFAIL -t crm_config -s SAPHanaSR
Cmnd_Alias SOK_SITEB   = /usr/sbin/crm_attribute -n hana_&lt;sid&gt;_site_srHook_&lt;siteB&gt; -v SOK   -t crm_config -s SAPHanaSR
Cmnd_Alias SFAIL_SITEB = /usr/sbin/crm_attribute -n hana_&lt;sid&gt;_site_srHook_&lt;siteB&gt; -v SFAIL -t crm_config -s SAPHanaSR
&lt;sidadm&gt; ALL=(ALL) NOPASSWD: SOK_SITEA, SFAIL_SITEA, SOK_SITEB, SFAIL_SITEB</pre></div></div></div></div></div></div><div class="sect1" id="_configuration_of_the_cluster"><div class="titlepage"><div><div><h2 class="title"><span class="number">9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuration of the Cluster</span> <a title="Permalink" class="permalink" href="#_configuration_of_the_cluster">#</a></h2></div></div></div><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/SAPHanaSR-ScaleOut-Plan-Phase6.svg" target="_blank"><img src="images/SAPHanaSR-ScaleOut-Plan-Phase6.svg" width="" alt="SAPHanaSR ScaleOut Plan Phase6" /></a></div></div><p>This chapter describes the configuration of the cluster software SUSE
Linux Enterprise High Availability Extension, which is part of the SUSE
Linux Enterprise Server for SAP Applications, and SAP HANA Database
Integration.</p><div class="orderedlist "><div class="orderedlist-title-wrap"><h6 class="orderedlist-title"><span class="name">Actions </span><a title="Permalink" class="permalink" href="#id-1.10.4">#</a></h6></div><ol class="orderedlist" type="1"><li class="listitem "><p>Basic Cluster Configuration.</p></li><li class="listitem "><p>Configure Cluster Properties and Resources.</p></li></ol></div><div class="sect2" id="_basic_cluster_configuration"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Basic Cluster Configuration</span> <a title="Permalink" class="permalink" href="#_basic_cluster_configuration">#</a></h3></div></div></div><p>The first step is to set up the basic cluster framework. For
convenience, use YaST2 or the ha-cluster-init script. It is strongly
recommended to add a second corosync ring, change to UCAST communication
and adjust the timeout values to your environment.</p><div class="sect3" id="_set_up_watchdog_for_storage_based_fencing"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Set up Watchdog for "Storage-based Fencing"</span> <a title="Permalink" class="permalink" href="#_set_up_watchdog_for_storage_based_fencing">#</a></h4></div></div></div><p>If you use the SBD fencing mechanism (disk-less or disk-based), you must also
configure a watchdog. The watchdog is needed to reset a node if the system
could not longer access the SBD (disk-less or disk-based).
It is mandatory that to configure the Linux system to load a
watchdog driver. It is strongly recommended to use a watchdog with hardware #
assistance (as is available on most modern systems), such as hpwdt, iTCO_wdt,
or others. As a fall-back, you can use the softdog module.</p><div class="complex-example"><div class="example" id="id-1.10.5.3.3"><div class="example-title-wrap"><h6 class="example-title"><span class="number">Example 15: </span><span class="name">Set up for Watchdog </span><a title="Permalink" class="permalink" href="#id-1.10.5.3.3">#</a></h6></div><div class="example-contents"><div id="id-1.10.5.3.3.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>Access to the Watchdog Timer:
No other software must access the watchdog timer; it can only be
accessed by one process at any time. Some hardware vendors ship
systems management software that use the watchdog for system resets
(for example HP ASR daemon). Such software must be disabled if the watchdog
is to be used by SBD.</p></div><p>Determine the right watchdog module. Alternatively, you can find a list of
installed drivers with your kernel version.</p><div class="verbatim-wrap"><pre class="screen">ls -l /lib/modules/$(uname -r)/kernel/drivers/watchdog</pre></div><p>Check if any watchdog module is already loaded.</p><div class="verbatim-wrap"><pre class="screen">lsmod | egrep "(wd|dog|i6|iT|ibm)"</pre></div><p>If you get a result, the system has already a loaded watchdog. If the watchdog
does not match your watchdog device, you need to unload the module.</p><p>To safely unload the module, check first if an application is using the watchdog
device.</p><div class="verbatim-wrap"><pre class="screen">lsof /dev/watchdog
rmmod &lt;wrong_module&gt;</pre></div><p>Enable your watchdog module and make it persistent. For the example below,
<span class="emphasis"><em>softdog</em></span> has been used which has some restrictions and should not be used as
first option.</p><div class="verbatim-wrap"><pre class="screen">echo softdog &gt; /etc/modules-load.d/watchdog.conf
systemctl restart systemd-modules-load</pre></div><p>Check if the watchdog module is loaded correctly.</p><div class="verbatim-wrap"><pre class="screen">lsmod | grep dog
ls -l /dev/watchdog</pre></div><p>Testing the watchdog can be done with a simple action. Ensure to switch of your SAP HANA
first because watchdog will force an unclean reset / shutdown of your system.</p><p>In case of a hardware watchdog a desired action is predefined after the timeout of the watchdog has
reached. If your watchdog module is loaded and not controlled by any other application, do the following:</p><div id="id-1.10.5.3.3.16" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>Triggering the watchdog without continuously updating the watchdog
resets/switches off the system. This is the intended mechanism. The following
commands will force your system to be reset/switched off.</p></div><div class="verbatim-wrap"><pre class="screen">touch /dev/watchdog</pre></div><p>In case the softdog module is used the following action can be performed:</p><div class="verbatim-wrap"><pre class="screen">echo 1&gt; /dev/watchdog</pre></div><p>After your test was successful you must implement the watchdog on all cluster
members.</p></div></div></div></div><div class="sect3" id="_initial_cluster_setup_using_ha_cluster_init"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Initial Cluster Setup Using <code class="literal">ha-cluster-init</code></span> <a title="Permalink" class="permalink" href="#_initial_cluster_setup_using_ha_cluster_init">#</a></h4></div></div></div><p>For more information, see <span class="emphasis"><em>Automatic Cluster Setup</em></span>, SUSE Linux
Enterprise High Availability Extension.</p><p>Create an initial setup, using <code class="literal">ha-cluster-init</code> command and follow the
dialogs. This has only to be done on the first cluster node.</p><div class="verbatim-wrap"><pre class="screen">suse01:~&gt; ha-cluster-init -u -s &lt;sbddevice&gt;</pre></div><p>This command configures the basic cluster framework including:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>SSH keys</p></li><li class="listitem "><p>csync2 to transfer configuration files</p></li><li class="listitem "><p>SBD (at least one device)</p></li><li class="listitem "><p>corosync (at least one ring)</p></li><li class="listitem "><p>HAWK Web interface</p></li></ul></div><div id="id-1.10.5.4.7" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>As requested by <code class="literal">ha-cluster-init</code>, change the password of the user hacluster.</p></div></div><div class="sect3" id="_adapting_the_corosync_and_sbd_configuration"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Adapting the Corosync and SBD Configuration</span> <a title="Permalink" class="permalink" href="#_adapting_the_corosync_and_sbd_configuration">#</a></h4></div></div></div><p>It is recommended to add a second corosync ring. If you did not start ha-cluster-init
with the <span class="emphasis"><em>-u</em></span> option, you need to change corosync to use UCAST communication.
To change to UCAST stop the already running cluster by using <code class="literal">systemctl stop pacemaker</code>.
After the setup of the corosync configuration and the SBD parameters, start the cluster again.</p><div class="sect4" id="_corosync_configuration"><div class="titlepage"><div><div><h5 class="title"><span class="number">9.1.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Corosync Configuration</span> <a title="Permalink" class="permalink" href="#_corosync_configuration">#</a></h5></div></div></div><p>Check the following blocks in the file <span class="emphasis"><em>/etc/corosync/corosync.conf</em></span>.
See also the example at the end of this document.</p><div class="verbatim-wrap"><pre class="screen">totem {
    ...

    interface {
        ringnumber: 0
        mcastport:  5405
        ttl:        1
    }
    #Transport protocol
    transport:      udpu

}
nodelist {
        node {
        #ring0 address
        ring0_addr:     192.168.1.11
        nodeid: 1

        }
    }</pre></div></div><div class="sect4" id="_adapting_sbd_configuration"><div class="titlepage"><div><div><h5 class="title"><span class="number">9.1.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Adapting SBD Configuration</span> <a title="Permalink" class="permalink" href="#_adapting_sbd_configuration">#</a></h5></div></div></div><p>You can skip this section if you do not have any SBD devices, but be
sure to implement another supported fencing mechanism.</p><p>See man pages sbd.8 and stonith_sbd.7 for details.</p><div class="table" id="id-1.10.5.5.4.4"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 3: </span><span class="name">SBD Options in File /etc/sysconfig/SBD </span><a title="Permalink" class="permalink" href="#id-1.10.5.5.4.4">#</a></h6></div><div class="table-contents"><table class="table" summary="SBD Options in File /etc/sysconfig/SBD" border="1"><colgroup><col class="col_1" /><col class="col_2" /></colgroup><thead><tr><th align="left" valign="top">Parameter</th><th align="left" valign="top">Description</th></tr></thead><tbody><tr><td align="left" valign="top"><p>SBD_WATCHDOG_DEV</p></td><td align="left" valign="top"><p>Define the watchdog device. It is mandatory to use a watchdog. SBD does not work
reliable without watchdog. Refer to the SLES manual and SUSE
TIDs 7016880 for setting up a watchdog.</p></td></tr><tr><td align="left" valign="top"><p>SBD_WATCHDOG_TIMEOUT</p></td><td align="left" valign="top"><p>Defines the timeout, in seconds, the watchdog will wait before panicking the node if no-one tickles it.</p><p>This parameter must match your storage environemt. If you expect your storage could hang for 30s, then
this parameter must be set to a higher value. The typical setting of 5s could be to aggressive in
you production environment.</p><p>The parameter must also be aligned with the pacameker cluster property stonith-watchdog-timeout.
The property stonith-watchdog-timeout must be set &gt;= the value of SBD_WATCHDOG_TIMEOUT.</p><p>If you set <code class="literal">stonith-watchdog-timeout</code> to a negative value, Pacemaker will automatically calculate this timeout and set it to twice the value of <code class="literal">SBD_WATCHDOG_TIMEOUT</code> starting with SUSE Linux Enterprise High Availability Extension 15.</p></td></tr><tr><td align="left" valign="top"><p>SBD_STARTMODE</p></td><td align="left" valign="top"><p>Start mode. If set to <code class="literal">clean</code>, sbd will only start if the node was
previously shut down cleanly or if the slot is empty.</p></td></tr><tr><td align="left" valign="top"><p>SBD_PACEMAKER</p></td><td align="left" valign="top"><p>Check Pacemaker quorum and node health.</p></td></tr></tbody></table></div></div><p>In the following, replace /dev/disk/by-id/SBDA and /dev/disk/by-id/SBDB by your real sbd
device names. As an example the <code class="literal">SBD_WATCHDOG_TIMEOUT</code> is set to 15s to be less
aggressive than the typical 5s.</p><div class="verbatim-wrap"><pre class="screen"># /etc/sysconfig/sbd
SBD_DEVICE="/dev/disk/by-id/SBDA;/dev/disk/by-id/SBDB"
SBD_WATCHDOG_DEV="/dev/watchdog"
SBD_WATCHDOG_TIMEOUT=15
SBD_PACEMAKER="yes"
SBD_STARTMODE="clean"
SBD_OPTS=""</pre></div><div id="id-1.10.5.5.4.7" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>Also read the SUSE product documentation about calculation of timeouts for more details:
<a class="link" href="https://documentation.suse.com/sle-ha/15-SP1/single-html/SLE-HA-guide/#sec-ha-storage-protect-watchdog-timings" target="_blank">https://documentation.suse.com/sle-ha/15-SP1/single-html/SLE-HA-guide/#sec-ha-storage-protect-watchdog-timings</a></p></div></div><div class="sect4" id="_verifying_the_sbd_device"><div class="titlepage"><div><div><h5 class="title"><span class="number">9.1.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Verifying the SBD Device</span> <a title="Permalink" class="permalink" href="#_verifying_the_sbd_device">#</a></h5></div></div></div><p>You can skip this section if you do not have any SBD devices, but make
sure to implement a supported fencing mechanism.</p><p>It is a good practice to check if the SBD device can be accessed from
both nodes and does contain valid records. Check this for all devices
configured in <span class="emphasis"><em>/etc/sysconfig/sbd</em></span>.</p><div class="verbatim-wrap"><pre class="screen">suse01:~ # sbd -d /dev/disk/by-id/SBDA dump
==Dumping header on disk /dev/disk/by-id/SBDA
Header version     : 2.1
UUID               : 0f4ea13e-fab8-4147-b9b2-3cdcfff07f86
Number of slots    : 255
Sector size        : 512
Timeout (watchdog) : 20
Timeout (allocate) : 2
Timeout (loop)     : 1
Timeout (msgwait)  : 40
==Header on disk /dev/disk/by-id/SBDA is dumped</pre></div><p>The timeout values in our sample are only start values, which need to be
tuned to your environment.</p><p>To check the current SBD entries for the various cluster nodes, you can
use <code class="literal">sbd list</code>. If all entries are <code class="literal">clear</code>, no fencing task is marked in
the SBD device.</p><div class="verbatim-wrap"><pre class="screen">suse01:~ # sbd -d /dev/disk/by-id/SBDA list
0     suse01      clear</pre></div><p>For more information on SBD configuration parameters, read the
section <span class="emphasis"><em>Storage-based Fencing</em></span>, SUSE Linux Enterprise High
Availability Extension and TIDs 7016880 and 7008216.</p><p>Now it is time to restart the cluster at the first node again
(<code class="literal">systemctl start pacemaker</code>).</p></div></div><div class="sect3" id="_cluster_configuration_on_the_second_node"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Cluster Configuration on the Second Node</span> <a title="Permalink" class="permalink" href="#_cluster_configuration_on_the_second_node">#</a></h4></div></div></div><p>The second node of the two nodes cluster could be integrated by starting
the command <code class="literal">ha-cluster-join</code>. This command asks for the IP address or
name of the first cluster node. Than all needed configuration files are
copied over. As a result the cluster is started on both nodes.</p><div class="verbatim-wrap"><pre class="screen"># ha-cluster-join -c &lt;host1&gt;</pre></div></div><div class="sect3" id="_checking_the_cluster_for_the_first_time"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.1.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Checking the Cluster for the First Time</span> <a title="Permalink" class="permalink" href="#_checking_the_cluster_for_the_first_time">#</a></h4></div></div></div><p>Now it is time to check and optionally start the cluster for the first
time on both nodes.</p><div class="verbatim-wrap"><pre class="screen">suse01:~ # systemctl status pacemaker
suse01:~ # systemctl status sbd
suse02:~ # systemctl status pacemaker
suse01:~ # systemctl start pacemaker
suse02:~ # systemctl status sbd
suse02:~ # systemctl start pacemaker</pre></div><p>Check the cluster status with crm_mon. We use the option "-r" to also
see resources, which are configured but stopped.</p><div class="verbatim-wrap"><pre class="screen"># crm_mon -r</pre></div><p>The command will show the "empty" cluster and will print something like
the following screen output. The most interesting information for now is
that there are two nodes in the status "online" and the message
"partition with quorum".</p><div class="verbatim-wrap"><pre class="screen">Stack: corosync
Current DC: suse01 (version 2.0.1+20190417.13d370ca9-3.6.1-2.0.1+20190417.13d370ca9) - partition with quorum
Last updated: Wed Feb  5 15:06:35 2020
Last change: Wed Feb  5 15:04:42 2020 by hacluster via crmd on suse02
2 nodes configured
1 resource configured
Online: [ suse01 suse02 ]
Full list of resources:
 stonith-sbd    (stonith:external/sbd): Started suse01</pre></div></div></div><div class="sect2" id="_configuring_cluster_properties_and_resources"><div class="titlepage"><div><div><h3 class="title"><span class="number">9.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Configuring Cluster Properties and Resources</span> <a title="Permalink" class="permalink" href="#_configuring_cluster_properties_and_resources">#</a></h3></div></div></div><p>This section describes how to configure constraints, resources,
bootstrap and STONITH using the <code class="literal">crm configure</code> shell command as described
in section <span class="emphasis"><em>Configuring and Managing Cluster Resources (Command Line)</em></span> of the
SUSE Linux Enterprise High Availability Extension documentation.</p><p>Use the command <code class="literal">crm</code> to add the objects to CRM. Copy the following
examples to a local file, edit the file and then load the configuration
to the CIB:</p><div class="verbatim-wrap"><pre class="screen">suse01:~ # vi crm-fileXX
suse01:~ # crm configure load update crm-fileXX</pre></div><div class="sect3" id="_cluster_bootstrap_and_more"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Cluster Bootstrap and More</span> <a title="Permalink" class="permalink" href="#_cluster_bootstrap_and_more">#</a></h4></div></div></div><p>The first example defines the cluster bootstrap options, the resource
and operation defaults. The stonith-timeout should be greater than 1.2
times the SBD msgwait timeout.</p><div class="verbatim-wrap"><pre class="screen">suse01:~ # vi crm-bs.txt
# enter the following to crm-bs.txt
property $id="cib-bootstrap-options" \
              stonith-enabled="true" \
              stonith-action="reboot" \
              stonith-timeout="150s"
rsc_defaults $id="rsc-options" \
              resource-stickiness="1000" \
              migration-threshold="5000"
op_defaults $id="op-options" \
                 timeout="600"</pre></div><p>Now we add the configuration to the cluster.</p><div class="verbatim-wrap"><pre class="screen">suse01:~ # crm configure load update crm-bs.txt</pre></div></div><div class="sect3" id="_stonith_device"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">STONITH device</span> <a title="Permalink" class="permalink" href="#_stonith_device">#</a></h4></div></div></div><p>Skip this section if you are using disk-less SBD.</p><p>The next configuration part defines an SBD disk STONITH resource.</p><div class="verbatim-wrap"><pre class="screen"># vi crm-sbd.txt
# enter the following to crm-sbd.txt
primitive stonith-sbd stonith:external/sbd \
    params pcmk_delay_max="15"</pre></div><p>Again we add the configuration to the cluster.</p><div class="verbatim-wrap"><pre class="screen">suse01:~ # crm configure load update crm-sbd.txt</pre></div><p>For fencing with IPMI/ILO see section <a class="xref" href="#_using_ipmi_as_fencing_mechanism" title="9.2.3. Using IPMI as fencing mechanism">Section 9.2.3, “Using IPMI as fencing mechanism”</a>.</p></div><div class="sect3" id="_using_ipmi_as_fencing_mechanism"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using IPMI as fencing mechanism</span> <a title="Permalink" class="permalink" href="#_using_ipmi_as_fencing_mechanism">#</a></h4></div></div></div><p>For details about IPMI/ILO fencing see our cluster product documentation (<a class="link" href="https://documentation.suse.com/sle-ha/15-SP1/single-html/SLE-HA-guide/" target="_blank">https://documentation.suse.com/sle-ha/15-SP1/single-html/SLE-HA-guide/</a>).
An example for an IPMI STONITH resource can be found in section <a class="xref" href="#_example_for_the_ipmi_stonith_method" title="13.4. Example for the IPMI STONITH Method">Section 13.4, “Example for the IPMI STONITH Method”</a>
of this document.</p><p>To use IPMI the remote management boards must be compatible with the
IPMI standard.</p><p>For the IPMI based fencing you need to configure a primitive per cluster
node. Each resource is responsible to fence exactly one cluster node.
You need to adapt the IP addresses and login user / password of the
remote management boards to the STONITH resource agent. We recommend to
create a special STONITH user instead of providing root access to the
management board. Location rules must guarantee that a host should never
run its own STONITH resource.</p></div><div class="sect3" id="_using_other_fencing_mechanisms"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.2.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Using Other Fencing Mechanisms</span> <a title="Permalink" class="permalink" href="#_using_other_fencing_mechanisms">#</a></h4></div></div></div><p>We recommend to use SBD (best practice) or IPMI (second choice) as
STONITH mechanism. The SUSE Linux Enterprise High Availability product
also supports additional fencing mechanism not covered here.</p><p>For further information about fencing, see SUSE Linux Enterprise High
Availability Guide.</p></div><div class="sect3" id="_saphanatopology"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.2.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SAPHanaTopology</span> <a title="Permalink" class="permalink" href="#_saphanatopology">#</a></h4></div></div></div><p>Next we define the group of resources needed, before the HANA instances
can be started. Prepare the changes in a text file, for example
<span class="emphasis"><em>crm-saphanatop.txt</em></span>, and load it with the command:</p><p><code class="literal">crm configure load update crm-saphanatop.txt</code></p><div class="verbatim-wrap"><pre class="screen"># vi crm-saphanatop.txt
# enter the following to crm-saphanatop.txt
primitive rsc_SAPHanaTopology_HA1_HDB10 ocf:suse:SAPHanaTopology \
        op monitor interval="10" timeout="600" \
        op start interval="0" timeout="600" \
        op stop interval="0" timeout="300" \
        params SID="HA1" InstanceNumber="10"
clone cln_SAPHanaTopology_HA1_HDB10 rsc_SAPHanaTopology_HA1_HDB10 \
        meta clone-node-max="1" interleave="true"</pre></div><p>Additional information about all parameters can be found with the
command:</p><p><code class="literal">man ocf_suse_SAPHanaTopology</code></p><p>Again we add the configuration to the cluster.</p><div class="verbatim-wrap"><pre class="screen">suse01:~ # crm configure load update crm-saphanatop.txt</pre></div><p>The most important parameters here are SID and InstanceNumber, which are
in the SAP context quite self explaining. Beside these parameters, the
timeout values or the operations (start, monitor, stop) are typical
tuneables.</p></div><div class="sect3" id="_saphana"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.2.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SAPHana</span> <a title="Permalink" class="permalink" href="#_saphana">#</a></h4></div></div></div><p>Next we define the group of resources needed, before the HANA instances
can be started. Edit the changes in a text file, for example
<span class="emphasis"><em>crm-saphana.txt</em></span>, and load it with the command:</p><p><code class="literal">crm configure load update crm-saphana.txt</code></p><div class="table" id="id-1.10.6.10.4"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 4: </span><span class="name">Typical Resource Agent parameter settings for different scenarios </span><a title="Permalink" class="permalink" href="#id-1.10.6.10.4">#</a></h6></div><div class="table-contents"><table class="table" summary="Typical Resource Agent parameter settings for different scenarios" border="1" width="99%"><colgroup><col class="col_1" /><col class="col_2" /><col class="col_3" /><col class="col_4" /></colgroup><thead><tr><th align="left" valign="top">Parameter</th><th align="left" valign="top">Performance Optimized</th><th align="left" valign="top">Cost Optimized</th><th align="left" valign="top">Multi-Tier</th></tr></thead><tbody><tr><td align="left" valign="top"><p>PREFER_SITE_TAKEOVER</p></td><td align="left" valign="top"><p>true</p></td><td align="left" valign="top"><p>false</p></td><td align="left" valign="top"><p>false / true</p></td></tr><tr><td align="left" valign="top"><p>AUTOMATED_REGISTER</p></td><td align="left" valign="top"><p>false / true</p></td><td align="left" valign="top"><p>false / true</p></td><td align="left" valign="top"><p>false</p></td></tr><tr><td align="left" valign="top"><p>DUPLICATE_PRIMARY_TIMEOUT</p></td><td align="left" valign="top"><p>7200</p></td><td align="left" valign="top"><p>7200</p></td><td align="left" valign="top"><p>7200</p></td></tr></tbody></table></div></div><div class="table" id="id-1.10.6.10.5"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 5: </span><span class="name">Description of important Resource Agent parameters </span><a title="Permalink" class="permalink" href="#id-1.10.6.10.5">#</a></h6></div><div class="table-contents"><table class="table" summary="Description of important Resource Agent parameters" border="1" width="100%"><colgroup><col class="col_1" /><col class="col_2" /></colgroup><thead><tr><th align="left" valign="top">Parameter</th><th align="left" valign="top">Description</th></tr></thead><tbody><tr><td align="left" valign="top"><p>PREFER_SITE_TAKEOVER</p></td><td align="left" valign="top"><p>Defines whether RA should prefer to takeover to
the secondary instance instead of restarting the failed primary locally.</p></td></tr><tr><td align="left" valign="top"><p>AUTOMATED_REGISTER</p></td><td align="left" valign="top"><p>Defines whether a former primary should be automatically registered to
be secondary of the new primary. With this parameter you can adapt the
level of system replication automation.</p>
<p>If set to <code class="literal">false</code>, the former primary must be manually registered. The
cluster will not start this SAP HANA RDBMS until it is registered to avoid
double primary up situations.</p></td></tr><tr><td align="left" valign="top"><p>DUPLICATE_PRIMARY_TIMEOUT</p></td><td align="left" valign="top"><p>Time difference needed between two primary
time stamps if a dual-primary situation occurs. If the time difference
is less than the time gap, than the cluster hold one or both instances
in a "WAITING" status. This is to give an administrator the chance to react on a
fail-over. If the complete node of the former primary crashed, the former
primary will be registered after the time difference is passed. If
"only" the SAP HANA RDBMS has crashed, then the former primary will be
registered immediately. After this registration to the new primary all
data will be overwritten by the system replication.</p></td></tr></tbody></table></div></div><p>Additional information about all parameters could be found with the
command:</p><p><code class="literal">man ocf_suse_SAPHana</code></p><div class="verbatim-wrap"><pre class="screen"># vi crm-saphana.txt
# enter the following to crm-saphana.txt
primitive rsc_SAPHana_HA1_HDB10 ocf:suse:SAPHana \
        op start interval="0" timeout="3600" \
        op stop interval="0" timeout="3600" \
        op promote interval="0" timeout="3600" \
        op monitor interval="60" role="Master" timeout="700" \
        op monitor interval="61" role="Slave" timeout="700" \
        params SID="HA1" InstanceNumber="10" PREFER_SITE_TAKEOVER="true" \
        DUPLICATE_PRIMARY_TIMEOUT="7200" AUTOMATED_REGISTER="false"
ms msl_SAPHana_HA1_HDB10 rsc_SAPHana_HA1_HDB10 \
        meta clone-max="2" clone-node-max="1" interleave="true"</pre></div><p>We add the configuration to the cluster.</p><div class="verbatim-wrap"><pre class="screen">suse01:~ # crm configure load update crm-saphana.txt</pre></div><p>The most important parameters here are again SID and InstanceNumber.
Beside these parameters the timeout values for the operations (start,
promote, monitors, stop) are typical tuneables.</p></div><div class="sect3" id="_the_virtual_ip_address_for_the_primary_site"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.2.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">The virtual IP address for The Primary Site</span> <a title="Permalink" class="permalink" href="#_the_virtual_ip_address_for_the_primary_site">#</a></h4></div></div></div><p>The last resource to be added to the cluster is covering the virtual IP
address.</p><div class="verbatim-wrap"><pre class="screen"># vi crm-vip.txt
# enter the following to crm-vip.txt

primitive rsc_ip_HA1_HDB10 ocf:heartbeat:IPaddr2 \
        op monitor interval="10s" timeout="20s" \
        params ip="192.168.1.20"</pre></div><p>We load the file to the cluster.</p><div class="verbatim-wrap"><pre class="screen">suse01:~ # crm configure load update crm-vip.txt</pre></div><p>In most installations, only the parameter ip needs to be set to the
virtual IP address to be presented to the client systems.</p></div><div class="sect3" id="_constraints"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.2.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Constraints</span> <a title="Permalink" class="permalink" href="#_constraints">#</a></h4></div></div></div><p>Two constraints are organizing the correct placement of the virtual IP
address for the client database access and the start order between the
two resource agents SAPHana and SAPHanaTopology.</p><div class="verbatim-wrap"><pre class="screen"># vi crm-cs.txt
# enter the following to crm-cs.txt

colocation col_saphana_ip_HA1_HDB10 2000: rsc_ip_HA1_HDB10:Started \
    msl_SAPHana_HA1_HDB10:Master
order ord_SAPHana_HA1_HDB10 Optional: cln_SAPHanaTopology_HA1_HDB10 \
    msl_SAPHana_HA1_HDB10</pre></div><p>We load the file to the cluster.</p><div class="verbatim-wrap"><pre class="screen">suse01:~ # crm configure load update crm-cs.txt</pre></div></div><div class="sect3" id="_activeactive_read_enabled_scenario"><div class="titlepage"><div><div><h4 class="title"><span class="number">9.2.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Active/Active Read-Enabled Scenario</span> <a title="Permalink" class="permalink" href="#_activeactive_read_enabled_scenario">#</a></h4></div></div></div><p>This step is optional. If you have an active/active SAP HANA system
replication with a read-enabled secondary, it is possible to integrate the
needed second virtual IP address into the cluster. This is been done by adding
a second virtual IP address resource and a location constraint binding the
address to the secondary site.</p><div class="verbatim-wrap"><pre class="screen"># vi crm-re.txt
# enter the following to crm-re.txt

primitive rsc_ip_HA1_HDB10_readenabled ocf:heartbeat:IPaddr2 \
        op monitor interval="10s" timeout="20s" \
        params ip="192.168.1.21"
colocation col_saphana_ip_HA1_HDB10_readenabled 2000: \
    rsc_ip_HA1_HDB10_readenabled:Started msl_SAPHana_HA1_HDB10:Slave</pre></div></div></div></div><div class="sect1" id="cha.s4s.test-cluster"><div class="titlepage"><div><div><h2 class="title"><span class="number">10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Testing the Cluster</span> <a title="Permalink" class="permalink" href="#cha.s4s.test-cluster">#</a></h2></div></div></div><div class="informalfigure"><div class="mediaobject"><a xmlns="" href="images/SAPHanaSR-ScaleOut-Plan-Phase7.svg" target="_blank"><img src="images/SAPHanaSR-ScaleOut-Plan-Phase7.svg" width="" alt="SAPHanaSR ScaleOut Plan Phase7" /></a></div></div><p>The lists of tests will be improved in the next update of this document.</p><p>As with any cluster testing is crucial. Make sure that all test
cases derived from customer expectations are implemented and passed
fully. Otherwise the project is likely to fail in production.</p><p>The test prerequisite, if not described differently, is always that both
nodes are booted, normal members of the cluster and the HANA RDBMS is
running. The system replication is in sync (SOK).</p><div class="sect2" id="_test_cases_for_semi_automation"><div class="titlepage"><div><div><h3 class="title"><span class="number">10.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Test Cases for Semi Automation</span> <a title="Permalink" class="permalink" href="#_test_cases_for_semi_automation">#</a></h3></div></div></div><p>In the following test descriptions we assume
<code class="literal">PREFER_SITE_TAKEOVER="true"</code> and <code class="literal">AUTOMATED_REGISTER="false".</code></p><div id="id-1.11.6.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>The following tests are designed to be run in sequence and depend
on the exit state of the proceeding tests.</p></div><div class="sect3" id="_test_stop_primary_database_on_site_a_node_1"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.1.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Test: Stop Primary Database on Site A (Node 1)</span> <a title="Permalink" class="permalink" href="#_test_stop_primary_database_on_site_a_node_1">#</a></h4></div></div></div><div class="complex-example"><div class="example" id="id-1.11.6.4.2"><div class="example-title-wrap"><h6 class="example-title"><span class="number">Example 16: </span><span class="name">Test STOP_PRIMARY_SITE_A </span><a title="Permalink" class="permalink" href="#id-1.11.6.4.2">#</a></h6></div><div class="example-contents"><div class="itemizedlist "><div class="itemizedlist-title-wrap"><h6 class="itemizedlist-title"><span class="name">Component: </span><a title="Permalink" class="permalink" href="#id-1.11.6.4.2.2">#</a></h6></div><ul class="itemizedlist"><li class="listitem "><p>Primary Database</p></li></ul></div><div class="itemizedlist "><div class="itemizedlist-title-wrap"><h6 class="itemizedlist-title"><span class="name">Description: </span><a title="Permalink" class="permalink" href="#id-1.11.6.4.2.3">#</a></h6></div><ul class="itemizedlist"><li class="listitem "><p>The primary HANA database is stopped during normal cluster operation.</p></li></ul></div><div class="orderedlist "><div class="orderedlist-title-wrap"><h6 class="orderedlist-title"><span class="name">Test Procedure: </span><a title="Permalink" class="permalink" href="#id-1.11.6.4.2.4">#</a></h6></div><ol class="orderedlist" type="1"><li class="listitem "><p>Stop the primary HANA database gracefully as <span class="emphasis"><em>&lt;sid&gt;adm</em></span>.</p><div class="verbatim-wrap"><pre class="screen">suse01# HDB stop</pre></div></li></ol></div><div class="orderedlist "><div class="orderedlist-title-wrap"><h6 class="orderedlist-title"><span class="name">Recovery Procedure: </span><a title="Permalink" class="permalink" href="#id-1.11.6.4.2.5">#</a></h6></div><ol class="orderedlist" type="1"><li class="listitem "><p>Manually register the old primary (on node 1) with the new primary after takeover (on node 2) as <span class="emphasis"><em>&lt;sid&gt;adm</em></span>.</p><div class="verbatim-wrap"><pre class="screen">suse01# hdbnsutil -sr_register --remoteHost=suse02 --remoteInstance=10 \
          --replicationMode=sync --operationMode=logreplay \
          --name=WDF</pre></div></li><li class="listitem "><p>Restart the HANA database (now secondary) on node 1 as root.</p><div class="verbatim-wrap"><pre class="screen">suse01# crm resource refresh rsc_SAPHana_HA1_HDB10 suse01</pre></div></li></ol></div><div class="orderedlist "><div class="orderedlist-title-wrap"><h6 class="orderedlist-title"><span class="name">Expected: </span><a title="Permalink" class="permalink" href="#id-1.11.6.4.2.6">#</a></h6></div><ol class="orderedlist" type="1"><li class="listitem "><p>The cluster detects the stopped primary HANA database (on node 1)
and marks the resource failed.</p></li><li class="listitem "><p>The cluster promotes the secondary HANA database (on node 2) to
take over as primary.</p></li><li class="listitem "><p>The cluster migrates the IP address to the new primary (on node 2).</p></li><li class="listitem "><p>After some time the cluster shows the sync_state of the stopped
primary (on node 1) as SFAIL.</p></li><li class="listitem "><p>Because AUTOMATED_REGISTER="false" the cluster does not restart
the failed HANA database or register it against the new primary.</p></li><li class="listitem "><p>After the manual register and resource refresh the system
replication pair is marked as in sync (SOK).</p></li><li class="listitem "><p>The cluster "failed actions" are cleaned up after following the
recovery procedure.</p></li></ol></div></div></div></div></div><div class="sect3" id="_test_stop_primary_database_on_site_b_node_2"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.1.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Test: Stop Primary Database on Site B (Node 2)</span> <a title="Permalink" class="permalink" href="#_test_stop_primary_database_on_site_b_node_2">#</a></h4></div></div></div><div class="complex-example"><div class="example" id="id-1.11.6.5.2"><div class="example-title-wrap"><h6 class="example-title"><span class="number">Example 17: </span><span class="name">Test STOP_PRIMARY_DB_SITE_B </span><a title="Permalink" class="permalink" href="#id-1.11.6.5.2">#</a></h6></div><div class="example-contents"><div class="variablelist "><dl class="variablelist"><dt id="id-1.11.6.5.2.2.1"><span class="term ">Component:</span></dt><dd><p>Primary Database</p></dd><dt id="id-1.11.6.5.2.2.2"><span class="term ">Description:</span></dt><dd><p>The primary HANA database is stopped during normal cluster operation.</p></dd></dl></div><div class="orderedlist "><div class="orderedlist-title-wrap"><h6 class="orderedlist-title"><span class="name">Test Procedure: </span><a title="Permalink" class="permalink" href="#id-1.11.6.5.2.3">#</a></h6></div><ol class="orderedlist" type="1"><li class="listitem "><p>Stop the database gracefully as <span class="emphasis"><em>&lt;sid&gt;adm</em></span>.</p><div class="verbatim-wrap"><pre class="screen">suse02# HDB stop</pre></div></li></ol></div><div class="orderedlist "><div class="orderedlist-title-wrap"><h6 class="orderedlist-title"><span class="name">Recovery Procedure: </span><a title="Permalink" class="permalink" href="#id-1.11.6.5.2.4">#</a></h6></div><ol class="orderedlist" type="1"><li class="listitem "><p>Manually register the old primary (on node 2) with the new primary
after takeover (on node 1) as <span class="emphasis"><em>&lt;sid&gt;adm</em></span>.</p><div class="verbatim-wrap"><pre class="screen">suse02# hdbnsutil -sr_register --remoteHost=suse01 --remoteInstance=10 \
               --replicationMode=sync --operationMode=logreplay \
               --name=ROT</pre></div></li><li class="listitem "><p>Restart the HANA database (now secondary) on node 1 as root.</p><div class="verbatim-wrap"><pre class="screen">suse02# crm resource refresh rsc_SAPHana_HA1_HDB10 suse02</pre></div></li></ol></div><div class="orderedlist "><div class="orderedlist-title-wrap"><h6 class="orderedlist-title"><span class="name">Expected: </span><a title="Permalink" class="permalink" href="#id-1.11.6.5.2.5">#</a></h6></div><ol class="orderedlist" type="1"><li class="listitem "><p>The cluster detects the stopped primary HANA database (on node 2)
and marks the resource failed.</p></li><li class="listitem "><p>The cluster promotes the secondary HANA database (on node 1) to
take over as primary.</p></li><li class="listitem "><p>The cluster migrates the IP address to the new primary (on node
1).</p></li><li class="listitem "><p>After some time the cluster shows the sync_state of the stopped
primary (on node 2) as SFAIL.</p></li><li class="listitem "><p>Because AUTOMATED_REGISTER="false" the cluster does not restart
the failed HANA database or register it against the new primary.</p></li><li class="listitem "><p>After the manual register and resource refresh the system
replication pair is marked as in sync (SOK).</p></li><li class="listitem "><p>The cluster "failed actions" are cleaned up after following the
recovery procedure.</p></li></ol></div></div></div></div></div><div class="sect3" id="_test_crash_primary_database_on_site_a_node_1"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.1.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Test: Crash Primary Database on Site A (Node 1)</span> <a title="Permalink" class="permalink" href="#_test_crash_primary_database_on_site_a_node_1">#</a></h4></div></div></div><div class="complex-example"><div class="example" id="id-1.11.6.6.2"><div class="example-title-wrap"><h6 class="example-title"><span class="number">Example 18: </span><span class="name">Test CRASH_PRIMARY_DB_SITE_A </span><a title="Permalink" class="permalink" href="#id-1.11.6.6.2">#</a></h6></div><div class="example-contents"><div class="variablelist "><dl class="variablelist"><dt id="id-1.11.6.6.2.2.1"><span class="term ">Component:</span></dt><dd><p>Primary Database</p></dd><dt id="id-1.11.6.6.2.2.2"><span class="term ">Description:</span></dt><dd><p>Simulate a complete break-down of the primary database system.</p></dd></dl></div><div class="orderedlist "><div class="orderedlist-title-wrap"><h6 class="orderedlist-title"><span class="name">Test Procedure: </span><a title="Permalink" class="permalink" href="#id-1.11.6.6.2.3">#</a></h6></div><ol class="orderedlist" type="1"><li class="listitem "><p>Kill the primary database system using signals as <span class="emphasis"><em>&lt;sid&gt;adm</em></span>.</p><div class="verbatim-wrap"><pre class="screen">suse01# HDB kill-9</pre></div></li></ol></div><div class="orderedlist "><div class="orderedlist-title-wrap"><h6 class="orderedlist-title"><span class="name">Recovery Procedure: </span><a title="Permalink" class="permalink" href="#id-1.11.6.6.2.4">#</a></h6></div><ol class="orderedlist" type="1"><li class="listitem "><p>Manually register the old primary (on node 1) with the new primary
after takeover (on node 2) as <span class="emphasis"><em>&lt;sid&gt;adm</em></span>.</p><div class="verbatim-wrap"><pre class="screen">suse01# hdbnsutil -sr_register --remoteHost=suse02 --remoteInstance=10 \
               --replicationMode=sync  --operationMode=logreplay \
               --name=WDF</pre></div></li><li class="listitem "><p>Restart the HANA database (now secondary) on node 1 as root.</p><div class="verbatim-wrap"><pre class="screen">suse01# crm resource refresh rsc_SAPHana_HA1_HDB10 suse01</pre></div></li></ol></div><div class="orderedlist "><div class="orderedlist-title-wrap"><h6 class="orderedlist-title"><span class="name">Expected: </span><a title="Permalink" class="permalink" href="#id-1.11.6.6.2.5">#</a></h6></div><ol class="orderedlist" type="1"><li class="listitem "><p>The cluster detects the stopped primary HANA database (on node 1)
and marks the resource failed.</p></li><li class="listitem "><p>The cluster promotes the secondary HANA database (on node 2) to
take over as primary.</p></li><li class="listitem "><p>The cluster migrates the IP address to the new primary (on node
2).</p></li><li class="listitem "><p>After some time the cluster shows the sync_state of the stopped
primary (on node 1) as SFAIL.</p></li><li class="listitem "><p>Because AUTOMATED_REGISTER="false" the cluster does not restart
the failed HANA database or register it against the new primary.</p></li><li class="listitem "><p>After the manual register and resource refresh the system
replication pair is marked as in sync (SOK).</p></li><li class="listitem "><p>The cluster "failed actions" are cleaned up after following the
recovery procedure.</p></li></ol></div></div></div></div></div><div class="sect3" id="_test_crash_primary_database_on_site_b_node_2"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.1.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Test: Crash Primary Database on Site B (Node 2)</span> <a title="Permalink" class="permalink" href="#_test_crash_primary_database_on_site_b_node_2">#</a></h4></div></div></div><div class="complex-example"><div class="example" id="id-1.11.6.7.2"><div class="example-title-wrap"><h6 class="example-title"><span class="number">Example 19: </span><span class="name">Test CRASH_PRIMARY_DB_SITE_B </span><a title="Permalink" class="permalink" href="#id-1.11.6.7.2">#</a></h6></div><div class="example-contents"><div class="variablelist "><dl class="variablelist"><dt id="id-1.11.6.7.2.2.1"><span class="term ">Component:</span></dt><dd><p>Primary Database</p></dd><dt id="id-1.11.6.7.2.2.2"><span class="term ">Description:</span></dt><dd><p>Simulate a complete break-down of the primary database system.</p></dd></dl></div><div class="orderedlist "><div class="orderedlist-title-wrap"><h6 class="orderedlist-title"><span class="name">Test Procedure: </span><a title="Permalink" class="permalink" href="#id-1.11.6.7.2.3">#</a></h6></div><ol class="orderedlist" type="1"><li class="listitem "><p>Kill the primary database system using signals as <span class="emphasis"><em>&lt;sid&gt;adm</em></span>.</p><div class="verbatim-wrap"><pre class="screen">suse02# HDB kill-9</pre></div></li></ol></div><div class="orderedlist "><div class="orderedlist-title-wrap"><h6 class="orderedlist-title"><span class="name">Recovery Procedure: </span><a title="Permalink" class="permalink" href="#id-1.11.6.7.2.4">#</a></h6></div><ol class="orderedlist" type="1"><li class="listitem "><p>Manually register the old primary (on node 2) with the new primary
after takeover (on node 1) as <span class="emphasis"><em>&lt;sid&gt;adm</em></span>.</p><div class="verbatim-wrap"><pre class="screen">suse02# hdbnsutil -sr_register --remoteHost=suse01 --remoteInstance=10 \
               --replicationMode=sync  --operationMode=logreplay \
               --name=ROT</pre></div></li><li class="listitem "><p>Restart the HANA database (now secondary) on node 1 as root.</p><div class="verbatim-wrap"><pre class="screen">suse02# crm resource refresh rsc_SAPHana_HA1_HDB10 suse02</pre></div></li></ol></div><div class="orderedlist "><div class="orderedlist-title-wrap"><h6 class="orderedlist-title"><span class="name">Expected: </span><a title="Permalink" class="permalink" href="#id-1.11.6.7.2.5">#</a></h6></div><ol class="orderedlist" type="1"><li class="listitem "><p>The cluster detects the stopped primary HANA database (on node 2)
and marks the resource failed.</p></li><li class="listitem "><p>The cluster promotes the secondary HANA database (on node 1) to
take over as primary.</p></li><li class="listitem "><p>The cluster migrates the IP address to the new primary (on node
1).</p></li><li class="listitem "><p>After some time the cluster shows the sync_state of the stopped
primary (on node 2) as SFAIL.</p></li><li class="listitem "><p>Because AUTOMATED_REGISTER="false" the cluster does not restart
the failed HANA database or register it against the new primary.</p></li><li class="listitem "><p>After the manual register and resource refresh the system
replication pair is marked as in sync (SOK).</p></li><li class="listitem "><p>The cluster "failed actions" are cleaned up after following the
recovery procedure.</p></li></ol></div></div></div></div></div><div class="sect3" id="_test_crash_primary_node_on_site_a_node_1"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.1.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Test: Crash Primary Node on Site A (Node 1)</span> <a title="Permalink" class="permalink" href="#_test_crash_primary_node_on_site_a_node_1">#</a></h4></div></div></div><div class="complex-example"><div class="example" id="id-1.11.6.8.2"><div class="example-title-wrap"><h6 class="example-title"><span class="number">Example 20: </span><span class="name">Test CRASH_PRIMARY_NODE_SITE_A </span><a title="Permalink" class="permalink" href="#id-1.11.6.8.2">#</a></h6></div><div class="example-contents"><div class="variablelist "><dl class="variablelist"><dt id="id-1.11.6.8.2.2.1"><span class="term ">Component:</span></dt><dd><p>Cluster node of primary site</p></dd><dt id="id-1.11.6.8.2.2.2"><span class="term ">Description:</span></dt><dd><p>Simulate a crash of the primary site node running the primary HANA
database.</p></dd></dl></div><div class="orderedlist "><div class="orderedlist-title-wrap"><h6 class="orderedlist-title"><span class="name">Test Procedure: </span><a title="Permalink" class="permalink" href="#id-1.11.6.8.2.3">#</a></h6></div><ol class="orderedlist" type="1"><li class="listitem "><p>Crash the primary node by sending a 'fast-reboot' system request.</p><div class="verbatim-wrap"><pre class="screen">suse01# echo 'b' &gt; /proc/sysrq-trigger</pre></div></li></ol></div><div class="orderedlist "><div class="orderedlist-title-wrap"><h6 class="orderedlist-title"><span class="name">Recovery Procedure: </span><a title="Permalink" class="permalink" href="#id-1.11.6.8.2.4">#</a></h6></div><ol class="orderedlist" type="1"><li class="listitem "><p>If SBD fencing is used, pacemaker will not automatically restart after being fenced. In this case clear the fencing flag on all SBD devices and subsequently start pacemaker.</p><div class="verbatim-wrap"><pre class="screen">suse01# sbd -d /dev/disk/by-id/SBDA message suse01 clear
suse01# sbd -d /dev/disk/by-id/SBDB message suse01 clear
...</pre></div></li><li class="listitem "><p>Start the cluster framework</p><div class="verbatim-wrap"><pre class="screen">suse01# systemctl start pacemaker</pre></div></li><li class="listitem "><p>Manually register the old primary (on node 1) with the new primary after takeover (on node 2) as <span class="emphasis"><em>&lt;sid&gt;adm</em></span>.</p><div class="verbatim-wrap"><pre class="screen">suse01# hdbnsutil -sr_register --remoteHost=suse02 --remoteInstance=10 \
               --replicationMode=sync  --operationMode=logreplay \
               --name=WDF</pre></div></li><li class="listitem "><p>Restart the HANA database (now secondary) on node 1 as root.</p><div class="verbatim-wrap"><pre class="screen">suse01# crm resource refresh rsc_SAPHana_HA1_HDB10 suse01</pre></div></li></ol></div><div class="orderedlist "><div class="orderedlist-title-wrap"><h6 class="orderedlist-title"><span class="name">Expected: </span><a title="Permalink" class="permalink" href="#id-1.11.6.8.2.5">#</a></h6></div><ol class="orderedlist" type="1"><li class="listitem "><p>The cluster detects the failed node (node 1) and declares it
UNCLEAN and sets the secondary node (node 2) to status "partition
WITHOUT quorum".</p></li><li class="listitem "><p>The cluster fences the failed node (node 1).</p></li><li class="listitem "><p>The cluster declares the failed node (node 1) OFFLINE.</p></li><li class="listitem "><p>The cluster promotes the secondary HANA database (on node 2) to
take over as primary.</p></li><li class="listitem "><p>The cluster migrates the IP address to the new primary (on node
2).</p></li><li class="listitem "><p>After some time the cluster shows the sync_state of the stopped
primary (on node 2) as SFAIL.</p></li><li class="listitem "><p>If SBD fencing is used, then the manual recovery procedure will be
used to clear the fencing flag and restart pacemaker on the node.</p></li><li class="listitem "><p>Because AUTOMATED_REGISTER="false" the cluster does not restart
the failed HANA database or register it against the new primary.</p></li><li class="listitem "><p>After the manual register and resource refresh the system
replication pair is marked as in sync (SOK).</p></li><li class="listitem "><p>The cluster "failed actions" are cleaned up after following the
recovery procedure.</p></li></ol></div></div></div></div></div><div class="sect3" id="_test_crash_primary_node_on_site_b_node_2"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.1.6 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Test: Crash Primary Node on Site B (Node 2)</span> <a title="Permalink" class="permalink" href="#_test_crash_primary_node_on_site_b_node_2">#</a></h4></div></div></div><div class="complex-example"><div class="example" id="id-1.11.6.9.2"><div class="example-title-wrap"><h6 class="example-title"><span class="number">Example 21: </span><span class="name">Test CRASH_PRIMARY_NODE_SITE_B </span><a title="Permalink" class="permalink" href="#id-1.11.6.9.2">#</a></h6></div><div class="example-contents"><div class="variablelist "><dl class="variablelist"><dt id="id-1.11.6.9.2.2.1"><span class="term ">Component:</span></dt><dd><p>Cluster node of secondary site</p></dd><dt id="id-1.11.6.9.2.2.2"><span class="term ">Description:</span></dt><dd><p>Simulate a crash of the secondary site node running the primary HANA
database.</p></dd></dl></div><div class="orderedlist "><div class="orderedlist-title-wrap"><h6 class="orderedlist-title"><span class="name">Test Procedure: </span><a title="Permalink" class="permalink" href="#id-1.11.6.9.2.3">#</a></h6></div><ol class="orderedlist" type="1"><li class="listitem "><p>Crash the secondary node by sending a 'fast-reboot' system request.</p><div class="verbatim-wrap"><pre class="screen">suse02# echo 'b' &gt; /proc/sysrq-trigger</pre></div></li></ol></div><div class="orderedlist "><div class="orderedlist-title-wrap"><h6 class="orderedlist-title"><span class="name">Recovery Procedure: </span><a title="Permalink" class="permalink" href="#id-1.11.6.9.2.4">#</a></h6></div><ol class="orderedlist" type="1"><li class="listitem "><p>If SBD fencing is used, pacemaker will not automatically restart
after being fenced. In this case clear the fencing flag on all SBD
devices and subsequently start pacemaker.</p><div class="verbatim-wrap"><pre class="screen">suse02# sbd -d /dev/disk/by-id/SBDA message suse02 clear
suse02# sbd -d /dev/disk/by-id/SBDB message suse02 clear
...</pre></div></li><li class="listitem "><p>Start the cluster Framework</p><div class="verbatim-wrap"><pre class="screen">suse02# systemctl start pacemaker</pre></div></li><li class="listitem "><p>Manually register the old primary (on node 2) with the new primary
after takeover (on node 1) as <span class="emphasis"><em>&lt;sid&gt;adm</em></span>.</p><div class="verbatim-wrap"><pre class="screen">suse02# hdbnsutil -sr_register --remoteHost=suse01 --remoteInstance=10 \
               --replicationMode=sync  --operationMode=logreplay \
               --name=ROT</pre></div></li><li class="listitem "><p>Restart the HANA database (now secondary) on node 2 as root.</p><div class="verbatim-wrap"><pre class="screen">suse02# crm resource refresh rsc_SAPHana_HA1_HDB10 suse02</pre></div></li></ol></div><div class="orderedlist "><div class="orderedlist-title-wrap"><h6 class="orderedlist-title"><span class="name">Expected: </span><a title="Permalink" class="permalink" href="#id-1.11.6.9.2.5">#</a></h6></div><ol class="orderedlist" type="1"><li class="listitem "><p>The cluster detects the failed secondary node (node 2) and
declares it UNCLEAN and sets the primary node (node 1) to status
"partition WITHOUT quorum".</p></li><li class="listitem "><p>The cluster fences the failed secondary node (node 2).</p></li><li class="listitem "><p>The cluster declares the failed secondary node (node 2) OFFLINE.</p></li><li class="listitem "><p>The cluster promotes the secondary HANA database (on node 1) to
take over as primary.</p></li><li class="listitem "><p>The cluster migrates the IP address to the new primary (on node
1).</p></li><li class="listitem "><p>After some time the cluster shows the sync_state of the stopped
secondary (on node 2) as SFAIL.</p></li><li class="listitem "><p>If SBD fencing is used, then the manual recovery procedure will be
used to clear the fencing flag and restart pacemaker on the node.</p></li><li class="listitem "><p>Because AUTOMATED_REGISTER="false" the cluster does not restart
the failed HANA database or register it against the new primary.</p></li><li class="listitem "><p>After the manual register and resource refresh the system
replication pair is marked as in sync (SOK).</p></li><li class="listitem "><p>The cluster "failed actions" are cleaned up after following the
recovery procedure.</p></li></ol></div></div></div></div></div><div class="sect3" id="_test_stop_the_secondary_database_on_site_b_node_2"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.1.7 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Test: Stop the Secondary Database on Site B (Node 2)</span> <a title="Permalink" class="permalink" href="#_test_stop_the_secondary_database_on_site_b_node_2">#</a></h4></div></div></div><div class="complex-example"><div class="example" id="id-1.11.6.10.2"><div class="example-title-wrap"><h6 class="example-title"><span class="number">Example 22: </span><span class="name">Test STOP_SECONDARY_DB_SITE_B </span><a title="Permalink" class="permalink" href="#id-1.11.6.10.2">#</a></h6></div><div class="example-contents"><div class="variablelist "><dl class="variablelist"><dt id="id-1.11.6.10.2.2.1"><span class="term ">Component:</span></dt><dd><p>Secondary HANA database</p></dd><dt id="id-1.11.6.10.2.2.2"><span class="term ">Description:</span></dt><dd><p>The secondary HANA database is stopped during normal cluster
operation.</p></dd></dl></div><div class="orderedlist "><div class="orderedlist-title-wrap"><h6 class="orderedlist-title"><span class="name">Test Procedure: </span><a title="Permalink" class="permalink" href="#id-1.11.6.10.2.3">#</a></h6></div><ol class="orderedlist" type="1"><li class="listitem "><p>Stop the secondary HANA database gracefully as <span class="emphasis"><em>&lt;sid&gt;adm</em></span>.</p><div class="verbatim-wrap"><pre class="screen">suse02# HDB stop</pre></div></li></ol></div><div class="orderedlist "><div class="orderedlist-title-wrap"><h6 class="orderedlist-title"><span class="name">Recovery Procedure: </span><a title="Permalink" class="permalink" href="#id-1.11.6.10.2.4">#</a></h6></div><ol class="orderedlist" type="1"><li class="listitem "><p>Refresh the failed resource status of the secondary HANA database (on
node 2) as root.</p><div class="verbatim-wrap"><pre class="screen">suse02# crm resource refresh rsc_SAPHana_HA1_HDB10 suse02</pre></div></li></ol></div><div class="orderedlist "><div class="orderedlist-title-wrap"><h6 class="orderedlist-title"><span class="name">Expected: </span><a title="Permalink" class="permalink" href="#id-1.11.6.10.2.5">#</a></h6></div><ol class="orderedlist" type="1"><li class="listitem "><p>The cluster detects the stopped secondary database (on node 2) and
marks the resource failed.</p></li><li class="listitem "><p>The cluster detects the broken system replication and marks it as
failed (SFAIL).</p></li><li class="listitem "><p>The cluster restarts the secondary HANA database on the same node
(node 2).</p></li><li class="listitem "><p>The cluster detects that the system replication is in sync again
and marks it as ok (SOK).</p></li><li class="listitem "><p>The cluster "failed actions" are cleaned up after following the
recovery procedure.</p></li></ol></div></div></div></div></div><div class="sect3" id="_test_crash_the_secondary_database_on_site_b_node_2"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.1.8 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Test: Crash the Secondary Database on Site B (Node 2)</span> <a title="Permalink" class="permalink" href="#_test_crash_the_secondary_database_on_site_b_node_2">#</a></h4></div></div></div><div class="complex-example"><div class="example" id="id-1.11.6.11.2"><div class="example-title-wrap"><h6 class="example-title"><span class="number">Example 23: </span><span class="name">Test CRASH_SECONDARY_DB_SITE_B </span><a title="Permalink" class="permalink" href="#id-1.11.6.11.2">#</a></h6></div><div class="example-contents"><div class="variablelist "><dl class="variablelist"><dt id="id-1.11.6.11.2.2.1"><span class="term ">Component:</span></dt><dd><p>Secondary HANA database</p></dd><dt id="id-1.11.6.11.2.2.2"><span class="term ">Description:</span></dt><dd><p>Simulate a complete break-down of the secondary database system.</p></dd></dl></div><div class="orderedlist "><div class="orderedlist-title-wrap"><h6 class="orderedlist-title"><span class="name">Test Procedure: </span><a title="Permalink" class="permalink" href="#id-1.11.6.11.2.3">#</a></h6></div><ol class="orderedlist" type="1"><li class="listitem "><p>Kill the secondary database system using signals as <span class="emphasis"><em>&lt;sid&gt;adm</em></span>.</p><div class="verbatim-wrap"><pre class="screen">suse02# HDB kill-9</pre></div></li></ol></div><div class="orderedlist "><div class="orderedlist-title-wrap"><h6 class="orderedlist-title"><span class="name">Recovery Procedure: </span><a title="Permalink" class="permalink" href="#id-1.11.6.11.2.4">#</a></h6></div><ol class="orderedlist" type="1"><li class="listitem "><p>Clean up the failed resource status of the secondary HANA database (on node 2) as root.</p><div class="verbatim-wrap"><pre class="screen">suse02# crm resource refresh rsc_SAPHana_HA1_HDB10 suse02</pre></div></li></ol></div><div class="orderedlist "><div class="orderedlist-title-wrap"><h6 class="orderedlist-title"><span class="name">Expected: </span><a title="Permalink" class="permalink" href="#id-1.11.6.11.2.5">#</a></h6></div><ol class="orderedlist" type="1"><li class="listitem "><p>The cluster detects the stopped secondary database (on node 2) and
marks the resource failed.</p></li><li class="listitem "><p>The cluster detects the broken system replication and marks it as
failed (SFAIL).</p></li><li class="listitem "><p>The cluster restarts the secondary HANA database on the same node
(node 2).</p></li><li class="listitem "><p>The cluster detects that the system replication is in sync again
and marks it as ok (SOK).</p></li><li class="listitem "><p>The cluster "failed actions" are cleaned up after following the
recovery procedure.</p></li></ol></div></div></div></div></div><div class="sect3" id="_test_crash_the_secondary_node_on_site_b_node2"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.1.9 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Test: Crash the Secondary Node on Site B (Node2)</span> <a title="Permalink" class="permalink" href="#_test_crash_the_secondary_node_on_site_b_node2">#</a></h4></div></div></div><div class="complex-example"><div class="example" id="id-1.11.6.12.2"><div class="example-title-wrap"><h6 class="example-title"><span class="number">Example 24: </span><span class="name">Test CRASH_SECONDARY_NODE_SITE_B </span><a title="Permalink" class="permalink" href="#id-1.11.6.12.2">#</a></h6></div><div class="example-contents"><div class="variablelist "><dl class="variablelist"><dt id="id-1.11.6.12.2.2.1"><span class="term ">Component:</span></dt><dd><p>Cluster node of secondary site</p></dd><dt id="id-1.11.6.12.2.2.2"><span class="term ">Description:</span></dt><dd><p>Simulate a crash of the secondary site node running the seconday HANA
database.</p></dd></dl></div><div class="orderedlist "><div class="orderedlist-title-wrap"><h6 class="orderedlist-title"><span class="name">Test Procedure: </span><a title="Permalink" class="permalink" href="#id-1.11.6.12.2.3">#</a></h6></div><ol class="orderedlist" type="1"><li class="listitem "><p>Crash the secondary node by sending a 'fast-reboot' system request.</p><div class="verbatim-wrap"><pre class="screen">suse02# echo 'b' &gt; /proc/sysrq-trigger</pre></div></li></ol></div><div class="orderedlist "><div class="orderedlist-title-wrap"><h6 class="orderedlist-title"><span class="name">Recovery Procedure: </span><a title="Permalink" class="permalink" href="#id-1.11.6.12.2.4">#</a></h6></div><ol class="orderedlist" type="1"><li class="listitem "><p>If SBD fencing is used, pacemaker will not automatically restart
after being fenced. In this case clear the fencing flag on <span class="strong"><strong>all</strong></span> SBD
devices and subsequently start pacemaker.</p><div class="verbatim-wrap"><pre class="screen">suse02# sbd -d /dev/disk/by-id/SBDA message suse02 clear
suse02# sbd -d /dev/disk/by-id/SBDB message suse02 clear
...</pre></div></li><li class="listitem "><p>Start the cluster framework.</p><div class="verbatim-wrap"><pre class="screen">suse02# systemctl start pacemaker</pre></div></li></ol></div><div class="orderedlist "><div class="orderedlist-title-wrap"><h6 class="orderedlist-title"><span class="name">Expected: </span><a title="Permalink" class="permalink" href="#id-1.11.6.12.2.5">#</a></h6></div><ol class="orderedlist" type="1"><li class="listitem "><p>The cluster detects the failed secondary node (node 2) and
declares it UNCLEAN and sets the primary node (node 1) to status
"partition WITHOUT quorum".</p></li><li class="listitem "><p>The cluster fences the failed secondary node (node 2).</p></li><li class="listitem "><p>The cluster declares the failed secondary node (node 2) OFFLINE.</p></li><li class="listitem "><p>After some time the cluster shows the sync_state of the stopped
secondary (on node 2) as SFAIL.</p></li><li class="listitem "><p>If SBD fencing is used, then the manual recovery procedure will be
used to clear the fencing flag and restart pacemaker on the node.</p></li><li class="listitem "><p>When the fenced node (node 2) rejoins the cluster the former
secondary HANA database is started automatically.</p></li><li class="listitem "><p>The cluster detects that the system replication is in sync again
and marks it as ok (SOK).</p></li></ol></div></div></div></div></div><div class="sect3" id="_test_failure_of_replication_lan"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.1.10 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Test: Failure of Replication LAN</span> <a title="Permalink" class="permalink" href="#_test_failure_of_replication_lan">#</a></h4></div></div></div><div class="complex-example"><div class="example" id="id-1.11.6.13.2"><div class="example-title-wrap"><h6 class="example-title"><span class="number">Example 25: </span><span class="name">Test FAIL_NETWORK_SR </span><a title="Permalink" class="permalink" href="#id-1.11.6.13.2">#</a></h6></div><div class="example-contents"><div class="variablelist "><dl class="variablelist"><dt id="id-1.11.6.13.2.2.1"><span class="term ">Component:</span></dt><dd><p>Replication LAN</p></dd><dt id="id-1.11.6.13.2.2.2"><span class="term ">Description:</span></dt><dd><p>Loss of replication LAN connectivity between the primary and secondary
node.</p></dd></dl></div><div class="orderedlist "><div class="orderedlist-title-wrap"><h6 class="orderedlist-title"><span class="name">Test Procedure: </span><a title="Permalink" class="permalink" href="#id-1.11.6.13.2.3">#</a></h6></div><ol class="orderedlist" type="1"><li class="listitem "><p>Break the connection between the cluster nodes on the replication LAN.</p></li></ol></div><div class="orderedlist "><div class="orderedlist-title-wrap"><h6 class="orderedlist-title"><span class="name">Recovery Procedure: </span><a title="Permalink" class="permalink" href="#id-1.11.6.13.2.4">#</a></h6></div><ol class="orderedlist" type="1"><li class="listitem "><p>Re-establish the connection between the cluster nodes on the
replication LAN.</p></li></ol></div><div class="orderedlist "><div class="orderedlist-title-wrap"><h6 class="orderedlist-title"><span class="name">Expected: </span><a title="Permalink" class="permalink" href="#id-1.11.6.13.2.5">#</a></h6></div><ol class="orderedlist" type="1"><li class="listitem "><p>After some time the cluster shows the sync_state of the secondary
(on node 2) as SFAIL.</p></li><li class="listitem "><p>The primary HANA database (node 1) "HDBSettings.sh
systemReplicationStatus.py" shows "CONNECTION TIMEOUT" and the
secondary HANA database (node 2) is not able to reach the primary
database (node 1).</p></li><li class="listitem "><p>The primary HANA database continues to operate as “normal”, but no
system replication takes place and is therefore no longer a valid take
over destination.</p></li><li class="listitem "><p>When the LAN connection is re-established, HDB automatically
detects connectivity between the HANA databases and restarts the
system replication process</p></li><li class="listitem "><p>The cluster detects that the system replication is in sync again
and marks it as ok (SOK).</p></li></ol></div></div></div></div></div></div><div class="sect2" id="_test_cases_for_full_automation"><div class="titlepage"><div><div><h3 class="title"><span class="number">10.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Test Cases for Full Automation</span> <a title="Permalink" class="permalink" href="#_test_cases_for_full_automation">#</a></h3></div></div></div><p>In the following test descriptions we assume
<span class="strong"><strong>PREFER_SITE_TAKEOVER="true"</strong></span> and <span class="strong"><strong>AUTOMATED_REGISTER="true".</strong></span></p><div id="id-1.11.7.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>The following tests are designed to be run in sequence and depend
on the exit state of the proceeding tests.</p></div><div class="sect3" id="_test_stop_primary_database_on_site_a"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Test: Stop Primary Database on Site A</span> <a title="Permalink" class="permalink" href="#_test_stop_primary_database_on_site_a">#</a></h4></div></div></div><div class="complex-example"><div class="example" id="id-1.11.7.4.2"><div class="example-title-wrap"><h6 class="example-title"><span class="number">Example 26: </span><span class="name">Test STOP_PRIMARY_DB_SITE_A </span><a title="Permalink" class="permalink" href="#id-1.11.7.4.2">#</a></h6></div><div class="example-contents"><div class="itemizedlist "><div class="itemizedlist-title-wrap"><h6 class="itemizedlist-title"><span class="name">Component: </span><a title="Permalink" class="permalink" href="#id-1.11.7.4.2.2">#</a></h6></div><ul class="itemizedlist"><li class="listitem "><p>Primary Database</p></li></ul></div><div class="itemizedlist "><div class="itemizedlist-title-wrap"><h6 class="itemizedlist-title"><span class="name">Description: </span><a title="Permalink" class="permalink" href="#id-1.11.7.4.2.3">#</a></h6></div><ul class="itemizedlist"><li class="listitem "><p>The primary HANA database is stopped during normal cluster operation.</p></li></ul></div><div class="itemizedlist "><div class="itemizedlist-title-wrap"><h6 class="itemizedlist-title"><span class="name">Test Procedure: </span><a title="Permalink" class="permalink" href="#id-1.11.7.4.2.4">#</a></h6></div><ul class="itemizedlist"><li class="listitem "><p>Stop the primary HANA database gracefully as <span class="emphasis"><em>&lt;sid&gt;adm</em></span>.</p></li></ul></div><div class="verbatim-wrap"><pre class="screen">suse01# HDB stop</pre></div><div class="orderedlist "><div class="orderedlist-title-wrap"><h6 class="orderedlist-title"><span class="name">Recovery Procedure: </span><a title="Permalink" class="permalink" href="#id-1.11.7.4.2.6">#</a></h6></div><ol class="orderedlist" type="1"><li class="listitem "><p>Not needed, everything is automated</p></li><li class="listitem "><p>Refresh the cluster resources on node 1 as root.</p></li></ol></div><div class="verbatim-wrap"><pre class="screen">suse01# crm resource refresh rsc_SAPHana_HA1_HDB10 suse01</pre></div><div class="orderedlist "><div class="orderedlist-title-wrap"><h6 class="orderedlist-title"><span class="name">Expected: </span><a title="Permalink" class="permalink" href="#id-1.11.7.4.2.8">#</a></h6></div><ol class="orderedlist" type="1"><li class="listitem "><p>The cluster detects the stopped primary HANA database (on node 1)
and marks the resource failed.</p></li><li class="listitem "><p>The cluster promotes the secondary HANA database (on node 2) to
take over as primary.</p></li><li class="listitem "><p>The cluster migrates the IP address to the new primary (on node 2).</p></li><li class="listitem "><p>After some time the cluster shows the sync_state of the stopped
primary (on node 1) as SFAIL.</p></li><li class="listitem "><p>Because AUTOMATED_REGISTER="true" the cluster does restart
the failed HANA database and register it against the new primary.</p></li><li class="listitem "><p>After the automated register and resource refresh the system
replication pair is marked as in sync (SOK).</p></li><li class="listitem "><p>The cluster "failed actions" are cleaned up after following the
recovery procedure.</p></li></ol></div></div></div></div></div><div class="sect3" id="_test_crash_the_primary_node_on_site_b_node_2"><div class="titlepage"><div><div><h4 class="title"><span class="number">10.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Test: Crash the Primary Node on Site B (Node 2)</span> <a title="Permalink" class="permalink" href="#_test_crash_the_primary_node_on_site_b_node_2">#</a></h4></div></div></div><div class="complex-example"><div class="example" id="id-1.11.7.5.2"><div class="example-title-wrap"><h6 class="example-title"><span class="number">Example 27: </span><span class="name">Test CRASH_PRIMARY_NODE_SITE_B </span><a title="Permalink" class="permalink" href="#id-1.11.7.5.2">#</a></h6></div><div class="example-contents"><div class="itemizedlist "><div class="itemizedlist-title-wrap"><h6 class="itemizedlist-title"><span class="name">Component: </span><a title="Permalink" class="permalink" href="#id-1.11.7.5.2.2">#</a></h6></div><ul class="itemizedlist"><li class="listitem "><p>Cluster node of site B</p></li></ul></div><div class="itemizedlist "><div class="itemizedlist-title-wrap"><h6 class="itemizedlist-title"><span class="name">Description: </span><a title="Permalink" class="permalink" href="#id-1.11.7.5.2.3">#</a></h6></div><ul class="itemizedlist"><li class="listitem "><p>Simulate a crash of the site B node running the primary HANA database.</p></li></ul></div><div class="itemizedlist "><div class="itemizedlist-title-wrap"><h6 class="itemizedlist-title"><span class="name">Test Procedure: </span><a title="Permalink" class="permalink" href="#id-1.11.7.5.2.4">#</a></h6></div><ul class="itemizedlist"><li class="listitem "><p>Crash the secondary node by sending a 'fast-reboot' system request.</p></li></ul></div><div class="verbatim-wrap"><pre class="screen">suse02# echo 'b' &gt; /proc/sysrq-trigger</pre></div><div class="itemizedlist "><div class="itemizedlist-title-wrap"><h6 class="itemizedlist-title"><span class="name">Recovery Procedure: </span><a title="Permalink" class="permalink" href="#id-1.11.7.5.2.6">#</a></h6></div><ul class="itemizedlist"><li class="listitem "><p>If SBD fencing is used, pacemaker will not automatically restart
after being fenced. In this case clear the fencing flag on <span class="strong"><strong>all</strong></span> SBD
devices and subsequently start pacemaker.</p></li></ul></div><div class="verbatim-wrap"><pre class="screen">suse02# sbd -d /dev/disk/by-id/SBDA message suse02 clear
suse02# sbd -d /dev/disk/by-id/SBDB message suse02 clear
...</pre></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Start the cluster framework.</p></li></ul></div><div class="verbatim-wrap"><pre class="screen">suse02# systemctl start pacemaker</pre></div><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Refresh the cluster resources on node 2 as root.</p></li></ul></div><div class="verbatim-wrap"><pre class="screen">suse02# crm resource refresh rsc_SAPHana_HA1_HDB10 suse02</pre></div><div class="orderedlist "><div class="orderedlist-title-wrap"><h6 class="orderedlist-title"><span class="name">Expected: </span><a title="Permalink" class="permalink" href="#id-1.11.7.5.2.12">#</a></h6></div><ol class="orderedlist" type="1"><li class="listitem "><p>The cluster detects the failed primary node (node 2) and
declares it UNCLEAN and sets the primary node (node 2) to status
"partition WITHOUT quorum".</p></li><li class="listitem "><p>The cluster fences the failed primary node (node 2).</p></li><li class="listitem "><p>The cluster declares the failed primary node (node 2) OFFLINE.</p></li><li class="listitem "><p>The cluster promotes the secondary HANA database (on node 1) to
take over as primary.</p></li><li class="listitem "><p>The cluster migrates the IP address to the new primary (on node 1).</p></li><li class="listitem "><p>After some time the cluster shows the sync_state of the stopped
secondary (on node 2) as SFAIL.</p></li><li class="listitem "><p>If SBD fencing is used, then the manual recovery procedure will be
used to clear the fencing flag and restart pacemaker on the node.</p></li><li class="listitem "><p>When the fenced node (node 2) rejoins the cluster the former primary became a secondary.</p></li><li class="listitem "><p>Because AUTOMATED_REGISTER="true" the cluster does restart
the failed HANA database and register it against the new primary.</p></li><li class="listitem "><p>The cluster detects that the system replication is in sync again
and marks it as ok (SOK).</p></li></ol></div></div></div></div></div></div></div><div class="sect1" id="cha.hana-sr.administrate"><div class="titlepage"><div><div><h2 class="title"><span class="number">11 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Administration</span> <a title="Permalink" class="permalink" href="#cha.hana-sr.administrate">#</a></h2></div></div></div><div class="sect2" id="_dos_and_donts"><div class="titlepage"><div><div><h3 class="title"><span class="number">11.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Do’s and Don’ts</span> <a title="Permalink" class="permalink" href="#_dos_and_donts">#</a></h3></div></div></div><p>In your project, you should:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Define STONITH before adding other resources to the cluster</p></li><li class="listitem "><p>Do intensive testing.</p></li><li class="listitem "><p>Tune the timeouts of operations of SAPHana and SAPHanaTopology.</p></li><li class="listitem "><p>Start with PREFER_SITE_TAKEOVER=”true”, AUTOMATED_REGISTER=”false” and
DUPLICATE_PRIMARY_TIMEOUT=”7200”.</p></li></ul></div><p>In your project, avoid:</p><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Rapidly changing/changing back cluster configuration, such as: Setting
nodes to standby and online again or stopping/starting the master/slave
resource.</p></li><li class="listitem "><p>Creating a cluster without proper time synchronization or unstable
name resolutions for hosts, users and groups</p></li><li class="listitem "><p>Adding location rules for the clone, master/slave or IP resource. Only
location rules mentioned in this setup guide are allowed.</p></li><li class="listitem "><p>As "migrating" or "moving" resources in crm-shell, HAWK or other tools
would add client-prefer location rules this activities are completely
forbidden.</p></li></ul></div></div><div class="sect2" id="_monitoring_and_tools"><div class="titlepage"><div><div><h3 class="title"><span class="number">11.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Monitoring and Tools</span> <a title="Permalink" class="permalink" href="#_monitoring_and_tools">#</a></h3></div></div></div><p>You can use the High Availability Web Console (HAWK), SAP HANA Studio and
different command line tools for cluster status requests.</p><div class="sect3" id="_hawk_cluster_status_and_more"><div class="titlepage"><div><div><h4 class="title"><span class="number">11.2.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">HAWK – Cluster Status and more</span> <a title="Permalink" class="permalink" href="#_hawk_cluster_status_and_more">#</a></h4></div></div></div><p>You can use an Internet browser to check the cluster status.</p><div class="figure" id="id-1.12.3.3.3"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/SAPHanaSR-ScaleUp-HAWK-Status-SLE12.png" target="_blank"><img src="images/SAPHanaSR-ScaleUp-HAWK-Status-SLE12.png" width="" alt="SAPHanaSR ScaleUp HAWK Status SLE12" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 9: </span><span class="name">Cluster Status in HAWK </span><a title="Permalink" class="permalink" href="#id-1.12.3.3.3">#</a></h6></div></div><p>If you set up the cluster using ha-cluster-init and you have installed all
packages as described above, your system will provide a very useful Web
interface. You can use this graphical Web interface to get an overview
of the complete cluster status, perform administrative tasks or configure resources and cluster bootstrap parameters. Read our product
manuals for a complete documentation of this powerful user interface.</p></div><div class="sect3" id="_sap_hana_studio"><div class="titlepage"><div><div><h4 class="title"><span class="number">11.2.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SAP HANA Studio</span> <a title="Permalink" class="permalink" href="#_sap_hana_studio">#</a></h4></div></div></div><p>Database-specific administration and checks can be done with SAP HANA
studio.</p><div class="figure" id="id-1.12.3.4.3"><div class="figure-contents"><div class="mediaobject"><a xmlns="" href="images/hana_studio_landscape.png" target="_blank"><img src="images/hana_studio_landscape.png" width="" alt="hana studio landscape" /></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="number">Figure 10: </span><span class="name">SAP HANA Studio – Landscape </span><a title="Permalink" class="permalink" href="#id-1.12.3.4.3">#</a></h6></div></div></div><div class="sect3" id="_cluster_command_line_tools"><div class="titlepage"><div><div><h4 class="title"><span class="number">11.2.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Cluster Command Line Tools</span> <a title="Permalink" class="permalink" href="#_cluster_command_line_tools">#</a></h4></div></div></div><p>A simple overview can be obtained by calling <code class="literal">crm_mon</code>. Using option
<code class="literal">-r</code> shows also stopped but already configured resources. Option <code class="literal">-1</code>
tells <code class="literal">crm_mon</code> to output the status once instead of periodically.</p><div class="verbatim-wrap"><pre class="screen">Stack: corosync
Current DC: suse01 (version 2.0.1+20190417.13d370ca9-3.6.1-2.0.1+20190417.13d370ca9) - partition with quorum
Last updated: Thu Feb  6 12:20:03 2020
Last change: Thu Feb  6 12:19:43 2020 by root via crm_attribute on suse01

2 nodes configured
6 resources configured

Online: [ suse01 suse02 ]

Full list of resources:

 stonith-sbd    (stonith:external/sbd): Started suse01
 Clone Set: cln_SAPHanaTopology_HA1_HDB10 [rsc_SAPHanaTopology_HA1_HDB10]
     Started: [ suse01 suse02 ]
 Clone Set: msl_SAPHana_HA1_HDB10 [rsc_SAPHana_HA1_HDB10] (promotable)
     Masters: [ suse01 ]
     Slaves: [ suse02 ]
 rsc_ip_HA1_HDB10       (ocf::heartbeat:IPaddr2):       Started suse01</pre></div><p>See the manual page crm_mon(8) for details.</p></div><div class="sect3" id="_saphanasr_command_line_tools"><div class="titlepage"><div><div><h4 class="title"><span class="number">11.2.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SAPHanaSR Command Line Tools</span> <a title="Permalink" class="permalink" href="#_saphanasr_command_line_tools">#</a></h4></div></div></div><p>To show some SAPHana or SAPHanaTopology resource agent internal
values, you can call the program <code class="literal">SAPHanaSR-showAttr</code>. The internal
values, the storage location and their parameter names may change in the next
versions. The command <code class="literal">SAPHanaSR-showAttr</code> will always fetch the values
from the correct storage location.</p><p>Do not use cluster commands like <code class="literal">crm_attribute</code> to fetch the values
directly from the cluster. If you use such commands, your methods will be
broken when you need to move an attribute to a different storage place
or even out of the cluster. At first <span class="emphasis"><em>SAPHanaSR-showAttr</em></span> is a test
program only and should not be used for automated system monitoring.</p><div class="verbatim-wrap"><pre class="screen"> suse01:~ # SAPHanaSR-showAttr
 Host \ Attr clone_state remoteHost roles       ... site    srmode sync_state ...
 ---------------------------------------------------------------------------------
 suse01      PROMOTED    suse02     4:P:master1:... WDF      sync  PRIM       ...
 suse02      DEMOTED     suse01     4:S:master1:... ROT      sync  SOK        ...</pre></div><p><code class="literal">SAPHanaSR-showAttr</code> also supports other output formats such as <span class="strong"><strong>script</strong></span>. The script
format is intended to allow running filters. The SAPHanaSR package beginning with
version 0.153 also provides a filter engine <code class="literal">SAPHanaSR-filter</code>. In combination of
<code class="literal">SAPHanaSR-showAttr</code> with output format script and <code class="literal">SAPHanaSR-filter</code> you can define
effective queries:</p><div class="verbatim-wrap"><pre class="screen">suse01:~ # SAPHanaSR-showAttr --format=script | \
   SAPHanaSR-filter --search='remote'
Thu Feb  6 12:28:10 2020; Hosts/suse01/remoteHost=suse02
Thu Feb  6 12:28:10 2020; Hosts/suse02/remoteHost=suse01</pre></div><p><code class="literal">SAPHanaSR-replay-archive</code> can help to analyze the SAPHanaSR attribute values from
<code class="literal">hb_report</code> (<code class="literal">crm_report</code>) archives. This allows post mortem analyzes.</p><p>In our example, the administrator killed the primary SAP HANA instance using the command
<code class="literal">HDB kill-9</code>. This happened around 9:10 pm.</p><div class="verbatim-wrap"><pre class="screen">suse01:~ # hb_report -f 19:00
INFO: suse01# The report is saved in ./hb_report-1-11-11-2019.tar.bz2
INFO: suse01# Report timespan: 11/11/19 19:00:00 - 11/11/19 21:05:33
INFO: suse01# Thank you for taking time to create this report.
suse01:~ # SAPHanaSR-replay-archive --format=script \
    ./hb_report-1-11-11-2019.tar.bz2 | \
    SAPHanaSR-filter --search='roles' --filterDouble
Mon Nov 11 20:38:01 2019; Hosts/suse01/roles=4:P:master1:master:worker:master
Mon Nov 11 20:38:01 2019; Hosts/suse02/roles=4:S:master1:master:worker:master
Mon Nov 11 21:11:37 2019; Hosts/suse01/roles=1:P:master1::worker:
Mon Nov 11 21:12:43 2019; Hosts/suse02/roles=4:P:master1:master:worker:master</pre></div><p>In the above example the attributes indicate that at the beginning suse01
was running primary (4:P) and suse02 was running secondary (4:S).</p><p>At 21:11 (CET) suddenly the primary on suse01 died - it was falling down to 1:P.</p><p>The cluster did jump-in and initiated a takeover. At 21:12 (CET) the former secondary
was detected as new running master (changing from 4:S to 4:P).</p></div><div class="sect3" id="_sap_hana_landscapehostconfiguration"><div class="titlepage"><div><div><h4 class="title"><span class="number">11.2.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SAP HANA LandscapeHostConfiguration</span> <a title="Permalink" class="permalink" href="#_sap_hana_landscapehostconfiguration">#</a></h4></div></div></div><p>To check the status of an SAPHana database and to find out if the
cluster should react, you can use the script <span class="strong"><strong>landscapeHostConfiguration</strong></span>
to be called as Linux user <span class="emphasis"><em>&lt;sid&gt;adm</em></span>.</p><div class="verbatim-wrap"><pre class="screen">suse01:~&gt; HDBSettings.sh landscapeHostConfiguration.py
| Host   | Host   | ... NameServer   | NameServer  | IndexServer | IndexServer |
|        | Active | ... Config Role  | Actual Role | Config Role | Actual Role |
| ------ | ------ | ... ------------ | ----------- | ----------- | ----------- |
| suse01 | yes    | ... master 1     | master      | worker      | master      |

overall host status: ok</pre></div><p>Following the SAP HA guideline, the SAPHana resource agent interprets
the return codes in the following way:</p><div class="table" id="id-1.12.3.7.5"><div class="table-title-wrap"><h6 class="table-title"><span class="number">Table 6: </span><span class="name">Interpretation of Return Codes </span><a title="Permalink" class="permalink" href="#id-1.12.3.7.5">#</a></h6></div><div class="table-contents"><table class="table" summary="Interpretation of Return Codes" border="1" width="100%"><colgroup><col class="col_1" /><col class="col_2" /></colgroup><thead><tr><th align="left" valign="top">Return Code</th><th align="left" valign="top">Interpretation</th></tr></thead><tbody><tr><td align="left" valign="top"><p>4</p></td><td align="left" valign="top"><p>SAP HANA database is up and OK. The cluster does interpret this as a
correctly running database.</p></td></tr><tr><td align="left" valign="top"><p>3</p></td><td align="left" valign="top"><p>SAP HANA database is up and in status info. The cluster does
interpret this as a correctly running database.</p></td></tr><tr><td align="left" valign="top"><p>2</p></td><td align="left" valign="top"><p>SAP HANA database is up and in status warning. The cluster does
interpret this as a correctly running database.</p></td></tr><tr><td align="left" valign="top"><p>1</p></td><td align="left" valign="top"><p>SAP HANA database is down. If the database should be up and is not
down by intention, this could trigger a takeover.</p></td></tr><tr><td align="left" valign="top"><p>0</p></td><td align="left" valign="top"><p>Internal Script Error – to be ignored.</p></td></tr></tbody></table></div></div></div></div><div class="sect2" id="_maintenance"><div class="titlepage"><div><div><h3 class="title"><span class="number">11.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Maintenance</span> <a title="Permalink" class="permalink" href="#_maintenance">#</a></h3></div></div></div><p>To receive updates for the operating system or the SUSE Linux Enterprise High Availability Extension,
it is recommended to register your systems to either a local SUSE Manager or SMT or remotely with SUSE Customer Center.</p><div class="sect3" id="_updating_the_os_and_cluster"><div class="titlepage"><div><div><h4 class="title"><span class="number">11.3.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Updating the OS and Cluster</span> <a title="Permalink" class="permalink" href="#_updating_the_os_and_cluster">#</a></h4></div></div></div><p>For an update of SUSE Linux Enterprise Server for SAP Applications packages including cluster software follow the
rolling update procedure defined in the product documentation
of SUSE Linux Enterprise High Availability Extension <span class="emphasis"><em>Upgrading Your Cluster and Updating Software Packages</em></span>
 High Availability Administration Guide.</p></div><div class="sect3" id="_updating_sap_hana_seamless_sap_hana_maintenance"><div class="titlepage"><div><div><h4 class="title"><span class="number">11.3.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Updating SAP HANA - Seamless SAP HANA Maintenance</span> <a title="Permalink" class="permalink" href="#_updating_sap_hana_seamless_sap_hana_maintenance">#</a></h4></div></div></div><p>For updating SAP HANA database systems in system replication you need to
follow the defined SAP processes. This section describes the steps to be
done before and after the update procedure to get the system replication
automated again.</p><p>SUSE has optimized the SAP HANA maintenance process in the cluster.
The improved procedure only sets the master-slave-resource to maintenance and
keep the rest of the cluster (SAPHanaTopology clones and IPaddr2 vIP resource)
still active. Using the updated procedure allows a seamless SAP HANA maintenance
in the cluster as the virtual IP address can automatically follow the running
primary.</p><p>Prepare the cluster not to react on the maintenance work to be done on
the SAP HANA database systems. Set the master/slave resource to be
unmanaged and the cluster nodes in maintenance mode.</p><div class="complex-example"><div class="example" id="id-1.12.4.4.5"><div class="example-title-wrap"><h6 class="example-title"><span class="number">Example 28: </span><span class="name">Main SAP HANA Update procedure </span><a title="Permalink" class="permalink" href="#id-1.12.4.4.5">#</a></h6></div><div class="example-contents"><div class="variablelist "><dl class="variablelist"><dt id="id-1.12.4.4.5.2.1"><span class="term ">Pre Update Task</span></dt><dd><p>For the master-slave-resource set the maintenance mode:</p><div class="verbatim-wrap"><pre class="screen">crm resource maintenance &lt;master-slave-resource&gt;</pre></div><p>The &lt;master-slave-resource&gt; in the given guide is <code class="literal">msl_SAPHana_HA1_HDB10</code>.</p></dd><dt id="id-1.12.4.4.5.2.2"><span class="term ">Update</span></dt><dd><p>Process the SAP Update for both SAP HANA database systems. This
procedure is described by SAP.</p></dd><dt id="id-1.12.4.4.5.2.3"><span class="term ">Post Update Task</span></dt><dd><p>Expect the primary/secondary roles to be exchanged
after the maintenance. Therefore, tell the cluster to forget about these states and to
reprobe the updated SAP HANA database systems.</p><div class="verbatim-wrap"><pre class="screen">crm resource refresh &lt;master-slave-resource&gt;</pre></div><p>After the SAP HANA update is complete on both sites, tell the cluster
about the end of the maintenance process. This allows the cluster to
actively control and monitor the SAP again.</p><div class="verbatim-wrap"><pre class="screen">crm resource maintenance &lt;master-slave-resource&gt; off</pre></div></dd></dl></div></div></div></div></div><div class="sect3" id="_migrating_an_sap_hana_primary"><div class="titlepage"><div><div><h4 class="title"><span class="number">11.3.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Migrating an SAP HANA Primary</span> <a title="Permalink" class="permalink" href="#_migrating_an_sap_hana_primary">#</a></h4></div></div></div><p>In the following procedures we assume the primary to be running on node1
and the secondary on node2. The goal is to "exchange" the roles of the
nodes, so finally the primary should run on node2 and the secondary
should run on node1.</p><p>There are different methods to get the exchange of the roles done. The
following procedure shows how to tell the cluster to "accept" a role
change via native HANA commands.</p><div class="complex-example"><div class="example" id="id-1.12.4.5.4"><div class="example-title-wrap"><h6 class="example-title"><span class="number">Example 29: </span><span class="name">Migrating an SAP HANA primary using SAP Toolset </span><a title="Permalink" class="permalink" href="#id-1.12.4.5.4">#</a></h6></div><div class="example-contents"><div class="variablelist "><dl class="variablelist"><dt id="id-1.12.4.5.4.2.1"><span class="term ">Pre move</span></dt><dd><p>Set the master-slave-resource to be maintenance. This could be done on any
cluster node.</p><div class="verbatim-wrap"><pre class="screen">crm resource maintenance &lt;master-slave-resource-name&gt;</pre></div></dd></dl></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.12.4.5.4.3.1"><span class="term ">Manual Takeover Process</span></dt><dd><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Stop the primary SAP HANA database system. Enter the command in our
example on node1 as user <span class="emphasis"><em>&lt;sid&gt;adm</em></span>.</p><div class="verbatim-wrap"><pre class="screen">HDB stop</pre></div></li><li class="listitem "><p>Start the takeover process on the secondary SAP HANA database system.
Enter the command in our example on node2 as user <span class="emphasis"><em>&lt;sid&gt;adm</em></span>.</p><div class="verbatim-wrap"><pre class="screen">hdbnsutil -sr_takeover</pre></div></li><li class="listitem "><p>Register the former primary to become the new secondary. Enter the
command in our example on node1 as user <span class="emphasis"><em>&lt;sid&gt;adm</em></span>.</p><div class="verbatim-wrap"><pre class="screen">hdbnsutil -sr_register --remoteHost=suse02 --remoteInstance=10 \
 --replicationMode=sync --name=WDF \
 --operationMode=logreplay</pre></div></li><li class="listitem "><p>Start the new secondary SAP HANA database system. Enter the command in
our example on node1 as user <span class="emphasis"><em>&lt;sid&gt;adm</em></span>.</p><div class="verbatim-wrap"><pre class="screen">HDB start</pre></div></li></ul></div></dd><dt id="id-1.12.4.5.4.3.2"><span class="term ">Post Migrate</span></dt><dd><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Wait some time till SAPHanaSR-showAttr shows both SAP HANA database
systems to be up again (field roles must start with the digit 4). The new
secondary should have role "S" (for secondary).</p></li><li class="listitem "><p>Tell the cluster to forget about the former master-slave roles and to
re-monitor the failed master. The command could be submitted on any
cluster node as user root.</p><div class="verbatim-wrap"><pre class="screen">crm resource refresh master-slave-resource-name</pre></div></li><li class="listitem "><p>Set the master/slave resource to the status managed again. The command
could be submitted on any cluster node as user root.</p><div class="verbatim-wrap"><pre class="screen">crm resource maintenance &lt;master-slave-resource-name&gt; off</pre></div></li></ul></div></dd></dl></div></div></div></div><p>Now we explain how to use the cluster to partially automatize the
migration. For the described attribute query using <span class="emphasis"><em>SAPHanaSR-showAttr</em></span> and
<span class="emphasis"><em>SAPHanaSR-filter</em></span> you need at least SAPHanaSR with package version 0.153.</p><div class="complex-example"><div class="example" id="id-1.12.4.5.6"><div class="example-title-wrap"><h6 class="example-title"><span class="number">Example 30: </span><span class="name">Moving an SAP HANA primary using the Cluster Toolset </span><a title="Permalink" class="permalink" href="#id-1.12.4.5.6">#</a></h6></div><div class="example-contents"><div class="itemizedlist "><ul class="itemizedlist"><li class="listitem "><p>Create a "move away" from this node rule by using the <span class="strong"><strong>force</strong></span> option.</p><div class="verbatim-wrap"><pre class="screen">crm resource move &lt;master-slave-resource-name&gt; <span class="strong"><strong>force</strong></span></pre></div><p>Because of the "move away" (<span class="strong"><strong>force</strong></span>) rule the cluster will <span class="strong"><strong>stop</strong></span> the
current primary. After that, run a <span class="strong"><strong>promote</strong></span> on the secondary site if the system
replication was in sync before. You should not migrate the primary if
the status of the system replication is not in sync (SFAIL).</p><div id="id-1.12.4.5.6.2.1.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.png" /><h6>Important</h6><p>Migration without the <span class="strong"><strong>force</strong></span> option will cause a takeover without the
former primary to be stopped. Only the migration with <span class="strong"><strong>force</strong></span> option is supported.</p></div><div id="id-1.12.4.5.6.2.1.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>The crm resource command <span class="strong"><strong>move</strong></span> was previously named <span class="strong"><strong>migrate</strong></span>. The <span class="strong"><strong>migrate</strong></span>
command is still valid but already known as obsolete.</p></div></li><li class="listitem "><p>Wait till the secondary has completely taken over to be the new primary role. You
see this using the command line tool <span class="emphasis"><em>SAPHanaSR-showAttr</em></span> and check for the
attributes "roles" for the new primary. It must start with "<span class="strong"><strong>4:P</strong></span>".</p><div class="verbatim-wrap"><pre class="screen">suse01:~ # SAPHanaSR-showAttr --format=script | \
   SAPHanaSR-filter --search='roles'
Mon Nov 11 20:38:50 2019; Hosts/suse01/roles=<span class="strong"><strong>1:P</strong></span>:master1::worker:
Mon Nov 11 20:38:50 2019; Hosts/suse02/roles=<span class="strong"><strong>4:P</strong></span>:master1:master:worker:master</pre></div></li><li class="listitem "><p>If you have set up <code class="literal">AUTOMATED_REGISTER="true"</code>, you can skip this step. In
other cases you now need to register the old primary. Enter the command
in our example on node1 as user <span class="emphasis"><em>&lt;sid&gt;adm</em></span>.</p><div class="verbatim-wrap"><pre class="screen">hdbnsutil -sr_register --remoteHost=suse02 --remoteInstance=10 \
    --replicationMode=sync --operationMode=logreplay \
    --name=WDF</pre></div></li><li class="listitem "><p>Clear the ban rules of the resource to allow the cluster to start the new secondary.</p><div class="verbatim-wrap"><pre class="screen">crm resource clear &lt;master-slave-resource-name&gt;</pre></div><div id="id-1.12.4.5.6.2.4.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.png" /><h6>Note</h6><p>The crm resource command <span class="strong"><strong>clear</strong></span> was previously named <span class="strong"><strong>unmigrate</strong></span>. The <span class="strong"><strong>unmigrate</strong></span>
command is still valid but already known as obsolete.</p></div></li><li class="listitem "><p>Wait till the new secondary has started. You
see this using the command line tool <span class="emphasis"><em>SAPHanaSR-showAttr</em></span> and check for the
attributes "roles" for the new primary. It must start with "<span class="strong"><strong>4:S</strong></span>".</p><div class="verbatim-wrap"><pre class="screen">suse01:~ # SAPHanaSR-showAttr --format=script | \
   SAPHanaSR-filter --search='roles'
Mon Nov 11 20:38:50 2019; Hosts/suse01/roles=<span class="strong"><strong>4:S</strong></span>:master1::worker:
Mon Nov 11 20:38:50 2019; Hosts/suse02/roles=<span class="strong"><strong>4:P</strong></span>:master1:master:worker:master</pre></div></li></ul></div></div></div></div></div></div></div><div class="sect1" id="app.hana-sr.information"><div class="titlepage"><div><div><h2 class="title"><span class="number">12 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Useful Links, Manuals, and SAP Notes</span> <a title="Permalink" class="permalink" href="#app.hana-sr.information">#</a></h2></div></div></div><div class="sect2" id="_suse_best_practices_and_more"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SUSE Best Practices and More</span> <a title="Permalink" class="permalink" href="#_suse_best_practices_and_more">#</a></h3></div></div></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.13.2.2.1"><span class="term ">Blog series #towardsZeroDowntime</span></dt><dd><p><a class="link" href="https://www.suse.com/c/tag/towardszerodowntime/" target="_blank">https://www.suse.com/c/tag/towardszerodowntime/</a></p></dd><dt id="id-1.13.2.2.2"><span class="term ">Best Practices for SAP on SUSE Linux Enterprise</span></dt><dd><p><a class="link" href="https://documentation.suse.com/sbp/all/" target="_blank">https://documentation.suse.com/sbp/all/</a></p></dd><dt id="id-1.13.2.2.3"><span class="term ">Blog in 2014 - Fail-Safe Operation of SAP HANA®: SUSE Extends Its High Availability Solution</span></dt><dd><p><a class="link" href="http://scn.sap.com/community/hana-in-memory/blog/2014/04/04/fail-safe-operation-of-sap-hana-suse-extends-its-high-availability-solution" target="_blank">http://scn.sap.com/community/hana-in-memory/blog/2014/04/04/fail-safe-operation-of-sap-hana-suse-extends-its-high-availability-solution</a></p></dd></dl></div></div><div class="sect2" id="_suse_product_documentation"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SUSE Product Documentation</span> <a title="Permalink" class="permalink" href="#_suse_product_documentation">#</a></h3></div></div></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.13.3.2.1"><span class="term ">SUSE product manuals and documentation</span></dt><dd><p><a class="link" href="https://documentation.suse.com/" target="_blank">https://documentation.suse.com/</a></p></dd><dt id="id-1.13.3.2.2"><span class="term ">Current online documentation of SLES for SAP</span></dt><dd><p><a class="link" href="https://documentation.suse.com/sles-sap/15-SP1/" target="_blank">https://documentation.suse.com/sles-sap/15-SP1/</a></p></dd><dt id="id-1.13.3.2.3"><span class="term ">Current online documentation of SUSE Linux Enterprise High Availability Extension</span></dt><dd><p><a class="link" href="https://documentation.suse.com/sle-ha/15-SP1/" target="_blank">https://documentation.suse.com/sle-ha/15-SP1/</a></p></dd><dt id="id-1.13.3.2.4"><span class="term ">Tuning guide for SUSE Linux Enterprise Server</span></dt><dd><p><a class="link" href="https://documentation.suse.com/sles/15-SP1/html/SLES-all/book-sle-tuning.html" target="_blank">https://documentation.suse.com/sles/15-SP1/html/SLES-all/book-sle-tuning.html</a></p></dd><dt id="id-1.13.3.2.5"><span class="term ">Storage administration guide for SUSE Linux Enterprise Server</span></dt><dd><p><a class="link" href="https://documentation.suse.com/sles/15-SP1/html/SLES-all/book-storage.html" target="_blank">https://documentation.suse.com/sles/15-SP1/html/SLES-all/book-storage.html</a></p></dd><dt id="id-1.13.3.2.6"><span class="term ">Release notes</span></dt><dd><p><a class="link" href="https://www.suse.com/releasenotes" target="_blank">https://www.suse.com/releasenotes</a></p></dd><dt id="id-1.13.3.2.7"><span class="term ">TID Estimate correct multipath timeout</span></dt><dd><p><a class="link" href="http://www.suse.com/support/kb/doc.php?id=7008216" target="_blank">http://www.suse.com/support/kb/doc.php?id=7008216</a></p></dd><dt id="id-1.13.3.2.8"><span class="term ">TID How to load the correct watchdog kernel module</span></dt><dd><p><a class="link" href="http://www.suse.com/support/kb/doc.php?id=7016880" target="_blank">http://www.suse.com/support/kb/doc.php?id=7016880</a></p></dd><dt id="id-1.13.3.2.9"><span class="term ">TID Addressing file system performance issues on NUMA machines</span></dt><dd><p><a class="link" href="http://www.suse.com/support/kb/doc.php?id=7008919" target="_blank">http://www.suse.com/support/kb/doc.php?id=7008919</a></p></dd><dt id="id-1.13.3.2.10"><span class="term ">TID Overcommit Memory in SLES</span></dt><dd><p><a class="link" href="https://www.suse.com/support/kb/doc.php?id=7002775" target="_blank">https://www.suse.com/support/kb/doc.php?id=7002775</a></p></dd><dt id="id-1.13.3.2.11"><span class="term ">SLES technical information</span></dt><dd><p><a class="link" href="https://www.suse.com/products/server/technical-information/" target="_blank">https://www.suse.com/products/server/technical-information/</a></p></dd><dt id="id-1.13.3.2.12"><span class="term ">XFS file system</span></dt><dd><p><a class="link" href="https://www.suse.com/communities/conversations/xfs-the-file-system-of-choice/" target="_blank">https://www.suse.com/communities/conversations/xfs-the-file-system-of-choice/</a></p></dd></dl></div></div><div class="sect2" id="_manual_pages"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Manual Pages</span> <a title="Permalink" class="permalink" href="#_manual_pages">#</a></h3></div></div></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.13.4.2.1"><span class="term ">crm</span></dt><dd><p>crm.8</p></dd><dt id="id-1.13.4.2.2"><span class="term ">crm_simulate</span></dt><dd><p>crm_simulate.8</p></dd><dt id="id-1.13.4.2.3"><span class="term ">cs_clusterstate</span></dt><dd><p>cs_clusterstate.8</p></dd><dt id="id-1.13.4.2.4"><span class="term ">ocf_suse_SAPHana</span></dt><dd><p>ocf_suse_SAPHana.7</p></dd><dt id="id-1.13.4.2.5"><span class="term ">ocf_suse_SAPHanaTopology</span></dt><dd><p>ocf_suse_SAPHanaTopology.7</p></dd><dt id="id-1.13.4.2.6"><span class="term ">sbd</span></dt><dd><p>sbd.8</p></dd><dt id="id-1.13.4.2.7"><span class="term ">stonith_sbd</span></dt><dd><p>stonith_sbd.7</p></dd><dt id="id-1.13.4.2.8"><span class="term ">SAPHanaSR</span></dt><dd><p>SAPHanaSR.7</p></dd><dt id="id-1.13.4.2.9"><span class="term ">SAPHanaSR-showAttr</span></dt><dd><p>SAPHanaSR-showAttr.8</p></dd><dt id="id-1.13.4.2.10"><span class="term ">SAPHanaSR-replay-archive</span></dt><dd><p>SAPHanaSR-replay-archive.8</p></dd><dt id="id-1.13.4.2.11"><span class="term ">SAPHanaSR_manitenance_examples</span></dt><dd><p>SAPHanaSR_manitenance_examples.8</p></dd></dl></div></div><div class="sect2" id="_sap_product_documentation"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SAP Product Documentation</span> <a title="Permalink" class="permalink" href="#_sap_product_documentation">#</a></h3></div></div></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.13.5.2.1"><span class="term ">SAP HANA Installation and Update Guide</span></dt><dd><p><a class="link" href="http://help.sap.com/hana/SAP_HANA_Server_Installation_Guide_en.pdf" target="_blank">http://help.sap.com/hana/SAP_HANA_Server_Installation_Guide_en.pdf</a></p></dd><dt id="id-1.13.5.2.2"><span class="term ">SAP HANA Administration Guide</span></dt><dd><p><a class="link" href="http://help.sap.com/hana/SAP_HANA_Administration_Guide_en.pdf" target="_blank">http://help.sap.com/hana/SAP_HANA_Administration_Guide_en.pdf</a></p></dd></dl></div></div><div class="sect2" id="_sap_notes"><div class="titlepage"><div><div><h3 class="title"><span class="number">12.5 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">SAP Notes</span> <a title="Permalink" class="permalink" href="#_sap_notes">#</a></h3></div></div></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.13.6.2.1"><span class="term ">2578899 - SUSE Linux Enterprise Server 15: Installation Note</span></dt><dd><p><a class="link" href="https://launchpad.support.sap.com/#/notes/2578899" target="_blank">https://launchpad.support.sap.com/#/notes/2578899</a></p></dd><dt id="id-1.13.6.2.2"><span class="term ">2684254 - SAP HANA DB: Recommended OS settings for SLES 15 / SLES for SAP Applications 15</span></dt><dd><p><a class="link" href="https://launchpad.support.sap.com/#/notes/2684254" target="_blank">https://launchpad.support.sap.com/#/notes/2684254</a></p></dd><dt id="id-1.13.6.2.3"><span class="term ">1876398 - Network configuration for System Replication in HANA SP6</span></dt><dd><p><a class="link" href="https://launchpad.support.sap.com/#/notes/1876398" target="_blank">https://launchpad.support.sap.com/#/notes/1876398</a></p></dd><dt id="id-1.13.6.2.4"><span class="term ">611361 - Hostnames of SAP servers</span></dt><dd><p><a class="link" href="https://launchpad.support.sap.com/#/notes/611361" target="_blank">https://launchpad.support.sap.com/#/notes/611361</a></p></dd><dt id="id-1.13.6.2.5"><span class="term ">1275776 - Preparing SLES for Sap Environments</span></dt><dd><p><a class="link" href="https://launchpad.support.sap.com/#/notes/1275776" target="_blank">https://launchpad.support.sap.com/#/notes/1275776</a></p></dd><dt id="id-1.13.6.2.6"><span class="term ">1514967 - SAP HANA: Central Note</span></dt><dd><p><a class="link" href="https://launchpad.support.sap.com/#/notes/1514967" target="_blank">https://launchpad.support.sap.com/#/notes/1514967</a></p></dd><dt id="id-1.13.6.2.7"><span class="term ">1523337 - SAP In-Memory Database 1.0: Central Note</span></dt><dd><p><a class="link" href="https://launchpad.support.sap.com/#/notes/1523337" target="_blank">https://launchpad.support.sap.com/#/notes/1523337</a></p></dd><dt id="id-1.13.6.2.8"><span class="term ">2380229 - SAP HANA Platform 2.0 - Central Note</span></dt><dd><p><a class="link" href="https://launchpad.support.sap.com/#/notes/2380229" target="_blank">https://launchpad.support.sap.com/#/notes/2380229</a></p></dd><dt id="id-1.13.6.2.9"><span class="term ">1501701 - Single Computing Unit Performance and Sizing</span></dt><dd><p><a class="link" href="https://launchpad.support.sap.com/#/notes/1501701" target="_blank">https://launchpad.support.sap.com/#/notes/1501701</a></p></dd><dt id="id-1.13.6.2.10"><span class="term ">1944799 - SAP HANA Guidelines for SLES Operating System Installation</span></dt><dd><p><a class="link" href="https://launchpad.support.sap.com/#/notes/1944799" target="_blank">https://launchpad.support.sap.com/#/notes/1944799</a></p></dd><dt id="id-1.13.6.2.11"><span class="term ">1890444 - Slow HANA system due to CPU power save mode</span></dt><dd><p><a class="link" href="https://launchpad.support.sap.com/#/notes/1890444" target="_blank">https://launchpad.support.sap.com/#/notes/1890444</a></p></dd><dt id="id-1.13.6.2.12"><span class="term ">1888072 - SAP HANA DB: Indexserver crash in strcmp sse42</span></dt><dd><p><a class="link" href="https://launchpad.support.sap.com/#/notes/1888072" target="_blank">https://launchpad.support.sap.com/#/notes/1888072</a></p></dd><dt id="id-1.13.6.2.13"><span class="term ">1846872 - "No space left on device" error reported from HANA</span></dt><dd><p><a class="link" href="https://launchpad.support.sap.com/#/notes/1846872" target="_blank">https://launchpad.support.sap.com/#/notes/1846872</a></p></dd></dl></div></div></div><div class="sect1" id="app.hana-sr.example"><div class="titlepage"><div><div><h2 class="title"><span class="number">13 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Examples</span> <a title="Permalink" class="permalink" href="#app.hana-sr.example">#</a></h2></div></div></div><div class="sect2" id="_example_ha_cluster_init_configuration"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Example <code class="literal">ha-cluster-init</code> Configuration</span> <a title="Permalink" class="permalink" href="#_example_ha_cluster_init_configuration">#</a></h3></div></div></div><div class="verbatim-wrap"><pre class="screen">suse01:~ # ha-cluster-init -u
  Generating SSH key
  Configuring csync2
  Generating csync2 shared key (this may take a while)...done
  csync2 checking files...done

Configure Corosync (unicast):
  This will configure the cluster messaging layer.  You will need
  to specify a network address over which to communicate (default
  is eth0's network, but you can use the network address of any
  active interface).

  Address for ring0 [192.168.1.11]
  Port for ring0 [5405]

Configure SBD:
  If you have shared storage, for example a SAN or iSCSI target,
  you can use it avoid split-brain scenarios by configuring SBD.
  This requires a 1 MB partition, accessible to all nodes in the
  cluster.  The device path must be persistent and consistent
  across all nodes in the cluster, so /dev/disk/by-id/* devices
  are a good choice.  Note that all data on the partition you
  specify here will be destroyed.

Do you wish to use SBD (y/n)? y
  Path to storage device (e.g. /dev/disk/by-id/...), or "none" []/dev/disk/by-id/SBDA
WARNING: All data on /dev/disk/by-id/SBDA will be destroyed!
Are you sure you wish to use this device (y/n)? y
  Initializing SBD......done
  Hawk cluster interface is now running. To see cluster status, open:
    https://192.168.1.11:7630/
  Log in with username 'hacluster', password 'linux'
You should change the hacluster password to something more secure!
  Waiting for cluster........done
  Loading initial cluster configuration

Configure Administration IP Address:
  Optionally configure an administration virtual IP
  address. The purpose of this IP address is to
  provide a single IP that can be used to interact
  with the cluster, rather than using the IP address
  of any specific cluster node.

Do you wish to configure a virtual IP address (y/n)? n
  Done (log saved to /var/log/ha-cluster-bootstrap.log)</pre></div></div><div class="sect2" id="_example_cluster_configuration"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.2 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Example Cluster Configuration</span> <a title="Permalink" class="permalink" href="#_example_cluster_configuration">#</a></h3></div></div></div><p>The following complete crm configuration is for a two-node cluster
(suse01, suse02) and an SAP HANA database with SID HA1
and instance number 10. The virtual IP address in the example is
192.168.1.20</p><div class="verbatim-wrap"><pre class="screen">node suse01
node suse02

primitive rsc_SAPHanaTopology_HA1_HDB10 ocf:suse:SAPHanaTopology \
        op monitor interval=10 timeout=300 \
        op start interval=0 timeout=300 \
        op stop interval=0 timeout=300 \
        params SID=HA1 InstanceNumber=10
primitive rsc_SAPHana_HA1_HDB10 ocf:suse:SAPHana \
        op monitor interval=61 role=Slave timeout=700 \
        op start interval=0 timeout=3600 \
        op stop interval=0 timeout=3600 \
        op promote interval=0 timeout=3600 \
        op monitor interval=60 role=Master timeout=700 \
        params SID=HA1 InstanceNumber=10 PREFER_SITE_TAKEOVER=true \
               DUPLICATE_PRIMARY_TIMEOUT=7200 AUTOMATED_REGISTER=false
primitive rsc_ip_HA1_HDB10 ocf:heartbeat:IPaddr2 \
        op monitor interval=10 timeout=20 \
        params ip="192.168.1.20"
primitive stonith-sbd stonith:external/sbd \
        params pcmk_delay_max=15
ms msl_SAPHana_HA1_HDB10 rsc_SAPHana_HA1_HDB10 \
        meta clone-max=2 clone-node-max=1 interleave=true
clone cln_SAPHanaTopology_HA1_HDB10 rsc_SAPHanaTopology_HA1_HDB10 \
        meta clone-node-max=1 interleave=true
colocation col_saphana_ip_HA1_HDB10 2000: \
        rsc_ip_HA1_HDB10:Started msl_SAPHana_HA1_HDB10:Master
order ord_SAPHana_HA1_HDB10 2000: \
        cln_SAPHanaTopology_HA1_HDB10 msl_SAPHana_HA1_HDB10
property cib-bootstrap-options: \
        dc-version="1.1.19+20180928.0d2680780-1.8-1.1.19+20180928.0d2680780" \
        cluster-infrastructure=corosync \
        stonith-enabled=true \
        stonith-action=reboot \
        stonith-timeout=150s \
        last-lrm-refresh=1398346620
rsc_defaults rsc-options: \
        resource-stickiness=1000 \
        migration-threshold=5000
op_defaults op-options \
        timeout=600 \
        record-pending=true</pre></div></div><div class="sect2" id="_example_for_etccorosynccorosync_conf"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.3 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Example for /etc/corosync/corosync.conf</span> <a title="Permalink" class="permalink" href="#_example_for_etccorosynccorosync_conf">#</a></h3></div></div></div><p>The following file shows a typical corosync configuration with one ring.
Review the SUSE product documentation about details and about
additional rings.</p><div class="verbatim-wrap"><pre class="screen"># Read the corosync.conf.5 manual page
totem {
        version: 2
        secauth: on
        crypto_hash: sha1
        crypto_cipher: aes256
        cluster_name: suse-ha
        clear_node_high_bit: yes
        token: 5000
        token_retransmits_before_loss_const: 10
        join: 60
        consensus: 6000
        max_messages: 20
        interface {
                ringnumber: 0
                mcastport: 5405
                ttl: 1
        }

        transport: udpu
}

logging {
        fileline: off
        to_stderr: no
        to_logfile: no
        logfile: /var/log/cluster/corosync.log
        to_syslog: yes
        debug: off
        timestamp: on
        logger_subsys {
                subsys: QUORUM
                debug: off
        }

}

nodelist {
        node {
                ring0_addr: 192.168.1.11
                nodeid: 1
        }

        node {
                ring0_addr: 192.168.1.12
                nodeid: 2
        }

}

quorum {

        # Enable and configure quorum subsystem (default: off)
        # see also corosync.conf.5 and votequorum.5
        provider: corosync_votequorum
        expected_votes: 2
        two_node: 1
}</pre></div></div><div class="sect2" id="_example_for_the_ipmi_stonith_method"><div class="titlepage"><div><div><h3 class="title"><span class="number">13.4 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Example for the IPMI STONITH Method</span> <a title="Permalink" class="permalink" href="#_example_for_the_ipmi_stonith_method">#</a></h3></div></div></div><div class="verbatim-wrap"><pre class="screen">primitive rsc_suse01_stonith stonith:external/ipmi \
    params hostname="suse01" ipaddr="192.168.1.101" userid="stonith" \
    passwd="k1llm3" interface="lanplus" \
    op monitor interval="1800" timeout="30"
    ...
primitive rsc_suse02_stonith stonith:external/ipmi \
    params hostname="suse02" ipaddr="192.168.1.102" userid="stonith" \
    passwd="k1llm3" interface="lanplus" \
    op monitor interval="1800" timeout="30"
    ...
location loc_suse01_stonith rsc_suse01_stonith -inf: suse01
location loc_suse02_stonith rsc_suse02_stonith -inf: suse02</pre></div></div></div><div class="sect1" id="_reference"><div class="titlepage"><div><div><h2 class="title"><span class="number">14 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Reference</span> <a title="Permalink" class="permalink" href="#_reference">#</a></h2></div></div></div><p>For more detailled information, have a look at the documents listed below.</p><div class="sect2" id="_pacemaker"><div class="titlepage"><div><div><h3 class="title"><span class="number">14.1 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Pacemaker</span> <a title="Permalink" class="permalink" href="#_pacemaker">#</a></h3></div></div></div><div class="variablelist "><dl class="variablelist"><dt id="id-1.15.3.2.1"><span class="term ">Pacemaker Project Documentation</span></dt><dd><p><a class="link" href="https://clusterlabs.org/pacemaker/doc/" target="_blank">https://clusterlabs.org/pacemaker/doc/</a></p></dd></dl></div></div></div><div class="sect1" id="_legal_notice"><div class="titlepage"><div><div><h2 class="title"><span class="number">15 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">Legal Notice</span> <a title="Permalink" class="permalink" href="#_legal_notice">#</a></h2></div></div></div><p>Copyright © 2006–2021 SUSE LLC and contributors. All rights reserved.</p><p>Permission is granted to copy, distribute and/or modify this document under the terms of
the GNU Free Documentation License, Version 1.2 or (at your option) version 1.3; with the
Invariant Section being this copyright notice and license. A copy of the license version 1.2
is included in the section entitled "GNU Free Documentation License".</p><p>SUSE, the SUSE logo and YaST are registered trademarks of SUSE LLC in the United States
and other countries. For SUSE trademarks, see <a class="link" href="https://www.suse.com/company/legal/" target="_blank">https://www.suse.com/company/legal/</a>.</p><p>Linux is a registered trademark of Linus Torvalds. All other names or trademarks mentioned in
this document may be trademarks or registered trademarks of their respective owners.</p><p>This article is part of a series of documents called "SUSE Best Practices". The individual
documents in the series were contributed voluntarily by SUSE’s employees and by third
parties. The articles are intended only to be one example of how a particular action could be
taken.</p><p>Also, SUSE cannot verify either that the actions described
in the articles do what they claim to do or that they don’t have unintended
consequences.</p><p>All information found in this article has been compiled with utmost attention to detail.
However, this does not guarantee complete accuracy. Therefore, we need to specifically
state that neither SUSE LLC, its affiliates, the authors, nor the translators may be held
liable for possible errors or the consequences thereof. Below we draw your attention to
the license under which the articles are published.</p></div><div class="sect1" id="_gnu_free_documentation_license"><div class="titlepage"><div><div><h2 class="title"><span class="number">16 </span><span xmlns:dm="urn:x-suse:ns:docmanager" class="name">GNU Free Documentation License</span> <a title="Permalink" class="permalink" href="#_gnu_free_documentation_license">#</a></h2></div></div></div><p>Copyright © 2000, 2001, 2002 Free Software Foundation, Inc.
51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed.</p><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect2 bridgehead"><h3 class="title" id="_0_preamble"><span class="name">0. PREAMBLE</span><a title="Permalink" class="permalink" href="#_0_preamble">#</a></h3></div><p>The purpose of this License is to make a manual, textbook, or other functional and useful document "free" in the sense of freedom: to assure everyone the effective freedom to copy and redistribute it, with or without modifying it, either commercially or noncommercially.
Secondarily, this License preserves for the author and publisher a way to get credit for their work, while not being considered responsible for modifications made by others.</p><p>This License is a kind of "copyleft", which means that derivative works of the document must themselves be free in the same sense.
It complements the GNU General Public License, which is a copyleft license designed for free software.</p><p>We have designed this License in order to use it for manuals for free software, because free software needs free documentation: a free program should come with manuals providing the same freedoms that the software does.
But this License is not limited to software manuals; it can be used for any textual work, regardless of subject matter or whether it is published as a printed book.
We recommend this License principally for works whose purpose is instruction or reference.</p><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect2 bridgehead"><h3 class="title" id="_1_applicability_and_definitions"><span class="name">1. APPLICABILITY AND DEFINITIONS</span><a title="Permalink" class="permalink" href="#_1_applicability_and_definitions">#</a></h3></div><p>This License applies to any manual or other work, in any medium, that contains a notice placed by the copyright holder saying it can be distributed under the terms of this License.
Such a notice grants a world-wide, royalty-free license, unlimited in duration, to use that work under the conditions stated herein.
The "Document", below, refers to any such manual or work.
Any member of the public is a licensee, and is addressed as "you". You accept the license if you copy, modify or distribute the work in a way requiring permission under copyright law.</p><p>A "Modified Version" of the Document means any work containing the Document or a portion of it, either copied verbatim, or with modifications and/or translated into another language.</p><p>A "Secondary Section" is a named appendix or a front-matter section of the Document that deals exclusively with the relationship of the publishers or authors of the Document to the Document’s overall subject (or to related matters) and contains nothing that could fall directly within that overall subject.
(Thus, if the Document is in part a textbook of mathematics, a Secondary Section may not explain any mathematics.) The relationship could be a matter of historical connection with the subject or with related matters, or of legal, commercial, philosophical, ethical or political position regarding them.</p><p>The "Invariant Sections" are certain Secondary Sections whose titles are designated, as being those of Invariant Sections, in the notice that says that the Document is released under this License.
If a section does not fit the above definition of Secondary then it is not allowed to be designated as Invariant.
The Document may contain zero Invariant Sections.
If the Document does not identify any Invariant Sections then there are none.</p><p>The "Cover Texts" are certain short passages of text that are listed, as Front-Cover Texts or Back-Cover Texts, in the notice that says that the Document is released under this License.
A Front-Cover Text may be at most 5 words, and a Back-Cover Text may be at most 25 words.</p><p>A "Transparent" copy of the Document means a machine-readable copy, represented in a format whose specification is available to the general public, that is suitable for revising the document straightforwardly with generic text editors or (for images composed of pixels) generic paint programs or (for drawings) some widely available drawing editor, and that is suitable for input to text formatters or for automatic translation to a variety of formats suitable for input to text formatters.
A copy made in an otherwise Transparent file format whose markup, or absence of markup, has been arranged to thwart or discourage subsequent modification by readers is not Transparent.
An image format is not Transparent if used for any substantial amount of text.
A copy that is not "Transparent" is called "Opaque".</p><p>Examples of suitable formats for Transparent copies include plain ASCII without markup, Texinfo input format, LaTeX input format, SGML or XML using a publicly available DTD, and standard-conforming simple HTML, PostScript or PDF designed for human modification.
Examples of transparent image formats include PNG, XCF and JPG.
Opaque formats include proprietary formats that can be read and edited only by proprietary word processors, SGML or XML for which the DTD and/or processing tools are not generally available, and the machine-generated HTML, PostScript or PDF produced by some word processors for output purposes only.</p><p>The "Title Page" means, for a printed book, the title page itself, plus such following pages as are needed to hold, legibly, the material this License requires to appear in the title page.
For works in formats which do not have any title page as such, "Title Page" means the text near the most prominent appearance of the work’s title, preceding the beginning of the body of the text.</p><p>A section "Entitled XYZ" means a named subunit of the Document whose title either is precisely XYZ or contains XYZ in parentheses following text that translates XYZ in another language.
(Here XYZ stands for a specific section name mentioned below, such as "Acknowledgements", "Dedications", "Endorsements", or "History".) To "Preserve the Title" of such a section when you modify the Document means that it remains a section "Entitled XYZ" according to this definition.</p><p>The Document may include Warranty Disclaimers next to the notice which states that this License applies to the Document.
These Warranty Disclaimers are considered to be included by reference in this License, but only as regards disclaiming warranties: any other implication that these Warranty Disclaimers may have is void and has no effect on the meaning of this License.</p><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect2 bridgehead"><h3 class="title" id="_2_verbatim_copying"><span class="name">2. VERBATIM COPYING</span><a title="Permalink" class="permalink" href="#_2_verbatim_copying">#</a></h3></div><p>You may copy and distribute the Document in any medium, either commercially or noncommercially, provided that this License, the copyright notices, and the license notice saying this License applies to the Document are reproduced in all copies, and that you add no other conditions whatsoever to those of this License.
You may not use technical measures to obstruct or control the reading or further copying of the copies you make or distribute.
However, you may accept compensation in exchange for copies.
If you distribute a large enough number of copies you must also follow the conditions in section 3.</p><p>You may also lend copies, under the same conditions stated above, and you may publicly display copies.</p><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect2 bridgehead"><h3 class="title" id="_3_copying_in_quantity"><span class="name">3. COPYING IN QUANTITY</span><a title="Permalink" class="permalink" href="#_3_copying_in_quantity">#</a></h3></div><p>If you publish printed copies (or copies in media that commonly have printed covers) of the Document, numbering more than 100, and the Document’s license notice requires Cover Texts, you must enclose the copies in covers that carry, clearly and legibly, all these Cover Texts: Front-Cover Texts on the front cover, and Back-Cover Texts on the back cover.
Both covers must also clearly and legibly identify you as the publisher of these copies.
The front cover must present the full title with all words of the title equally prominent and visible.
You may add other material on the covers in addition.
Copying with changes limited to the covers, as long as they preserve the title of the Document and satisfy these conditions, can be treated as verbatim copying in other respects.</p><p>If the required texts for either cover are too voluminous to fit legibly, you should put the first ones listed (as many as fit reasonably) on the actual cover, and continue the rest onto adjacent pages.</p><p>If you publish or distribute Opaque copies of the Document numbering more than 100, you must either include a machine-readable Transparent copy along with each Opaque copy, or state in or with each Opaque copy a computer-network location from which the general network-using public has access to download using public-standard network protocols a complete Transparent copy of the Document, free of added material.
If you use the latter option, you must take reasonably prudent steps, when you begin distribution of Opaque copies in quantity, to ensure that this Transparent copy will remain thus accessible at the stated location until at least one year after the last time you distribute an Opaque copy (directly or through your agents or retailers) of that edition to the public.</p><p>It is requested, but not required, that you contact the authors of the Document well before redistributing any large number of copies, to give them a chance to provide you with an updated version of the Document.</p><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect2 bridgehead"><h3 class="title" id="_4_modifications"><span class="name">4. MODIFICATIONS</span><a title="Permalink" class="permalink" href="#_4_modifications">#</a></h3></div><p>You may copy and distribute a Modified Version of the Document under the conditions of sections 2 and 3 above, provided that you release the Modified Version under precisely this License, with the Modified Version filling the role of the Document, thus licensing distribution and modification of the Modified Version to whoever possesses a copy of it.
In addition, you must do these things in the Modified Version:</p><div class="orderedlist "><ol class="orderedlist" type="A"><li class="listitem "><p>Use in the Title Page (and on the covers, if any) a title distinct from that of the Document, and from those of previous versions (which should, if there were any, be listed in the History section of the Document). You may use the same title as a previous version if the original publisher of that version gives permission.</p></li><li class="listitem "><p>List on the Title Page, as authors, one or more persons or entities responsible for authorship of the modifications in the Modified Version, together with at least five of the principal authors of the Document (all of its principal authors, if it has fewer than five), unless they release you from this requirement.</p></li><li class="listitem "><p>State on the Title page the name of the publisher of the Modified Version, as the publisher.</p></li><li class="listitem "><p>Preserve all the copyright notices of the Document.</p></li><li class="listitem "><p>Add an appropriate copyright notice for your modifications adjacent to the other copyright notices.</p></li><li class="listitem "><p>Include, immediately after the copyright notices, a license notice giving the public permission to use the Modified Version under the terms of this License, in the form shown in the Addendum below.</p></li><li class="listitem "><p>Preserve in that license notice the full lists of Invariant Sections and required Cover Texts given in the Document’s license notice.</p></li><li class="listitem "><p>Include an unaltered copy of this License.</p></li><li class="listitem "><p>Preserve the section Entitled "History", Preserve its Title, and add to it an item stating at least the title, year, new authors, and publisher of the Modified Version as given on the Title Page. If there is no section Entitled "History" in the Document, create one stating the title, year, authors, and publisher of the Document as given on its Title Page, then add an item describing the Modified Version as stated in the previous sentence.</p></li><li class="listitem "><p>Preserve the network location, if any, given in the Document for public access to a Transparent copy of the Document, and likewise the network locations given in the Document for previous versions it was based on. These may be placed in the "History" section. You may omit a network location for a work that was published at least four years before the Document itself, or if the original publisher of the version it refers to gives permission.</p></li><li class="listitem "><p>For any section Entitled "Acknowledgements" or "Dedications", Preserve the Title of the section, and preserve in the section all the substance and tone of each of the contributor acknowledgements and/or dedications given therein.</p></li><li class="listitem "><p>Preserve all the Invariant Sections of the Document, unaltered in their text and in their titles. Section numbers or the equivalent are not considered part of the section titles.</p></li><li class="listitem "><p>Delete any section Entitled "Endorsements". Such a section may not be included in the Modified Version.</p></li><li class="listitem "><p>Do not retitle any existing section to be Entitled "Endorsements" or to conflict in title with any Invariant Section.</p></li><li class="listitem "><p>Preserve any Warranty Disclaimers.</p></li></ol></div><p>If the Modified Version includes new front-matter sections or appendices that qualify as Secondary Sections and contain no material copied from the Document, you may at your option designate some or all of these sections as invariant.
To do this, add their titles to the list of Invariant Sections in the Modified Version’s license notice.
These titles must be distinct from any other section titles.</p><p>You may add a section Entitled "Endorsements", provided it contains nothing but endorsements of your Modified Version by various parties—​for example, statements of peer review or that the text has been approved by an organization as the authoritative definition of a standard.</p><p>You may add a passage of up to five words as a Front-Cover Text, and a passage of up to 25 words as a Back-Cover Text, to the end of the list of Cover Texts in the Modified Version.
Only one passage of Front-Cover Text and one of Back-Cover Text may be added by (or through arrangements made by) any one entity.
If the Document already includes a cover text for the same cover, previously added by you or by arrangement made by the same entity you are acting on behalf of, you may not add another; but you may replace the old one, on explicit permission from the previous publisher that added the old one.</p><p>The author(s) and publisher(s) of the Document do not by this License give permission to use their names for publicity for or to assert or imply endorsement of any Modified Version.</p><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect2 bridgehead"><h3 class="title" id="_5_combining_documents"><span class="name">5. COMBINING DOCUMENTS</span><a title="Permalink" class="permalink" href="#_5_combining_documents">#</a></h3></div><p>You may combine the Document with other documents released under this License, under the terms defined in section 4 above for modified versions, provided that you include in the combination all of the Invariant Sections of all of the original documents, unmodified, and list them all as Invariant Sections of your combined work in its license notice, and that you preserve all their Warranty Disclaimers.</p><p>The combined work need only contain one copy of this License, and multiple identical Invariant Sections may be replaced with a single copy.
If there are multiple Invariant Sections with the same name but different contents, make the title of each such section unique by adding at the end of it, in parentheses, the name of the original author or publisher of that section if known, or else a unique number.
Make the same adjustment to the section titles in the list of Invariant Sections in the license notice of the combined work.</p><p>In the combination, you must combine any sections Entitled "History" in the various original documents, forming one section Entitled "History"; likewise combine any sections Entitled "Acknowledgements", and any sections Entitled "Dedications". You must delete all sections Entitled "Endorsements".</p><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect2 bridgehead"><h3 class="title" id="_6_collections_of_documents"><span class="name">6. COLLECTIONS OF DOCUMENTS</span><a title="Permalink" class="permalink" href="#_6_collections_of_documents">#</a></h3></div><p>You may make a collection consisting of the Document and other documents released under this License, and replace the individual copies of this License in the various documents with a single copy that is included in the collection, provided that you follow the rules of this License for verbatim copying of each of the documents in all other respects.</p><p>You may extract a single document from such a collection, and distribute it individually under this License, provided you insert a copy of this License into the extracted document, and follow this License in all other respects regarding verbatim copying of that document.</p><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect2 bridgehead"><h3 class="title" id="_7_aggregation_with_independent_works"><span class="name">7. AGGREGATION WITH INDEPENDENT WORKS</span><a title="Permalink" class="permalink" href="#_7_aggregation_with_independent_works">#</a></h3></div><p>A compilation of the Document or its derivatives with other separate and independent documents or works, in or on a volume of a storage or distribution medium, is called an "aggregate" if the copyright resulting from the compilation is not used to limit the legal rights of the compilation’s users beyond what the individual works permit.
When the Document is included in an aggregate, this License does not apply to the other works in the aggregate which are not themselves derivative works of the Document.</p><p>If the Cover Text requirement of section 3 is applicable to these copies of the Document, then if the Document is less than one half of the entire aggregate, the Document’s Cover Texts may be placed on covers that bracket the Document within the aggregate, or the electronic equivalent of covers if the Document is in electronic form.
Otherwise they must appear on printed covers that bracket the whole aggregate.</p><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect1 bridgehead"><h2 class="title" id="_8_translation"><span class="name">8. TRANSLATION</span><a title="Permalink" class="permalink" href="#_8_translation">#</a></h2></div><p>Translation is considered a kind of modification, so you may distribute translations of the Document under the terms of section 4.
Replacing Invariant Sections with translations requires special permission from their copyright holders, but you may include translations of some or all Invariant Sections in addition to the original versions of these Invariant Sections.
You may include a translation of this License, and all the license notices in the Document, and any Warranty Disclaimers, provided that you also include the original English version of this License and the original versions of those notices and disclaimers.
In case of a disagreement between the translation and the original version of this License or a notice or disclaimer, the original version will prevail.</p><p>If a section in the Document is Entitled "Acknowledgements", "Dedications", or "History", the requirement (section 4) to Preserve its Title (section 1) will typically require changing the actual title.</p><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect2 bridgehead"><h3 class="title" id="_9_termination"><span class="name">9. TERMINATION</span><a title="Permalink" class="permalink" href="#_9_termination">#</a></h3></div><p>You may not copy, modify, sublicense, or distribute the Document except as expressly provided for under this License.
Any other attempt to copy, modify, sublicense or distribute the Document is void, and will automatically terminate your rights under this License.
However, parties who have received copies, or rights, from you under this License will not have their licenses terminated so long as such parties remain in full compliance.</p><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect2 bridgehead"><h3 class="title" id="_10_future_revisions_of_this_license"><span class="name">10. FUTURE REVISIONS OF THIS LICENSE</span><a title="Permalink" class="permalink" href="#_10_future_revisions_of_this_license">#</a></h3></div><p>The Free Software Foundation may publish new, revised versions of the GNU Free Documentation License from time to time.
Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns.
See <a class="link" href="http://www.gnu.org/copyleft/" target="_blank">http://www.gnu.org/copyleft/</a>.</p><p>Each version of the License is given a distinguishing version number.
If the Document specifies that a particular numbered version of this License "or any later version" applies to it, you have the option of following the terms and conditions either of that specified version or of any later version that has been published (not as a draft) by the Free Software Foundation.
If the Document does not specify a version number of this License, you may choose any version ever published (not as a draft) by the Free Software Foundation.</p><div xmlns:dm="urn:x-suse:ns:docmanager" class="sect2 bridgehead"><h3 class="title" id="_addendum_how_to_use_this_license_for_your_documents"><span class="name">ADDENDUM: How to use this License for your documents</span><a title="Permalink" class="permalink" href="#_addendum_how_to_use_this_license_for_your_documents">#</a></h3></div><div class="verbatim-wrap"><pre class="screen">Copyright (c) YEAR YOUR NAME.
   Permission is granted to copy, distribute and/or modify this document
   under the terms of the GNU Free Documentation License, Version 1.2
   or any later version published by the Free Software Foundation;
   with no Invariant Sections, no Front-Cover Texts, and no Back-Cover Texts.
   A copy of the license is included in the section entitled “GNU
   Free Documentation License”.</pre></div><p>If you have Invariant Sections, Front-Cover Texts and Back-Cover Texts, replace the “
with…​Texts.”
line with this:</p><div class="verbatim-wrap"><pre class="screen">with the Invariant Sections being LIST THEIR TITLES, with the
   Front-Cover Texts being LIST, and with the Back-Cover Texts being LIST.</pre></div><p>If you have Invariant Sections without Cover Texts, or some other combination of the three, merge those two alternatives to suit the situation.</p><p>If your document contains nontrivial examples of program code, we recommend releasing these examples in parallel under your choice of free software license, such as the GNU General Public License, to permit their use in free software.</p></div></div></div><div class="page-bottom"><div class="_share-print"><div class="online-contents share"><strong>Share this page: </strong><span class="share-buttons"><span class="_share-fb bottom-button">Facebook</span><span class="spacer"> • </span><span class="_share-in bottom-button">LinkedIn</span><span class="spacer"> • </span><span class="_share-tw bottom-button">Twitter</span><span class="spacer"> • </span><span class="_share-mail bottom-button">E-Mail</span></span></div><div class="print"><span class="_print-button bottom-button">Print this page</span></div><div class="clearme"></div></div></div></div><div id="_inward"></div></div><div id="_footer-wrap"><div id="_footer"><p>©
        2021 
        SUSE</p><ul><li><a href="https://jobs.suse.com/" target="_top">Careers</a></li><li><a href="https://www.suse.com/company/legal/" target="_top">Legal</a></li><li><a href="https://www.suse.com/company/about/" target="_top">About</a></li><li><a href="https://www.suse.com/contact/" target="_top">Contact Us</a></li></ul></div></div></body></html>